{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<pre><code>pip install idfkit\n</code></pre> <p> O(1) lookups  Reference tracking  IDF + epJSON  Schema validation  3-D geometry  Simulation  Weather data  v8.9 -- v25.2</p>"},{"location":"#idfkit","title":"idfkit","text":"<p> A fast, modern EnergyPlus IDF/epJSON toolkit for Python \u2014 with O(1) lookups, automatic reference tracking, and built-in simulation support. </p> <p> </p> <p>Get Started  API Reference</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from idfkit import load_idf, write_idf\n\n# Load an existing IDF file\ndoc = load_idf(\"in.idf\")\n\n# Query objects with O(1) lookups\nzone = doc[\"Zone\"][\"Office\"]\nprint(zone.x_origin, zone.y_origin)\n\n# Modify a field\nzone.x_origin = 10.0\n\n# See what references the zone\nfor obj in doc.get_referencing(\"Office\"):\n    print(obj.obj_type, obj.name)\n\n# Write back to IDF (or epJSON)\nwrite_idf(doc, \"out.idf\")\n</code></pre>"},{"location":"#run-simulations","title":"Run Simulations","text":"<pre><code>from idfkit.simulation import simulate\n\nresult = simulate(doc, \"weather.epw\", design_day=True)\n\n# Query results\nts = result.sql.get_timeseries(\n    variable_name=\"Zone Mean Air Temperature\",\n    key_value=\"Office\",\n)\nprint(f\"Max temp: {max(ts.values):.1f}\")\n</code></pre>"},{"location":"#find-weather-stations","title":"Find Weather Stations","text":"<pre><code>from idfkit.weather import StationIndex, geocode\n\nindex = StationIndex.load()\nresults = index.nearest(*geocode(\"Chicago, IL\"))\nprint(results[0].station.display_name)\n</code></pre>"},{"location":"#explore-the-docs","title":"Explore the Docs","text":"<ul> <li> <p> Get Started</p> <p>Installation, quick start guide, and interactive tutorial.</p> <p> Get Started</p> </li> <li> <p> Concepts</p> <p>Architecture decisions, caching strategy, and design principles.</p> <p> Concepts</p> </li> <li> <p> Simulation</p> <p>Run EnergyPlus, parse results, batch processing, and caching.</p> <p> Simulation Guide</p> </li> <li> <p> Weather</p> <p>Station search, downloads, design days, and geocoding.</p> <p> Weather Guide</p> </li> <li> <p> Examples</p> <p>Parametric studies, sizing workflows, and cloud simulations.</p> <p> Examples</p> </li> <li> <p> API Reference</p> <p>Complete API documentation for all modules.</p> <p> API Reference</p> </li> </ul>"},{"location":"#more-resources","title":"More Resources","text":"Page Description Core Tutorial Interactive notebook covering basic, advanced, and expert usage Migrating from eppy Side-by-side comparison of eppy and idfkit APIs Benchmarks Performance comparison against eppy and other tools Troubleshooting Common errors and solutions"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>idfkit is benchmarked against eppy, opyplus, and energyplus-idd-idf-utilities on a 1,700-object IDF file (500 zones, 100 materials, 100 constructions, 1,000 surfaces) using EnergyPlus V9.3 -- the newest version natively supported by all four tools.</p> <p>Each operation was run 10 times (100 for sub-millisecond ops) and the minimum time is reported. Bars are sorted fastest to slowest.</p>"},{"location":"benchmarks/#load-idf-file","title":"Load IDF file","text":""},{"location":"benchmarks/#get-all-objects-by-type","title":"Get all objects by type","text":""},{"location":"benchmarks/#get-single-object-by-name","title":"Get single object by name","text":""},{"location":"benchmarks/#add-100-objects","title":"Add 100 objects","text":""},{"location":"benchmarks/#modify-fields-all-zones","title":"Modify fields (all zones)","text":""},{"location":"benchmarks/#write-idf-to-string","title":"Write IDF to string","text":""},{"location":"benchmarks/#supported-energyplus-versions","title":"Supported EnergyPlus versions","text":"Tool Versions Schema format idfkit 8.9 -- 25.2 epJSON schema (bundled, gzip-compressed) eppy 1.1 -- 9.2 (bundled IDD); any version with external IDD IDD file opyplus 8.0 -- 9.6, 22.1 -- 24.1 IDD file (bundled) idd-idf-utilities any version up to ~23.2 (IDD parser breaks on 24.1+) IDD file (external)"},{"location":"benchmarks/#methodology","title":"Methodology","text":"<p>Benchmarks are run with <code>gc.collect()</code> before each iteration and <code>gc.disable()</code> during timing to avoid GC pauses. The test IDF is generated programmatically with realistic object types (zones, materials, constructions, surfaces).</p> <p>To reproduce:</p> <pre><code>uv run --group benchmark python benchmarks/bench.py\n</code></pre>"},{"location":"migration/","title":"Migrating from eppy","text":"<p>idfkit provides a compatibility layer so most eppy code works with only minor changes. You can migrate gradually -- all the eppy-style methods listed below are available alongside the newer idfkit API.</p>"},{"location":"migration/#loading-a-file","title":"Loading a file","text":"<p>eppy requires you to locate and pass the IDD file yourself:</p> <pre><code>from eppy.modeleditor import IDF\n\nIDF.setiddname(\"/path/to/Energy+.idd\")\nidf = IDF(\"/path/to/in.idf\")\n</code></pre> <p>idfkit bundles schemas and detects the version automatically:</p> <pre><code>from idfkit import load_idf\n\ndoc = load_idf(\"in.idf\")\n</code></pre> <p>No IDD path is needed. If you want to target a specific EnergyPlus version:</p> <pre><code>doc = load_idf(\"in.idf\", version=(24, 1, 0))\n</code></pre>"},{"location":"migration/#quick-reference","title":"Quick reference","text":"<p>The table below maps common eppy patterns to their idfkit equivalents. The eppy alias column shows that the old spelling still works in idfkit when one exists.</p> Task eppy idfkit eppy alias in idfkit? Load file <code>IDF(idd, idf)</code> <code>load_idf(path)</code> -- Create object <code>idf.newidfobject(\"ZONE\", Name=...)</code> <code>doc.add(\"Zone\", \"name\", ...)</code> <code>doc.newidfobject(...)</code> Get collection <code>idf.idfobjects[\"ZONE\"]</code> <code>doc[\"Zone\"]</code> or <code>doc.zones</code> <code>doc.idfobjects[...]</code> Get object by name <code>idf.getobject(\"ZONE\", \"name\")</code> <code>doc[\"Zone\"][\"name\"]</code> <code>doc.getobject(...)</code> Remove object <code>idf.removeidfobject(obj)</code> <code>doc.remove(obj)</code> <code>doc.removeidfobject(obj)</code> Remove by index <code>idf.popidfobject(\"ZONE\", 0)</code> <code>doc.popidfobject(\"Zone\", 0)</code> <code>doc.popidfobject(...)</code> Copy object <code>idf.copyidfobject(obj)</code> <code>doc.copyidfobject(obj)</code> <code>doc.copyidfobject(obj)</code> Object type <code>obj.key</code> <code>obj.obj_type</code> <code>obj.key</code> Object name <code>obj.Name</code> <code>obj.name</code> <code>obj.Name</code> Parent document <code>obj.theidf</code> <code>obj._document</code> <code>obj.theidf</code> Field names <code>obj.fieldnames</code> <code>list(obj.data.keys())</code> <code>obj.fieldnames</code> Field values <code>obj.fieldvalues</code> <code>list(obj.data.values())</code> <code>obj.fieldvalues</code> Field IDD info <code>obj.getfieldidd(name)</code> <code>obj.get_field_idd(name)</code> <code>obj.getfieldidd(name)</code> Field range <code>obj.getrange(name)</code> <code>obj.getrange(name)</code> <code>obj.getrange(name)</code> Check range <code>obj.checkrange(name)</code> <code>obj.checkrange(name)</code> <code>obj.checkrange(name)</code> Follow reference <code>obj.get_referenced_object(name)</code> <code>obj.get_referenced_object(name)</code> <code>obj.get_referenced_object(name)</code> Find referrers <code>obj.getreferingobjs()</code> <code>obj.get_referring_objects()</code> <code>obj.getreferingobjs()</code> Group dict <code>idf.getiddgroupdict()</code> <code>doc.getiddgroupdict()</code> <code>doc.getiddgroupdict()</code> Get surfaces <code>idf.getsurfaces()</code> <code>doc.getsurfaces()</code> <code>doc.getsurfaces()</code> Save file <code>idf.save()</code> <code>doc.save()</code> <code>doc.save()</code> Save as <code>idf.saveas(path)</code> <code>doc.saveas(path)</code> <code>doc.saveas(path)</code> Save copy <code>idf.savecopy(path)</code> <code>doc.savecopy(path)</code> <code>doc.savecopy(path)</code> Output type <code>idf.outputtype = \"compressed\"</code> <code>doc.save(output_type=\"compressed\")</code> -- Run simulation <code>idf.run(weather)</code> <code>simulate(doc, weather)</code> <code>doc.run(weather)</code> Batch update <code>json_functions.updateidf(idf, d)</code> <code>doc.update(d)</code> <code>doc.update(d)</code> HTML tables <code>readhtml.titletable(html)</code> <code>result.html.titletable()</code> -- Window-wall ratio <code>idf.set_wwr(0.4)</code> <code>set_wwr(doc, 0.4)</code> -- Match surfaces <code>idf.intersect_match()</code> <code>intersect_match(doc)</code> --"},{"location":"migration/#creating-objects","title":"Creating objects","text":"<p>eppy:</p> <pre><code>zone = idf.newidfobject(\"ZONE\")\nzone.Name = \"Office\"\nzone.X_Origin = 0.0\n</code></pre> <p>idfkit:</p> <pre><code>zone = doc.add(\"Zone\", \"Office\", x_origin=0.0)\n</code></pre> <p>Or using the eppy-compatible method:</p> <pre><code>zone = doc.newidfobject(\"Zone\", Name=\"Office\", X_Origin=0.0)\n</code></pre>"},{"location":"migration/#accessing-fields","title":"Accessing fields","text":"<p>eppy uses the capitalised IDD field names:</p> <pre><code>print(zone.X_Origin)\nzone.X_Origin = 5.0\n</code></pre> <p>idfkit uses snake_case names:</p> <pre><code>print(zone.x_origin)\nzone.x_origin = 5.0\n</code></pre> <p>Both styles resolve to the same underlying data.</p>"},{"location":"migration/#reference-tracking-new-in-idfkit","title":"Reference tracking (new in idfkit)","text":"<p>eppy has no built-in way to find which objects reference a given name. idfkit maintains a live reference graph:</p> <pre><code># Find every object that points to the \"Office\" zone\nfor obj in doc.get_referencing(\"Office\"):\n    print(obj.obj_type, obj.name)\n\n# Find every name that the People object references\nnames = doc.get_references(people_obj)\n</code></pre>"},{"location":"migration/#renaming-with-cascading-updates-new-in-idfkit","title":"Renaming with cascading updates (new in idfkit)","text":"<p>In eppy, renaming a zone requires you to manually update every surface, people, lights, and other object that references it. In idfkit the reference graph handles this automatically:</p> <pre><code>zone = doc[\"Zone\"][\"Office\"]\nzone.name = \"Open_Office\"\n# All fields across the document that pointed to \"Office\" now say \"Open_Office\"\n</code></pre>"},{"location":"migration/#validation-new-in-idfkit","title":"Validation (new in idfkit)","text":"<pre><code>from idfkit import validate_document\n\nresult = validate_document(doc)\nif not result.is_valid:\n    for error in result.errors:\n        print(error)\n</code></pre>"},{"location":"migration/#saving-files","title":"Saving files","text":"<p>eppy saves through methods on the IDF object:</p> <pre><code>idf.saveas(\"out.idf\")\nidf.savecopy(\"backup.idf\")\nidf.save()\n</code></pre> <p>idfkit supports the same methods:</p> <pre><code>doc.saveas(\"out.idf\")  # save and update doc.filepath\ndoc.savecopy(\"backup.idf\")  # save without changing doc.filepath\ndoc.save()  # save to current doc.filepath\n</code></pre> <p>Or use the standalone writer for more control:</p> <pre><code>from idfkit import write_idf, write_epjson\n\nwrite_idf(doc, \"out.idf\")\nwrite_epjson(doc, \"out.epJSON\")  # or convert to epJSON\n</code></pre>"},{"location":"migration/#output-formatting-modes","title":"Output formatting modes","text":"<p>eppy controls output formatting with <code>idf.outputtype</code>:</p> <pre><code>idf.outputtype = \"nocomment\"\nidf.saveas(\"out.idf\")\n</code></pre> <p>idfkit passes the mode to the writer:</p> <pre><code>from idfkit import write_idf\n\nwrite_idf(doc, \"out.idf\", output_type=\"nocomment\")  # no field comments\nwrite_idf(doc, \"out.idf\", output_type=\"compressed\")  # single-line objects\nwrite_idf(doc, \"out.idf\", output_type=\"standard\")  # default, with comments\n</code></pre>"},{"location":"migration/#following-references","title":"Following references","text":"<p>eppy lets you follow a reference field to get the target object:</p> <pre><code>surface = idf.idfobjects[\"BuildingSurface:Detailed\"][0]\nconstruction = surface.get_referenced_object(\"Construction_Name\")\n</code></pre> <p>idfkit provides the same method:</p> <pre><code>surface = doc[\"BuildingSurface:Detailed\"][0]\nconstruction = surface.get_referenced_object(\"construction_name\")\n</code></pre>"},{"location":"migration/#finding-referring-objects","title":"Finding referring objects","text":"<p>eppy finds all objects that reference a given object:</p> <pre><code>zone = idf.idfobjects[\"ZONE\"][0]\nreferrers = zone.getreferingobjs()\n</code></pre> <p>idfkit provides both the eppy spelling and a corrected alias:</p> <pre><code>zone = doc[\"Zone\"][\"Office\"]\n\n# eppy-compatible spelling\nreferrers = zone.getreferingobjs()\n\n# Corrected spelling\nreferrers = zone.get_referring_objects()\n\n# Optional filters -- by IDD group and/or field name\nsurfaces = zone.getreferingobjs(\n    iddgroups=[\"Thermal Zones and Surfaces\"],\n    fields=[\"zone_name\"],\n)\n</code></pre>"},{"location":"migration/#range-checking","title":"Range checking","text":"<p>eppy provides range checking on numeric fields:</p> <pre><code>obj.getrange(\"Density\")\n# {'minimum': 0, 'type': 'real'}\n\nobj.checkrange(\"Density\")  # raises RangeError if out of range\n</code></pre> <p>idfkit supports the same API:</p> <pre><code>obj.getrange(\"density\")\n# {'minimum': 0, 'type': 'real'}\n\nobj.checkrange(\"density\")  # True, or raises RangeError\n</code></pre>"},{"location":"migration/#removing-objects-by-index","title":"Removing objects by index","text":"<p>eppy removes objects by index with <code>popidfobject</code>:</p> <pre><code>removed = idf.popidfobject(\"ZONE\", 0)\n</code></pre> <p>idfkit:</p> <pre><code>removed = doc.popidfobject(\"Zone\", 0)\n</code></pre>"},{"location":"migration/#batch-updates","title":"Batch updates","text":"<p>eppy uses <code>json_functions.updateidf()</code> for parametric sweeps:</p> <pre><code>from eppy import json_functions\n\njson_functions.updateidf(idf, {\"Zone.Office.x_origin\": 10.0})\n</code></pre> <p>idfkit has this as a method on the document:</p> <pre><code>doc.update({\n    \"Zone.Office.x_origin\": 10.0,\n    \"Zone.Office.y_origin\": 5.0,\n})\n</code></pre>"},{"location":"migration/#geometry","title":"Geometry","text":"<p>eppy relies on geomeppy for geometry operations. idfkit ships its own <code>Vector3D</code> and <code>Polygon3D</code> classes with no external dependencies:</p> <pre><code>from idfkit.geometry import (\n    calculate_surface_area,\n    calculate_surface_tilt,\n    calculate_surface_azimuth,\n    calculate_zone_volume,\n    calculate_zone_height,\n    calculate_zone_ceiling_area,\n)\n\nfor surface in doc[\"BuildingSurface:Detailed\"]:\n    area = calculate_surface_area(surface)\n    tilt = calculate_surface_tilt(surface)  # 0=up, 90=vertical, 180=down\n    azimuth = calculate_surface_azimuth(surface)  # 0=north, 90=east, 180=south\n    print(f\"{surface.name}: {area:.1f} m2, tilt={tilt:.0f}, azimuth={azimuth:.0f}\")\n\nprint(\"Zone volume:\", calculate_zone_volume(doc, \"Office\"))\nprint(\"Zone height:\", calculate_zone_height(doc, \"Office\"))\nprint(\"Ceiling area:\", calculate_zone_ceiling_area(doc, \"Office\"))\n</code></pre>"},{"location":"migration/#building-transforms","title":"Building transforms","text":"<p>Translate or rotate all surfaces in the model:</p> <pre><code>from idfkit.geometry import translate_building, rotate_building, Vector3D\n\n# Shift the entire building 10m east and 5m north\ntranslate_building(doc, Vector3D(10.0, 5.0, 0.0))\n\n# Rotate 45 degrees counter-clockwise around the origin\nrotate_building(doc, 45.0)\n\n# Rotate around a custom anchor point\nrotate_building(doc, 90.0, anchor=Vector3D(5.0, 5.0, 0.0))\n</code></pre>"},{"location":"migration/#window-wall-ratio","title":"Window-wall ratio","text":"<p>geomeppy sets window-wall ratios as a method on the IDF:</p> <pre><code># geomeppy (extends eppy)\nidf.set_wwr(0.4)\nidf.set_wwr(0.4, construction=\"SimpleGlazing\")\nidf.set_wwr(0.25, orientation=\"south\")\n</code></pre> <p>idfkit uses a standalone function:</p> <pre><code>from idfkit.geometry import set_wwr\n\n# Apply 40% WWR to all exterior walls\nset_wwr(doc, 0.4)\n\n# Specify a window construction\nset_wwr(doc, 0.4, construction=\"SimpleGlazing\")\n\n# Target only south-facing walls\nset_wwr(doc, 0.25, orientation=\"south\")\n</code></pre>"},{"location":"migration/#surface-intersection-and-matching","title":"Surface intersection and matching","text":"<p>geomeppy matches adjacent surfaces as a method on the IDF:</p> <pre><code># geomeppy (extends eppy)\nidf.intersect_match()\n</code></pre> <p>idfkit uses a standalone function:</p> <pre><code>from idfkit.geometry import intersect_match\n\n# Match coincident walls between zones and set boundary conditions\nintersect_match(doc)\n</code></pre>"},{"location":"migration/#strict-field-access-new-in-idfkit","title":"Strict field access (new in idfkit)","text":"<p>eppy silently returns an empty string when you mistype a field name, making bugs hard to find. idfkit defaults to the same behaviour for compatibility, but you can opt in to strict mode to catch typos immediately:</p> <pre><code>from idfkit import new_document\n\n# Enable strict mode to catch field-name typos during migration\ndoc = new_document()\ndoc.strict = True\n\nzone = doc.add(\"Zone\", \"Office\", x_origin=5.0)\nprint(zone.x_origin)  # 5.0 \u2014 known field, works fine\n\nzone.x_orgin  # AttributeError: 'Zone' object has no field 'x_orgin'\n</code></pre> <p>Enable strict mode during migration to surface field-name mismatches early. Once your code is clean you can leave it on or turn it off.</p>"},{"location":"migration/#running-a-simulation","title":"Running a simulation","text":"<p>eppy runs simulations directly on the IDF object:</p> <pre><code>idf.run(\"weather.epw\")\n</code></pre> <p>idfkit uses a standalone <code>simulate()</code> function (recommended):</p> <pre><code>from idfkit.simulation import simulate\n\nresult = simulate(doc, \"weather.epw\")\nprint(result.errors.summary())\n</code></pre> <p>Or use the eppy-compatible convenience method:</p> <pre><code># eppy-compatible shortcut (calls simulate() internally)\nresult = doc.run(\"weather.epw\", design_day=True)\n</code></pre>"},{"location":"migration/#html-tabular-output","title":"HTML tabular output","text":"<p>eppy parses HTML tabular output using <code>readhtml</code>:</p> <pre><code>from eppy import readhtml\n\nwith open(\"eplustbl.htm\") as f:\n    html = f.read()\ntables = readhtml.titletable(html)\n</code></pre> <p>idfkit parses HTML output as part of <code>SimulationResult</code>:</p> <pre><code>result = simulate(doc, weather)\nhtml = result.html  # HTMLResult, lazily parsed\n\n# eppy-compatible (title, rows) pairs\nfor title, rows in html.titletable():\n    print(title, len(rows), \"rows\")\n\n# Lookup by name\ntable = html.tablebyname(\"Site and Source Energy\")\nprint(table.to_dict())  # {row_key: {col_header: value}}\n\n# Filter by report\nannual = html.tablesbyreport(\"Annual Building Utility Performance Summary\")\n</code></pre> <p>Or parse a standalone HTML file:</p> <pre><code>from idfkit.simulation.parsers.html import HTMLResult\n\nhtml = HTMLResult.from_file(\"eplustbl.htm\")\n</code></pre>"},{"location":"api/document/","title":"Document","text":"<p>The <code>IDFDocument</code> class is the top-level container for an EnergyPlus model. It owns every object collection, the reference graph, and the schema, and provides methods for querying, mutating, and serialising the model.</p> <p>IDFDocument - The main container for an EnergyPlus model.</p> <p>Provides: - Typed access to object collections - Reference tracking for O(1) dependency lookups - On-demand validation - Support for both IDF and epJSON formats</p>"},{"location":"api/document/#idfkit.document.IDFDocument","title":"<code>IDFDocument</code>","text":"<p>               Bases: <code>EppyDocumentMixin</code></p> <p>Main container for an EnergyPlus model.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>tuple[int, int, int]</code> <p>The EnergyPlus version tuple (major, minor, patch)</p> <code>filepath</code> <code>Path | None</code> <p>Path to the source file (if loaded from file)</p> <code>_collections</code> <code>dict[str, IDFCollection]</code> <p>Dict of object_type -&gt; IDFCollection</p> <code>_schema</code> <code>EpJSONSchema | None</code> <p>EpJSONSchema for validation and field info</p> <code>_references</code> <code>ReferenceGraph</code> <p>ReferenceGraph for dependency tracking</p> Source code in <code>src/idfkit/document.py</code> <pre><code>class IDFDocument(EppyDocumentMixin):\n    \"\"\"\n    Main container for an EnergyPlus model.\n\n    Attributes:\n        version: The EnergyPlus version tuple (major, minor, patch)\n        filepath: Path to the source file (if loaded from file)\n        _collections: Dict of object_type -&gt; IDFCollection\n        _schema: EpJSONSchema for validation and field info\n        _references: ReferenceGraph for dependency tracking\n    \"\"\"\n\n    __slots__ = (\n        \"_collections\",\n        \"_references\",\n        \"_schedules_cache\",\n        \"_schema\",\n        \"_strict\",\n        \"filepath\",\n        \"version\",\n    )\n\n    version: tuple[int, int, int]\n    filepath: Path | None\n    _collections: dict[str, IDFCollection]\n    _schema: EpJSONSchema | None\n    _references: ReferenceGraph\n    _schedules_cache: dict[str, IDFObject] | None\n    _strict: bool\n\n    def __init__(\n        self,\n        version: tuple[int, int, int] | None = None,\n        schema: EpJSONSchema | None = None,\n        filepath: Path | str | None = None,\n        *,\n        strict: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize an IDFDocument.\n\n        Args:\n            version: EnergyPlus version tuple\n            schema: EpJSONSchema for validation\n            filepath: Source file path\n            strict: When ``True``, accessing an unknown field name on any\n                [IDFObject][idfkit.objects.IDFObject] owned by this document raises\n                ``AttributeError`` instead of returning ``None``.  This\n                is useful during migration from eppy to catch field-name\n                typos early.\n        \"\"\"\n        self.version = version or LATEST_VERSION\n        self.filepath = Path(filepath) if filepath else None\n        self._schema = schema\n        self._collections: dict[str, IDFCollection] = {}\n        self._references = ReferenceGraph()\n        self._schedules_cache: dict[str, IDFObject] | None = None\n        self._strict = strict\n\n    @property\n    def strict(self) -&gt; bool:\n        \"\"\"Whether strict field access mode is enabled.\n\n        When ``True``, accessing an unknown field name on objects in this\n        document raises ``AttributeError`` instead of returning ``None``.\n        \"\"\"\n        return self._strict\n\n    @strict.setter\n    def strict(self, value: bool) -&gt; None:\n        self._strict = value\n\n    @property\n    def schema(self) -&gt; EpJSONSchema | None:\n        \"\"\"The EpJSON schema for validation and field info.\"\"\"\n        return self._schema\n\n    @property\n    def collections(self) -&gt; dict[str, IDFCollection]:\n        \"\"\"Dict of object_type -&gt; IDFCollection.\"\"\"\n        return self._collections\n\n    @property\n    def references(self) -&gt; ReferenceGraph:\n        \"\"\"The reference graph for dependency tracking.\"\"\"\n        return self._references\n\n    # -------------------------------------------------------------------------\n    # Collection Access\n    # -------------------------------------------------------------------------\n\n    def __getitem__(self, obj_type: str) -&gt; IDFCollection:\n        \"\"\"\n        Get collection by object type name.\n\n        Returns an empty collection if no objects of that type exist yet.\n\n        Examples:\n            Access all zones in the model:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")  # doctest: +ELLIPSIS\n            Zone('Core_ZN')\n            &gt;&gt;&gt; len(model[\"Zone\"])\n            2\n\n            Look up a specific zone by name (O(1)):\n\n            &gt;&gt;&gt; model[\"Zone\"][\"Perimeter_ZN_1\"].name\n            'Perimeter_ZN_1'\n        \"\"\"\n        try:\n            return self._collections[obj_type]\n        except KeyError:\n            coll = IDFCollection(obj_type)\n            self._collections[obj_type] = coll\n            return coll\n\n    def __getattr__(self, name: str) -&gt; IDFCollection:\n        \"\"\"\n        Get collection by Python-style attribute name.\n\n        Convenient shorthand names are mapped to their IDF equivalents\n        (e.g. ``zones`` -&gt; ``Zone``, ``building_surfaces`` -&gt;\n        ``BuildingSurface:Detailed``).\n\n        Examples:\n            Use shorthand attribute names for common object types:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; len(model.zones)\n            1\n            &gt;&gt;&gt; model.zones[0].name\n            'Perimeter_ZN_1'\n\n        Raises:\n            AttributeError: If the attribute is not a known collection mapping.\n        \"\"\"\n        if name.startswith(\"_\"):\n            raise AttributeError(name)\n\n        # Check the mapping\n        obj_type = _PYTHON_TO_IDF.get(name)\n        if obj_type:\n            return self[obj_type]\n\n        # Try as-is with different cases\n        for key in self._collections:\n            if key.lower().replace(\":\", \"_\").replace(\" \", \"_\") == name.lower():\n                return self._collections[key]\n\n        # Raise AttributeError for unknown attributes\n        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")  # noqa: TRY003\n\n    def __contains__(self, obj_type: str) -&gt; bool:\n        \"\"\"Check if document has objects of a type.\n\n        Examples:\n            Check whether the model contains any zones or materials:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n            Zone('Office')\n            &gt;&gt;&gt; \"Zone\" in model\n            True\n            &gt;&gt;&gt; \"Material\" in model\n            False\n        \"\"\"\n        return obj_type in self._collections and len(self._collections[obj_type]) &gt; 0\n\n    def __iter__(self) -&gt; Iterator[str]:\n        \"\"\"Iterate over object type names.\"\"\"\n        return iter(self._collections)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return total number of objects.\"\"\"\n        return sum(len(c) for c in self._collections.values())\n\n    def keys(self) -&gt; list[str]:\n        \"\"\"Return list of object type names that have objects.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n            Zone('Office')\n            &gt;&gt;&gt; model.keys()\n            ['Zone']\n        \"\"\"\n        return [k for k, v in self._collections.items() if v]\n\n    def values(self) -&gt; list[IDFCollection]:\n        \"\"\"Return list of non-empty collections.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n            Zone('Office')\n            &gt;&gt;&gt; len(model.values())\n            1\n        \"\"\"\n        return [v for v in self._collections.values() if v]\n\n    def items(self) -&gt; list[tuple[str, IDFCollection]]:\n        \"\"\"Return list of (object_type, collection) pairs for non-empty collections.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n            Zone('Office')\n            &gt;&gt;&gt; [(t, len(c)) for t, c in model.items()]\n            [('Zone', 1)]\n        \"\"\"\n        return [(k, v) for k, v in self._collections.items() if v]\n\n    def describe(self, obj_type: str) -&gt; ObjectDescription:\n        \"\"\"\n        Get detailed field information for an object type.\n\n        Returns a description of the object type including all fields,\n        their types, defaults, constraints, and whether they are required.\n\n        This is useful for discovering what fields are available when\n        creating new objects.\n\n        Args:\n            obj_type: Object type name (e.g., \"Zone\", \"Material\")\n\n        Returns:\n            ObjectDescription with detailed field information\n\n        Raises:\n            ValueError: If no schema is loaded\n            KeyError: If the object type is not found in the schema\n\n        Examples:\n            Discover which fields are needed for a new Material:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; mat_desc = model.describe(\"Material\")\n            &gt;&gt;&gt; mat_desc.required_fields\n            ['roughness', 'thickness', 'conductivity', 'density', 'specific_heat']\n\n            Explore Zone fields:\n\n            &gt;&gt;&gt; zone_desc = model.describe(\"Zone\")\n            &gt;&gt;&gt; zone_desc.obj_type\n            'Zone'\n            &gt;&gt;&gt; len(zone_desc.fields) &gt; 0\n            True\n        \"\"\"\n        if self._schema is None:\n            msg = \"No schema loaded - cannot describe object types\"\n            raise ValueError(msg)\n        return describe_object_type(self._schema, obj_type)\n\n    # -------------------------------------------------------------------------\n    # Object Manipulation\n    # -------------------------------------------------------------------------\n\n    def add(\n        self,\n        obj_type: str,\n        name: str = \"\",\n        data: dict[str, Any] | None = None,\n        *,\n        validate: bool = True,\n        **kwargs: Any,\n    ) -&gt; IDFObject:\n        \"\"\"\n        Add a new object to the document.\n\n        Args:\n            obj_type: Object type (e.g., \"Zone\")\n            name: Object name (optional for object types without a name field,\n                such as Timestep, SimulationControl, GlobalGeometryRules)\n            data: Field data as dict\n            validate: If True (default), validate the object against schema before adding.\n                Raises ValidationFailedError if validation fails. Set to False for\n                bulk operations where performance matters.\n            **kwargs: Additional field values\n\n        Returns:\n            The created IDFObject\n\n        Raises:\n            ValidationFailedError: If validation fails (unknown fields, missing\n                required fields, invalid values)\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n\n            Create a thermal zone for a south-facing perimeter office:\n\n            &gt;&gt;&gt; zone = model.add(\"Zone\", \"Perimeter_ZN_South\",\n            ...     x_origin=0.0, y_origin=0.0, z_origin=0.0)\n            &gt;&gt;&gt; zone.name\n            'Perimeter_ZN_South'\n\n            Define a concrete wall material (200 mm, k=1.4 W/m-K):\n\n            &gt;&gt;&gt; concrete = model.add(\"Material\", \"Concrete_200mm\",\n            ...     roughness=\"MediumRough\", thickness=0.2,\n            ...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n            &gt;&gt;&gt; concrete.conductivity\n            1.4\n\n            Build a construction and assign it to a surface:\n\n            &gt;&gt;&gt; wall = model.add(\"Construction\", \"Ext_Wall\",\n            ...     outside_layer=\"Concrete_200mm\", validate=False)\n\n            Disable validation for bulk loading (e.g., importing from\n            another tool):\n\n            &gt;&gt;&gt; for i in range(3):\n            ...     _ = model.add(\"Zone\", f\"Floor{i+1}_Core\", validate=False)\n            &gt;&gt;&gt; len(model[\"Zone\"])\n            4\n        \"\"\"\n        # Merge data and kwargs\n        field_data = dict(data) if data else {}\n        field_data.update(kwargs)\n\n        # Get schema info\n        obj_schema: dict[str, Any] | None = None\n        field_order: list[str] | None = None\n        ref_fields: frozenset[str] | None = None\n        if self._schema:\n            obj_schema = self._schema.get_object_schema(obj_type)\n            if self._schema.has_name(obj_type):\n                field_order = self._schema.get_field_names(obj_type)\n            else:\n                field_order = self._schema.get_all_field_names(obj_type)\n            ref_fields = self._compute_ref_fields(self._schema, obj_type)\n\n        # Create object\n        obj = IDFObject(\n            obj_type=obj_type,\n            name=name,\n            data=field_data,\n            schema=obj_schema,\n            document=self,\n            field_order=field_order,\n            ref_fields=ref_fields,\n        )\n\n        # Validate if requested\n        if validate and self._schema:\n            errors = validate_object(obj, self._schema)\n            if errors:\n                raise ValidationFailedError(errors)\n\n        # Add to collection\n        self[obj_type].add(obj)\n\n        # Index references\n        self._index_object_references(obj)\n\n        # Invalidate schedules cache\n        if obj_type.upper().startswith(\"SCHEDULE\"):\n            self._schedules_cache = None\n\n        return obj\n\n    def removeidfobject(self, obj: IDFObject) -&gt; None:\n        \"\"\"Remove an object from the document.\n\n        !!! tip\n            This method is also the recommended idfkit API.  Alternatively,\n            use `popidfobject()` to remove by index.\n        \"\"\"\n        obj_type = obj.obj_type\n\n        if obj_type in self._collections:\n            self._collections[obj_type].remove(obj)\n\n        # Remove from reference graph\n        self._references.unregister(obj)\n\n        # Invalidate caches\n        if obj_type.upper().startswith(\"SCHEDULE\"):\n            self._schedules_cache = None\n\n    def rename(self, obj_type: str, old_name: str, new_name: str) -&gt; None:\n        \"\"\"\n        Rename an object and update all references.\n\n        All objects that reference the old name are automatically updated\n        to point to the new name via the reference graph.\n\n        Args:\n            obj_type: Object type\n            old_name: Current name\n            new_name: New name\n\n        Examples:\n            Rename a zone -- all referencing surfaces, people, lights,\n            etc. are updated automatically via the reference graph:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"THERMAL ZONE 1\")  # doctest: +ELLIPSIS\n            Zone('THERMAL ZONE 1')\n            &gt;&gt;&gt; model.rename(\"Zone\", \"THERMAL ZONE 1\", \"Perimeter_ZN_South\")\n            &gt;&gt;&gt; model.getobject(\"Zone\", \"THERMAL ZONE 1\") is None\n            True\n            &gt;&gt;&gt; model.getobject(\"Zone\", \"Perimeter_ZN_South\").name\n            'Perimeter_ZN_South'\n        \"\"\"\n        obj = self.getobject(obj_type, old_name)\n        if not obj:\n            raise KeyError(f\"No {obj_type} named '{old_name}'\")  # noqa: TRY003\n\n        # Setting the name triggers _set_name -&gt; _on_name_change which handles\n        # collection index, referencing objects, and graph updates.\n        obj.name = new_name\n\n    # -------------------------------------------------------------------------\n    # Reference Graph\n    # -------------------------------------------------------------------------\n\n    @staticmethod\n    def _compute_ref_fields(schema: EpJSONSchema, obj_type: str) -&gt; frozenset[str]:\n        \"\"\"Return frozenset of reference field names (python-style) for an object type.\"\"\"\n        pc = schema.get_parsing_cache(obj_type)\n        if pc is not None:\n            return pc.ref_fields\n        field_names = schema.get_field_names(obj_type)\n        return frozenset(f for f in field_names if schema.is_reference_field(obj_type, f))\n\n    def notify_name_change(self, obj: IDFObject, old_name: str, new_name: str) -&gt; None:\n        \"\"\"Called by IDFObject._set_name when a name changes.\"\"\"\n        # 1. Update collection index\n        obj_type = obj.obj_type\n        if obj_type in self._collections:\n            collection = self._collections[obj_type]\n            old_key = old_name.upper()\n            if old_key in collection.by_name:\n                del collection.by_name[old_key]\n            new_key = new_name.upper()\n            if new_name:\n                collection.by_name[new_key] = obj\n\n        # 2. Update referencing objects' _data directly (bypass _set_field to avoid recursion)\n        referencing = self._references.get_referencing_with_fields(old_name)\n        for ref_obj, field_name in referencing:\n            current = ref_obj.data.get(field_name, \"\")\n            if isinstance(current, str) and current.upper() == old_name.upper():\n                ref_obj.data[field_name] = new_name\n\n        # 3. Update graph indexes\n        self._references.rename_target(old_name, new_name)\n\n        # 4. Invalidate schedules cache if needed\n        if obj_type.upper().startswith(\"SCHEDULE\"):\n            self._schedules_cache = None\n\n    def notify_reference_change(self, obj: IDFObject, field_name: str, old_value: Any, new_value: Any) -&gt; None:\n        \"\"\"Called by IDFObject._set_field when a reference field changes.\"\"\"\n        old_str = old_value if isinstance(old_value, str) else None\n        new_str = new_value if isinstance(new_value, str) else None\n        self._references.update_reference(obj, field_name, old_str, new_str)\n\n    def _index_object_references(self, obj: IDFObject) -&gt; None:\n        \"\"\"Index all references in an object using pre-computed ref_fields.\"\"\"\n        # Fast path: use pre-computed ref_fields from parser / _ParsingCache\n        ref_fields = object.__getattribute__(obj, \"_ref_fields\")\n        if ref_fields is not None:\n            data = obj.data\n            register = self._references.register\n            for field_name in ref_fields:\n                value = data.get(field_name)\n                if value and isinstance(value, str) and value.strip():\n                    register(obj, field_name, value)\n            return\n\n        # Fallback for objects without pre-computed ref_fields\n        if not self._schema:\n            return\n        obj_type = obj.obj_type\n        field_names = self._schema.get_field_names(obj_type)\n        for field_name in field_names:\n            if self._schema.is_reference_field(obj_type, field_name):\n                value = obj.data.get(field_name)\n                if value and isinstance(value, str) and value.strip():\n                    self._references.register(obj, field_name, value)\n\n    def get_referencing(self, name: str) -&gt; set[IDFObject]:\n        \"\"\"Get all objects that reference a given name.\n\n        Uses the reference graph for O(1) lookup.  This is the primary\n        way to find all surfaces in a zone, all objects using a schedule,\n        or all surfaces assigned to a construction.\n\n        Examples:\n            Find every surface that belongs to a zone:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n            ...     surface_type=\"Wall\", construction_name=\"\",\n            ...     zone_name=\"Perimeter_ZN_1\",\n            ...     outside_boundary_condition=\"Outdoors\",\n            ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n            ...     validate=False)  # doctest: +ELLIPSIS\n            BuildingSurface:Detailed('South_Wall')\n            &gt;&gt;&gt; refs = model.get_referencing(\"Perimeter_ZN_1\")\n            &gt;&gt;&gt; len(refs)\n            1\n        \"\"\"\n        return self._references.get_referencing(name)\n\n    def get_references(self, obj: IDFObject) -&gt; set[str]:\n        \"\"\"Get all names that an object references.\n\n        Useful for understanding the dependency chain of a surface\n        (which zone and construction does it point to?).\n\n        Examples:\n            Inspect what a wall surface depends on:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n            ...     surface_type=\"Wall\", construction_name=\"\",\n            ...     zone_name=\"Perimeter_ZN_1\",\n            ...     outside_boundary_condition=\"Outdoors\",\n            ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n            ...     validate=False)\n            &gt;&gt;&gt; refs = model.get_references(wall)\n            &gt;&gt;&gt; \"PERIMETER_ZN_1\" in refs\n            True\n        \"\"\"\n        return self._references.get_references(obj)\n\n    # -------------------------------------------------------------------------\n    # Schedules (common access pattern)\n    # -------------------------------------------------------------------------\n\n    @property\n    def schedules_dict(self) -&gt; dict[str, IDFObject]:\n        \"\"\"\n        Get dict mapping schedule names to schedule objects.\n\n        This is a cached property for fast schedule lookup.\n        \"\"\"\n        if self._schedules_cache is None:\n            self._schedules_cache = self._build_schedules_dict()\n        return self._schedules_cache\n\n    def _build_schedules_dict(self) -&gt; dict[str, IDFObject]:\n        \"\"\"Build the schedules lookup dict.\"\"\"\n        schedules: dict[str, IDFObject] = {}\n        schedule_types = [\n            \"Schedule:Year\",\n            \"Schedule:Compact\",\n            \"Schedule:File\",\n            \"Schedule:Constant\",\n            \"Schedule:Day:Hourly\",\n            \"Schedule:Day:Interval\",\n            \"Schedule:Day:List\",\n            \"Schedule:Week:Daily\",\n            \"Schedule:Week:Compact\",\n        ]\n\n        for sched_type in schedule_types:\n            if sched_type in self._collections:\n                for sched in self._collections[sched_type]:\n                    if sched.name:\n                        schedules[sched.name.upper()] = sched\n\n        return schedules\n\n    def get_schedule(self, name: str) -&gt; IDFObject | None:\n        \"\"\"Get a schedule by name (case-insensitive).\"\"\"\n        return self.schedules_dict.get(name.upper())\n\n    def get_used_schedules(self) -&gt; set[str]:\n        \"\"\"\n        Get names of schedules actually used in the model.\n\n        Uses the reference graph for O(1) lookup per schedule.\n        \"\"\"\n        used: set[str] = set()\n        for name in self.schedules_dict:\n            if self._references.is_referenced(name):\n                used.add(name)\n        return used\n\n    # -------------------------------------------------------------------------\n    # Zone Surfaces (common access pattern)\n    # -------------------------------------------------------------------------\n\n    def get_zone_surfaces(self, zone_name: str) -&gt; list[IDFObject]:\n        \"\"\"Get all surfaces belonging to a zone.\"\"\"\n        return list(self._references.get_referencing(zone_name))\n\n    # -------------------------------------------------------------------------\n    # Iteration\n    # -------------------------------------------------------------------------\n\n    @property\n    def all_objects(self) -&gt; Iterator[IDFObject]:\n        \"\"\"Iterate over all objects in the document.\"\"\"\n        for collection in self._collections.values():\n            yield from collection\n\n    def objects_by_type(self) -&gt; Iterator[tuple[str, IDFCollection]]:\n        \"\"\"Iterate over (type, collection) pairs.\"\"\"\n        for obj_type, collection in self._collections.items():\n            if collection:\n                yield obj_type, collection\n\n    # -------------------------------------------------------------------------\n    # Expansion\n    # -------------------------------------------------------------------------\n\n    def expand(\n        self,\n        *,\n        energyplus: EnergyPlusConfig | None = None,\n        timeout: float = 120.0,\n    ) -&gt; IDFDocument:\n        \"\"\"Run the EnergyPlus *ExpandObjects* preprocessor on this document.\n\n        This replaces ``HVACTemplate:*`` objects with their fully specified\n        low-level HVAC equivalents and returns a **new** document.  The\n        current document is not mutated.\n\n        If the document contains no expandable objects, a copy is returned\n        immediately without invoking the preprocessor.\n\n        Args:\n            energyplus: Pre-configured EnergyPlus installation.  If ``None``,\n                auto-discovery is used.\n            timeout: Maximum time in seconds to wait for the preprocessor\n                (default 120).\n\n        Returns:\n            A new [IDFDocument][idfkit.document.IDFDocument] with all template objects expanded.\n\n        Raises:\n            EnergyPlusNotFoundError: If no EnergyPlus installation is found.\n            ExpandObjectsError: If the preprocessor fails.\n\n        Examples:\n            Expand HVACTemplate objects into low-level HVAC components\n            so you can inspect or modify the resulting system:\n\n                ```python\n                model = load_idf(\"5ZoneAirCooled_HVACTemplate.idf\")\n                expanded = model.expand()\n                for ideal in expanded[\"ZoneHVAC:IdealLoadsAirSystem\"]:\n                    print(ideal.name, ideal.cooling_limit)\n                ```\n\n            Point to a specific EnergyPlus installation:\n\n                ```python\n                from idfkit.simulation import find_energyplus\n                ep = find_energyplus(version=(24, 1, 0))\n                expanded = model.expand(energyplus=ep)\n                ```\n        \"\"\"\n        from .simulation.expand import expand_objects\n\n        return expand_objects(self, energyplus=energyplus, timeout=timeout)\n\n    # -------------------------------------------------------------------------\n    # Copying\n    # -------------------------------------------------------------------------\n\n    def copy(self) -&gt; IDFDocument:\n        \"\"\"Create a deep copy of the document.\n\n        The copy is independent -- modifying the copy does not affect\n        the original.\n\n        Examples:\n            Create a copy for parametric comparison (e.g., testing\n            different insulation strategies without altering the baseline):\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; baseline = new_document()\n            &gt;&gt;&gt; baseline.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n            Zone('Office')\n            &gt;&gt;&gt; variant = baseline.copy()\n            &gt;&gt;&gt; len(variant)\n            1\n            &gt;&gt;&gt; variant.add(\"Zone\", \"Server_Room\")  # doctest: +ELLIPSIS\n            Zone('Server_Room')\n            &gt;&gt;&gt; len(baseline)\n            1\n            &gt;&gt;&gt; len(variant)\n            2\n        \"\"\"\n        new_doc = IDFDocument(\n            version=self.version,\n            schema=self._schema,\n            filepath=self.filepath,\n        )\n\n        for obj in self.all_objects:\n            new_obj = obj.copy()\n            new_doc.addidfobject(new_obj)\n\n        return new_doc\n\n    # -------------------------------------------------------------------------\n    # String Representation\n    # -------------------------------------------------------------------------\n\n    def __repr__(self) -&gt; str:\n        version_str = f\"{self.version[0]}.{self.version[1]}.{self.version[2]}\"\n        return f\"IDFDocument(version={version_str}, objects={len(self)})\"\n\n    def __str__(self) -&gt; str:\n        lines = [repr(self), \"\"]\n        for obj_type, collection in sorted(self._collections.items()):\n            if collection:\n                lines.append(f\"  {obj_type}: {len(collection)} objects\")\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.all_objects","title":"<code>all_objects</code>  <code>property</code>","text":"<p>Iterate over all objects in the document.</p>"},{"location":"api/document/#idfkit.document.IDFDocument.collections","title":"<code>collections</code>  <code>property</code>","text":"<p>Dict of object_type -&gt; IDFCollection.</p>"},{"location":"api/document/#idfkit.document.IDFDocument.references","title":"<code>references</code>  <code>property</code>","text":"<p>The reference graph for dependency tracking.</p>"},{"location":"api/document/#idfkit.document.IDFDocument.schedules_dict","title":"<code>schedules_dict</code>  <code>property</code>","text":"<p>Get dict mapping schedule names to schedule objects.</p> <p>This is a cached property for fast schedule lookup.</p>"},{"location":"api/document/#idfkit.document.IDFDocument.schema","title":"<code>schema</code>  <code>property</code>","text":"<p>The EpJSON schema for validation and field info.</p>"},{"location":"api/document/#idfkit.document.IDFDocument.strict","title":"<code>strict</code>  <code>property</code> <code>writable</code>","text":"<p>Whether strict field access mode is enabled.</p> <p>When <code>True</code>, accessing an unknown field name on objects in this document raises <code>AttributeError</code> instead of returning <code>None</code>.</p>"},{"location":"api/document/#idfkit.document.IDFDocument.__contains__","title":"<code>__contains__(obj_type)</code>","text":"<p>Check if document has objects of a type.</p> <p>Examples:</p> <p>Check whether the model contains any zones or materials:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Office\")\nZone('Office')\n&gt;&gt;&gt; \"Zone\" in model\nTrue\n&gt;&gt;&gt; \"Material\" in model\nFalse\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def __contains__(self, obj_type: str) -&gt; bool:\n    \"\"\"Check if document has objects of a type.\n\n    Examples:\n        Check whether the model contains any zones or materials:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n        Zone('Office')\n        &gt;&gt;&gt; \"Zone\" in model\n        True\n        &gt;&gt;&gt; \"Material\" in model\n        False\n    \"\"\"\n    return obj_type in self._collections and len(self._collections[obj_type]) &gt; 0\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Get collection by Python-style attribute name.</p> <p>Convenient shorthand names are mapped to their IDF equivalents (e.g. <code>zones</code> -&gt; <code>Zone</code>, <code>building_surfaces</code> -&gt; <code>BuildingSurface:Detailed</code>).</p> <p>Examples:</p> <p>Use shorthand attribute names for common object types:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; len(model.zones)\n1\n&gt;&gt;&gt; model.zones[0].name\n'Perimeter_ZN_1'\n</code></pre> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the attribute is not a known collection mapping.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def __getattr__(self, name: str) -&gt; IDFCollection:\n    \"\"\"\n    Get collection by Python-style attribute name.\n\n    Convenient shorthand names are mapped to their IDF equivalents\n    (e.g. ``zones`` -&gt; ``Zone``, ``building_surfaces`` -&gt;\n    ``BuildingSurface:Detailed``).\n\n    Examples:\n        Use shorthand attribute names for common object types:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; len(model.zones)\n        1\n        &gt;&gt;&gt; model.zones[0].name\n        'Perimeter_ZN_1'\n\n    Raises:\n        AttributeError: If the attribute is not a known collection mapping.\n    \"\"\"\n    if name.startswith(\"_\"):\n        raise AttributeError(name)\n\n    # Check the mapping\n    obj_type = _PYTHON_TO_IDF.get(name)\n    if obj_type:\n        return self[obj_type]\n\n    # Try as-is with different cases\n    for key in self._collections:\n        if key.lower().replace(\":\", \"_\").replace(\" \", \"_\") == name.lower():\n            return self._collections[key]\n\n    # Raise AttributeError for unknown attributes\n    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")  # noqa: TRY003\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.__getitem__","title":"<code>__getitem__(obj_type)</code>","text":"<p>Get collection by object type name.</p> <p>Returns an empty collection if no objects of that type exist yet.</p> <p>Examples:</p> <p>Access all zones in the model:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")\nZone('Core_ZN')\n&gt;&gt;&gt; len(model[\"Zone\"])\n2\n</code></pre> <p>Look up a specific zone by name (O(1)):</p> <pre><code>&gt;&gt;&gt; model[\"Zone\"][\"Perimeter_ZN_1\"].name\n'Perimeter_ZN_1'\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def __getitem__(self, obj_type: str) -&gt; IDFCollection:\n    \"\"\"\n    Get collection by object type name.\n\n    Returns an empty collection if no objects of that type exist yet.\n\n    Examples:\n        Access all zones in the model:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")  # doctest: +ELLIPSIS\n        Zone('Core_ZN')\n        &gt;&gt;&gt; len(model[\"Zone\"])\n        2\n\n        Look up a specific zone by name (O(1)):\n\n        &gt;&gt;&gt; model[\"Zone\"][\"Perimeter_ZN_1\"].name\n        'Perimeter_ZN_1'\n    \"\"\"\n    try:\n        return self._collections[obj_type]\n    except KeyError:\n        coll = IDFCollection(obj_type)\n        self._collections[obj_type] = coll\n        return coll\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.__init__","title":"<code>__init__(version=None, schema=None, filepath=None, *, strict=False)</code>","text":"<p>Initialize an IDFDocument.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>tuple[int, int, int] | None</code> <p>EnergyPlus version tuple</p> <code>None</code> <code>schema</code> <code>EpJSONSchema | None</code> <p>EpJSONSchema for validation</p> <code>None</code> <code>filepath</code> <code>Path | str | None</code> <p>Source file path</p> <code>None</code> <code>strict</code> <code>bool</code> <p>When <code>True</code>, accessing an unknown field name on any IDFObject owned by this document raises <code>AttributeError</code> instead of returning <code>None</code>.  This is useful during migration from eppy to catch field-name typos early.</p> <code>False</code> Source code in <code>src/idfkit/document.py</code> <pre><code>def __init__(\n    self,\n    version: tuple[int, int, int] | None = None,\n    schema: EpJSONSchema | None = None,\n    filepath: Path | str | None = None,\n    *,\n    strict: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize an IDFDocument.\n\n    Args:\n        version: EnergyPlus version tuple\n        schema: EpJSONSchema for validation\n        filepath: Source file path\n        strict: When ``True``, accessing an unknown field name on any\n            [IDFObject][idfkit.objects.IDFObject] owned by this document raises\n            ``AttributeError`` instead of returning ``None``.  This\n            is useful during migration from eppy to catch field-name\n            typos early.\n    \"\"\"\n    self.version = version or LATEST_VERSION\n    self.filepath = Path(filepath) if filepath else None\n    self._schema = schema\n    self._collections: dict[str, IDFCollection] = {}\n    self._references = ReferenceGraph()\n    self._schedules_cache: dict[str, IDFObject] | None = None\n    self._strict = strict\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over object type names.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:\n    \"\"\"Iterate over object type names.\"\"\"\n    return iter(self._collections)\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.__len__","title":"<code>__len__()</code>","text":"<p>Return total number of objects.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return total number of objects.\"\"\"\n    return sum(len(c) for c in self._collections.values())\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.add","title":"<code>add(obj_type, name='', data=None, *, validate=True, **kwargs)</code>","text":"<p>Add a new object to the document.</p> <p>Parameters:</p> Name Type Description Default <code>obj_type</code> <code>str</code> <p>Object type (e.g., \"Zone\")</p> required <code>name</code> <code>str</code> <p>Object name (optional for object types without a name field, such as Timestep, SimulationControl, GlobalGeometryRules)</p> <code>''</code> <code>data</code> <code>dict[str, Any] | None</code> <p>Field data as dict</p> <code>None</code> <code>validate</code> <code>bool</code> <p>If True (default), validate the object against schema before adding. Raises ValidationFailedError if validation fails. Set to False for bulk operations where performance matters.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional field values</p> <code>{}</code> <p>Returns:</p> Type Description <code>IDFObject</code> <p>The created IDFObject</p> <p>Raises:</p> Type Description <code>ValidationFailedError</code> <p>If validation fails (unknown fields, missing required fields, invalid values)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n</code></pre> <p>Create a thermal zone for a south-facing perimeter office:</p> <pre><code>&gt;&gt;&gt; zone = model.add(\"Zone\", \"Perimeter_ZN_South\",\n...     x_origin=0.0, y_origin=0.0, z_origin=0.0)\n&gt;&gt;&gt; zone.name\n'Perimeter_ZN_South'\n</code></pre> <p>Define a concrete wall material (200 mm, k=1.4 W/m-K):</p> <pre><code>&gt;&gt;&gt; concrete = model.add(\"Material\", \"Concrete_200mm\",\n...     roughness=\"MediumRough\", thickness=0.2,\n...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n&gt;&gt;&gt; concrete.conductivity\n1.4\n</code></pre> <p>Build a construction and assign it to a surface:</p> <pre><code>&gt;&gt;&gt; wall = model.add(\"Construction\", \"Ext_Wall\",\n...     outside_layer=\"Concrete_200mm\", validate=False)\n</code></pre> <p>Disable validation for bulk loading (e.g., importing from another tool):</p> <pre><code>&gt;&gt;&gt; for i in range(3):\n...     _ = model.add(\"Zone\", f\"Floor{i+1}_Core\", validate=False)\n&gt;&gt;&gt; len(model[\"Zone\"])\n4\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def add(\n    self,\n    obj_type: str,\n    name: str = \"\",\n    data: dict[str, Any] | None = None,\n    *,\n    validate: bool = True,\n    **kwargs: Any,\n) -&gt; IDFObject:\n    \"\"\"\n    Add a new object to the document.\n\n    Args:\n        obj_type: Object type (e.g., \"Zone\")\n        name: Object name (optional for object types without a name field,\n            such as Timestep, SimulationControl, GlobalGeometryRules)\n        data: Field data as dict\n        validate: If True (default), validate the object against schema before adding.\n            Raises ValidationFailedError if validation fails. Set to False for\n            bulk operations where performance matters.\n        **kwargs: Additional field values\n\n    Returns:\n        The created IDFObject\n\n    Raises:\n        ValidationFailedError: If validation fails (unknown fields, missing\n            required fields, invalid values)\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n\n        Create a thermal zone for a south-facing perimeter office:\n\n        &gt;&gt;&gt; zone = model.add(\"Zone\", \"Perimeter_ZN_South\",\n        ...     x_origin=0.0, y_origin=0.0, z_origin=0.0)\n        &gt;&gt;&gt; zone.name\n        'Perimeter_ZN_South'\n\n        Define a concrete wall material (200 mm, k=1.4 W/m-K):\n\n        &gt;&gt;&gt; concrete = model.add(\"Material\", \"Concrete_200mm\",\n        ...     roughness=\"MediumRough\", thickness=0.2,\n        ...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n        &gt;&gt;&gt; concrete.conductivity\n        1.4\n\n        Build a construction and assign it to a surface:\n\n        &gt;&gt;&gt; wall = model.add(\"Construction\", \"Ext_Wall\",\n        ...     outside_layer=\"Concrete_200mm\", validate=False)\n\n        Disable validation for bulk loading (e.g., importing from\n        another tool):\n\n        &gt;&gt;&gt; for i in range(3):\n        ...     _ = model.add(\"Zone\", f\"Floor{i+1}_Core\", validate=False)\n        &gt;&gt;&gt; len(model[\"Zone\"])\n        4\n    \"\"\"\n    # Merge data and kwargs\n    field_data = dict(data) if data else {}\n    field_data.update(kwargs)\n\n    # Get schema info\n    obj_schema: dict[str, Any] | None = None\n    field_order: list[str] | None = None\n    ref_fields: frozenset[str] | None = None\n    if self._schema:\n        obj_schema = self._schema.get_object_schema(obj_type)\n        if self._schema.has_name(obj_type):\n            field_order = self._schema.get_field_names(obj_type)\n        else:\n            field_order = self._schema.get_all_field_names(obj_type)\n        ref_fields = self._compute_ref_fields(self._schema, obj_type)\n\n    # Create object\n    obj = IDFObject(\n        obj_type=obj_type,\n        name=name,\n        data=field_data,\n        schema=obj_schema,\n        document=self,\n        field_order=field_order,\n        ref_fields=ref_fields,\n    )\n\n    # Validate if requested\n    if validate and self._schema:\n        errors = validate_object(obj, self._schema)\n        if errors:\n            raise ValidationFailedError(errors)\n\n    # Add to collection\n    self[obj_type].add(obj)\n\n    # Index references\n    self._index_object_references(obj)\n\n    # Invalidate schedules cache\n    if obj_type.upper().startswith(\"SCHEDULE\"):\n        self._schedules_cache = None\n\n    return obj\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.copy","title":"<code>copy()</code>","text":"<p>Create a deep copy of the document.</p> <p>The copy is independent -- modifying the copy does not affect the original.</p> <p>Examples:</p> <p>Create a copy for parametric comparison (e.g., testing different insulation strategies without altering the baseline):</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; baseline = new_document()\n&gt;&gt;&gt; baseline.add(\"Zone\", \"Office\")\nZone('Office')\n&gt;&gt;&gt; variant = baseline.copy()\n&gt;&gt;&gt; len(variant)\n1\n&gt;&gt;&gt; variant.add(\"Zone\", \"Server_Room\")\nZone('Server_Room')\n&gt;&gt;&gt; len(baseline)\n1\n&gt;&gt;&gt; len(variant)\n2\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def copy(self) -&gt; IDFDocument:\n    \"\"\"Create a deep copy of the document.\n\n    The copy is independent -- modifying the copy does not affect\n    the original.\n\n    Examples:\n        Create a copy for parametric comparison (e.g., testing\n        different insulation strategies without altering the baseline):\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; baseline = new_document()\n        &gt;&gt;&gt; baseline.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n        Zone('Office')\n        &gt;&gt;&gt; variant = baseline.copy()\n        &gt;&gt;&gt; len(variant)\n        1\n        &gt;&gt;&gt; variant.add(\"Zone\", \"Server_Room\")  # doctest: +ELLIPSIS\n        Zone('Server_Room')\n        &gt;&gt;&gt; len(baseline)\n        1\n        &gt;&gt;&gt; len(variant)\n        2\n    \"\"\"\n    new_doc = IDFDocument(\n        version=self.version,\n        schema=self._schema,\n        filepath=self.filepath,\n    )\n\n    for obj in self.all_objects:\n        new_obj = obj.copy()\n        new_doc.addidfobject(new_obj)\n\n    return new_doc\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.describe","title":"<code>describe(obj_type)</code>","text":"<p>Get detailed field information for an object type.</p> <p>Returns a description of the object type including all fields, their types, defaults, constraints, and whether they are required.</p> <p>This is useful for discovering what fields are available when creating new objects.</p> <p>Parameters:</p> Name Type Description Default <code>obj_type</code> <code>str</code> <p>Object type name (e.g., \"Zone\", \"Material\")</p> required <p>Returns:</p> Type Description <code>ObjectDescription</code> <p>ObjectDescription with detailed field information</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no schema is loaded</p> <code>KeyError</code> <p>If the object type is not found in the schema</p> <p>Examples:</p> <p>Discover which fields are needed for a new Material:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; mat_desc = model.describe(\"Material\")\n&gt;&gt;&gt; mat_desc.required_fields\n['roughness', 'thickness', 'conductivity', 'density', 'specific_heat']\n</code></pre> <p>Explore Zone fields:</p> <pre><code>&gt;&gt;&gt; zone_desc = model.describe(\"Zone\")\n&gt;&gt;&gt; zone_desc.obj_type\n'Zone'\n&gt;&gt;&gt; len(zone_desc.fields) &gt; 0\nTrue\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def describe(self, obj_type: str) -&gt; ObjectDescription:\n    \"\"\"\n    Get detailed field information for an object type.\n\n    Returns a description of the object type including all fields,\n    their types, defaults, constraints, and whether they are required.\n\n    This is useful for discovering what fields are available when\n    creating new objects.\n\n    Args:\n        obj_type: Object type name (e.g., \"Zone\", \"Material\")\n\n    Returns:\n        ObjectDescription with detailed field information\n\n    Raises:\n        ValueError: If no schema is loaded\n        KeyError: If the object type is not found in the schema\n\n    Examples:\n        Discover which fields are needed for a new Material:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; mat_desc = model.describe(\"Material\")\n        &gt;&gt;&gt; mat_desc.required_fields\n        ['roughness', 'thickness', 'conductivity', 'density', 'specific_heat']\n\n        Explore Zone fields:\n\n        &gt;&gt;&gt; zone_desc = model.describe(\"Zone\")\n        &gt;&gt;&gt; zone_desc.obj_type\n        'Zone'\n        &gt;&gt;&gt; len(zone_desc.fields) &gt; 0\n        True\n    \"\"\"\n    if self._schema is None:\n        msg = \"No schema loaded - cannot describe object types\"\n        raise ValueError(msg)\n    return describe_object_type(self._schema, obj_type)\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.expand","title":"<code>expand(*, energyplus=None, timeout=120.0)</code>","text":"<p>Run the EnergyPlus ExpandObjects preprocessor on this document.</p> <p>This replaces <code>HVACTemplate:*</code> objects with their fully specified low-level HVAC equivalents and returns a new document.  The current document is not mutated.</p> <p>If the document contains no expandable objects, a copy is returned immediately without invoking the preprocessor.</p> <p>Parameters:</p> Name Type Description Default <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, auto-discovery is used.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for the preprocessor (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument with all template objects expanded.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation is found.</p> <code>ExpandObjectsError</code> <p>If the preprocessor fails.</p> <p>Examples:</p> <p>Expand HVACTemplate objects into low-level HVAC components so you can inspect or modify the resulting system:</p> <pre><code>```python\nmodel = load_idf(\"5ZoneAirCooled_HVACTemplate.idf\")\nexpanded = model.expand()\nfor ideal in expanded[\"ZoneHVAC:IdealLoadsAirSystem\"]:\n    print(ideal.name, ideal.cooling_limit)\n```\n</code></pre> <p>Point to a specific EnergyPlus installation:</p> <pre><code>```python\nfrom idfkit.simulation import find_energyplus\nep = find_energyplus(version=(24, 1, 0))\nexpanded = model.expand(energyplus=ep)\n```\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def expand(\n    self,\n    *,\n    energyplus: EnergyPlusConfig | None = None,\n    timeout: float = 120.0,\n) -&gt; IDFDocument:\n    \"\"\"Run the EnergyPlus *ExpandObjects* preprocessor on this document.\n\n    This replaces ``HVACTemplate:*`` objects with their fully specified\n    low-level HVAC equivalents and returns a **new** document.  The\n    current document is not mutated.\n\n    If the document contains no expandable objects, a copy is returned\n    immediately without invoking the preprocessor.\n\n    Args:\n        energyplus: Pre-configured EnergyPlus installation.  If ``None``,\n            auto-discovery is used.\n        timeout: Maximum time in seconds to wait for the preprocessor\n            (default 120).\n\n    Returns:\n        A new [IDFDocument][idfkit.document.IDFDocument] with all template objects expanded.\n\n    Raises:\n        EnergyPlusNotFoundError: If no EnergyPlus installation is found.\n        ExpandObjectsError: If the preprocessor fails.\n\n    Examples:\n        Expand HVACTemplate objects into low-level HVAC components\n        so you can inspect or modify the resulting system:\n\n            ```python\n            model = load_idf(\"5ZoneAirCooled_HVACTemplate.idf\")\n            expanded = model.expand()\n            for ideal in expanded[\"ZoneHVAC:IdealLoadsAirSystem\"]:\n                print(ideal.name, ideal.cooling_limit)\n            ```\n\n        Point to a specific EnergyPlus installation:\n\n            ```python\n            from idfkit.simulation import find_energyplus\n            ep = find_energyplus(version=(24, 1, 0))\n            expanded = model.expand(energyplus=ep)\n            ```\n    \"\"\"\n    from .simulation.expand import expand_objects\n\n    return expand_objects(self, energyplus=energyplus, timeout=timeout)\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.get_references","title":"<code>get_references(obj)</code>","text":"<p>Get all names that an object references.</p> <p>Useful for understanding the dependency chain of a surface (which zone and construction does it point to?).</p> <p>Examples:</p> <p>Inspect what a wall surface depends on:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\",\n...     zone_name=\"Perimeter_ZN_1\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     validate=False)\n&gt;&gt;&gt; refs = model.get_references(wall)\n&gt;&gt;&gt; \"PERIMETER_ZN_1\" in refs\nTrue\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def get_references(self, obj: IDFObject) -&gt; set[str]:\n    \"\"\"Get all names that an object references.\n\n    Useful for understanding the dependency chain of a surface\n    (which zone and construction does it point to?).\n\n    Examples:\n        Inspect what a wall surface depends on:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\",\n        ...     zone_name=\"Perimeter_ZN_1\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     validate=False)\n        &gt;&gt;&gt; refs = model.get_references(wall)\n        &gt;&gt;&gt; \"PERIMETER_ZN_1\" in refs\n        True\n    \"\"\"\n    return self._references.get_references(obj)\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.get_referencing","title":"<code>get_referencing(name)</code>","text":"<p>Get all objects that reference a given name.</p> <p>Uses the reference graph for O(1) lookup.  This is the primary way to find all surfaces in a zone, all objects using a schedule, or all surfaces assigned to a construction.</p> <p>Examples:</p> <p>Find every surface that belongs to a zone:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\",\n...     zone_name=\"Perimeter_ZN_1\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     validate=False)\nBuildingSurface:Detailed('South_Wall')\n&gt;&gt;&gt; refs = model.get_referencing(\"Perimeter_ZN_1\")\n&gt;&gt;&gt; len(refs)\n1\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def get_referencing(self, name: str) -&gt; set[IDFObject]:\n    \"\"\"Get all objects that reference a given name.\n\n    Uses the reference graph for O(1) lookup.  This is the primary\n    way to find all surfaces in a zone, all objects using a schedule,\n    or all surfaces assigned to a construction.\n\n    Examples:\n        Find every surface that belongs to a zone:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\",\n        ...     zone_name=\"Perimeter_ZN_1\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     validate=False)  # doctest: +ELLIPSIS\n        BuildingSurface:Detailed('South_Wall')\n        &gt;&gt;&gt; refs = model.get_referencing(\"Perimeter_ZN_1\")\n        &gt;&gt;&gt; len(refs)\n        1\n    \"\"\"\n    return self._references.get_referencing(name)\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.get_schedule","title":"<code>get_schedule(name)</code>","text":"<p>Get a schedule by name (case-insensitive).</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def get_schedule(self, name: str) -&gt; IDFObject | None:\n    \"\"\"Get a schedule by name (case-insensitive).\"\"\"\n    return self.schedules_dict.get(name.upper())\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.get_used_schedules","title":"<code>get_used_schedules()</code>","text":"<p>Get names of schedules actually used in the model.</p> <p>Uses the reference graph for O(1) lookup per schedule.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def get_used_schedules(self) -&gt; set[str]:\n    \"\"\"\n    Get names of schedules actually used in the model.\n\n    Uses the reference graph for O(1) lookup per schedule.\n    \"\"\"\n    used: set[str] = set()\n    for name in self.schedules_dict:\n        if self._references.is_referenced(name):\n            used.add(name)\n    return used\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.get_zone_surfaces","title":"<code>get_zone_surfaces(zone_name)</code>","text":"<p>Get all surfaces belonging to a zone.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def get_zone_surfaces(self, zone_name: str) -&gt; list[IDFObject]:\n    \"\"\"Get all surfaces belonging to a zone.\"\"\"\n    return list(self._references.get_referencing(zone_name))\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.items","title":"<code>items()</code>","text":"<p>Return list of (object_type, collection) pairs for non-empty collections.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Office\")\nZone('Office')\n&gt;&gt;&gt; [(t, len(c)) for t, c in model.items()]\n[('Zone', 1)]\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def items(self) -&gt; list[tuple[str, IDFCollection]]:\n    \"\"\"Return list of (object_type, collection) pairs for non-empty collections.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n        Zone('Office')\n        &gt;&gt;&gt; [(t, len(c)) for t, c in model.items()]\n        [('Zone', 1)]\n    \"\"\"\n    return [(k, v) for k, v in self._collections.items() if v]\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.keys","title":"<code>keys()</code>","text":"<p>Return list of object type names that have objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Office\")\nZone('Office')\n&gt;&gt;&gt; model.keys()\n['Zone']\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def keys(self) -&gt; list[str]:\n    \"\"\"Return list of object type names that have objects.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n        Zone('Office')\n        &gt;&gt;&gt; model.keys()\n        ['Zone']\n    \"\"\"\n    return [k for k, v in self._collections.items() if v]\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.notify_name_change","title":"<code>notify_name_change(obj, old_name, new_name)</code>","text":"<p>Called by IDFObject._set_name when a name changes.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def notify_name_change(self, obj: IDFObject, old_name: str, new_name: str) -&gt; None:\n    \"\"\"Called by IDFObject._set_name when a name changes.\"\"\"\n    # 1. Update collection index\n    obj_type = obj.obj_type\n    if obj_type in self._collections:\n        collection = self._collections[obj_type]\n        old_key = old_name.upper()\n        if old_key in collection.by_name:\n            del collection.by_name[old_key]\n        new_key = new_name.upper()\n        if new_name:\n            collection.by_name[new_key] = obj\n\n    # 2. Update referencing objects' _data directly (bypass _set_field to avoid recursion)\n    referencing = self._references.get_referencing_with_fields(old_name)\n    for ref_obj, field_name in referencing:\n        current = ref_obj.data.get(field_name, \"\")\n        if isinstance(current, str) and current.upper() == old_name.upper():\n            ref_obj.data[field_name] = new_name\n\n    # 3. Update graph indexes\n    self._references.rename_target(old_name, new_name)\n\n    # 4. Invalidate schedules cache if needed\n    if obj_type.upper().startswith(\"SCHEDULE\"):\n        self._schedules_cache = None\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.notify_reference_change","title":"<code>notify_reference_change(obj, field_name, old_value, new_value)</code>","text":"<p>Called by IDFObject._set_field when a reference field changes.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def notify_reference_change(self, obj: IDFObject, field_name: str, old_value: Any, new_value: Any) -&gt; None:\n    \"\"\"Called by IDFObject._set_field when a reference field changes.\"\"\"\n    old_str = old_value if isinstance(old_value, str) else None\n    new_str = new_value if isinstance(new_value, str) else None\n    self._references.update_reference(obj, field_name, old_str, new_str)\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.objects_by_type","title":"<code>objects_by_type()</code>","text":"<p>Iterate over (type, collection) pairs.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def objects_by_type(self) -&gt; Iterator[tuple[str, IDFCollection]]:\n    \"\"\"Iterate over (type, collection) pairs.\"\"\"\n    for obj_type, collection in self._collections.items():\n        if collection:\n            yield obj_type, collection\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.removeidfobject","title":"<code>removeidfobject(obj)</code>","text":"<p>Remove an object from the document.</p> <p>Tip</p> <p>This method is also the recommended idfkit API.  Alternatively, use <code>popidfobject()</code> to remove by index.</p> Source code in <code>src/idfkit/document.py</code> <pre><code>def removeidfobject(self, obj: IDFObject) -&gt; None:\n    \"\"\"Remove an object from the document.\n\n    !!! tip\n        This method is also the recommended idfkit API.  Alternatively,\n        use `popidfobject()` to remove by index.\n    \"\"\"\n    obj_type = obj.obj_type\n\n    if obj_type in self._collections:\n        self._collections[obj_type].remove(obj)\n\n    # Remove from reference graph\n    self._references.unregister(obj)\n\n    # Invalidate caches\n    if obj_type.upper().startswith(\"SCHEDULE\"):\n        self._schedules_cache = None\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.rename","title":"<code>rename(obj_type, old_name, new_name)</code>","text":"<p>Rename an object and update all references.</p> <p>All objects that reference the old name are automatically updated to point to the new name via the reference graph.</p> <p>Parameters:</p> Name Type Description Default <code>obj_type</code> <code>str</code> <p>Object type</p> required <code>old_name</code> <code>str</code> <p>Current name</p> required <code>new_name</code> <code>str</code> <p>New name</p> required <p>Examples:</p> <p>Rename a zone -- all referencing surfaces, people, lights, etc. are updated automatically via the reference graph:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"THERMAL ZONE 1\")\nZone('THERMAL ZONE 1')\n&gt;&gt;&gt; model.rename(\"Zone\", \"THERMAL ZONE 1\", \"Perimeter_ZN_South\")\n&gt;&gt;&gt; model.getobject(\"Zone\", \"THERMAL ZONE 1\") is None\nTrue\n&gt;&gt;&gt; model.getobject(\"Zone\", \"Perimeter_ZN_South\").name\n'Perimeter_ZN_South'\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def rename(self, obj_type: str, old_name: str, new_name: str) -&gt; None:\n    \"\"\"\n    Rename an object and update all references.\n\n    All objects that reference the old name are automatically updated\n    to point to the new name via the reference graph.\n\n    Args:\n        obj_type: Object type\n        old_name: Current name\n        new_name: New name\n\n    Examples:\n        Rename a zone -- all referencing surfaces, people, lights,\n        etc. are updated automatically via the reference graph:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"THERMAL ZONE 1\")  # doctest: +ELLIPSIS\n        Zone('THERMAL ZONE 1')\n        &gt;&gt;&gt; model.rename(\"Zone\", \"THERMAL ZONE 1\", \"Perimeter_ZN_South\")\n        &gt;&gt;&gt; model.getobject(\"Zone\", \"THERMAL ZONE 1\") is None\n        True\n        &gt;&gt;&gt; model.getobject(\"Zone\", \"Perimeter_ZN_South\").name\n        'Perimeter_ZN_South'\n    \"\"\"\n    obj = self.getobject(obj_type, old_name)\n    if not obj:\n        raise KeyError(f\"No {obj_type} named '{old_name}'\")  # noqa: TRY003\n\n    # Setting the name triggers _set_name -&gt; _on_name_change which handles\n    # collection index, referencing objects, and graph updates.\n    obj.name = new_name\n</code></pre>"},{"location":"api/document/#idfkit.document.IDFDocument.values","title":"<code>values()</code>","text":"<p>Return list of non-empty collections.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Office\")\nZone('Office')\n&gt;&gt;&gt; len(model.values())\n1\n</code></pre> Source code in <code>src/idfkit/document.py</code> <pre><code>def values(self) -&gt; list[IDFCollection]:\n    \"\"\"Return list of non-empty collections.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n        Zone('Office')\n        &gt;&gt;&gt; len(model.values())\n        1\n    \"\"\"\n    return [v for v in self._collections.values() if v]\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":"<p>All custom exception classes raised by idfkit.</p> <p>Custom exceptions for idfkit.</p>"},{"location":"api/exceptions/#idfkit.exceptions.DanglingReferenceError","title":"<code>DanglingReferenceError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when an object references a non-existent object.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class DanglingReferenceError(IdfKitError):\n    \"\"\"Raised when an object references a non-existent object.\"\"\"\n\n    def __init__(self, source: IDFObject, field: str, target: str) -&gt; None:\n        self.source = source\n        self.field = field\n        self.target = target\n        super().__init__(\n            f\"Object {source.obj_type}:'{source.name}' field '{field}' references non-existent object '{target}'\"\n        )\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.DuplicateObjectError","title":"<code>DuplicateObjectError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when attempting to add an object with a duplicate name.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class DuplicateObjectError(IdfKitError):\n    \"\"\"Raised when attempting to add an object with a duplicate name.\"\"\"\n\n    def __init__(self, obj_type: str, name: str) -&gt; None:\n        self.obj_type = obj_type\n        self.name = name\n        super().__init__(f\"Duplicate {obj_type} object with name '{name}'\")\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.EnergyPlusNotFoundError","title":"<code>EnergyPlusNotFoundError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when EnergyPlus installation cannot be found.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class EnergyPlusNotFoundError(IdfKitError):\n    \"\"\"Raised when EnergyPlus installation cannot be found.\"\"\"\n\n    def __init__(self, searched_locations: list[str] | None = None) -&gt; None:\n        self.searched_locations = searched_locations or []\n        msg = \"Could not find an EnergyPlus installation.\"\n        if self.searched_locations:\n            msg += \"\\nSearched in:\\n\"\n            for loc in self.searched_locations:\n                msg += f\"  - {loc}\\n\"\n        msg += (\n            \"\\nTo fix this, either:\\n\"\n            \"  1. Set the ENERGYPLUS_DIR environment variable to your EnergyPlus install directory\\n\"\n            \"  2. Pass an explicit path: find_energyplus(path='/path/to/EnergyPlus')\\n\"\n            \"  3. Ensure 'energyplus' is on your PATH\"\n        )\n        super().__init__(msg)\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.ExpandObjectsError","title":"<code>ExpandObjectsError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when an EnergyPlus preprocessor fails.</p> <p>Attributes:</p> Name Type Description <code>preprocessor</code> <p>Name of the preprocessor that failed (e.g. <code>\"ExpandObjects\"</code>, <code>\"Slab\"</code>, <code>\"Basement\"</code>).</p> <code>exit_code</code> <p>Process exit code (<code>None</code> if timed out or not started).</p> <code>stderr</code> <p>Captured standard error output (truncated to 500 chars).</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class ExpandObjectsError(IdfKitError):\n    \"\"\"Raised when an EnergyPlus preprocessor fails.\n\n    Attributes:\n        preprocessor: Name of the preprocessor that failed (e.g.\n            ``\"ExpandObjects\"``, ``\"Slab\"``, ``\"Basement\"``).\n        exit_code: Process exit code (``None`` if timed out or not started).\n        stderr: Captured standard error output (truncated to 500 chars).\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        preprocessor: str | None = None,\n        exit_code: int | None = None,\n        stderr: str | None = None,\n    ) -&gt; None:\n        self.preprocessor = preprocessor\n        self.exit_code = exit_code\n        self.stderr = stderr\n        msg = message\n        if exit_code is not None:\n            msg += f\" (exit code {exit_code})\"\n        if stderr:\n            trimmed = stderr.strip()[:500]\n            msg += f\"\\nstderr: {trimmed}\"\n        super().__init__(msg)\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.IdfKitError","title":"<code>IdfKitError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all idfkit errors.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class IdfKitError(Exception):\n    \"\"\"Base exception for all idfkit errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.InvalidFieldError","title":"<code>InvalidFieldError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when an invalid field is accessed or set.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class InvalidFieldError(IdfKitError):\n    \"\"\"Raised when an invalid field is accessed or set.\"\"\"\n\n    def __init__(self, obj_type: str, field_name: str, available_fields: list[str] | None = None) -&gt; None:\n        self.obj_type = obj_type\n        self.field_name = field_name\n        self.available_fields = available_fields\n        msg = f\"Invalid field '{field_name}' for object type '{obj_type}'\"\n        if available_fields:\n            msg += f\"\\nAvailable fields: {', '.join(available_fields[:10])}\"\n            if len(available_fields) &gt; 10:\n                msg += f\" ... and {len(available_fields) - 10} more\"\n        super().__init__(msg)\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.NoDesignDaysError","title":"<code>NoDesignDaysError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when a DDY file contains no SizingPeriod:DesignDay objects.</p> <p>This typically occurs for weather stations that lack ASHRAE design conditions data in the climate.onebuilding.org database.</p> <p>Attributes:</p> Name Type Description <code>station_name</code> <p>Display name of the station (if available).</p> <code>ddy_path</code> <p>Path to the DDY file that was parsed.</p> <code>nearby_suggestions</code> <p>List of nearby stations that may have design days.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class NoDesignDaysError(IdfKitError):\n    \"\"\"Raised when a DDY file contains no SizingPeriod:DesignDay objects.\n\n    This typically occurs for weather stations that lack ASHRAE design\n    conditions data in the climate.onebuilding.org database.\n\n    Attributes:\n        station_name: Display name of the station (if available).\n        ddy_path: Path to the DDY file that was parsed.\n        nearby_suggestions: List of nearby stations that may have design days.\n    \"\"\"\n\n    def __init__(\n        self,\n        station_name: str | None = None,\n        ddy_path: str | None = None,\n        nearby_suggestions: list[str] | None = None,\n    ) -&gt; None:\n        self.station_name = station_name\n        self.ddy_path = ddy_path\n        self.nearby_suggestions = nearby_suggestions or []\n\n        if station_name:\n            msg = f\"DDY file for '{station_name}' contains no SizingPeriod:DesignDay objects.\"\n        elif ddy_path:\n            msg = f\"DDY file '{ddy_path}' contains no SizingPeriod:DesignDay objects.\"\n        else:\n            msg = \"DDY file contains no SizingPeriod:DesignDay objects.\"\n\n        msg += \"\\nThis station may lack ASHRAE design conditions data.\"\n\n        if self.nearby_suggestions:\n            msg += \"\\n\\nNearby stations that may have design days:\"\n            for suggestion in self.nearby_suggestions[:5]:\n                msg += f\"\\n  - {suggestion}\"\n\n        super().__init__(msg)\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.RangeError","title":"<code>RangeError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when a field value is outside the valid range (eppy compatibility).</p> <p>Attributes:</p> Name Type Description <code>obj_type</code> <p>The object type.</p> <code>obj_name</code> <p>The object name.</p> <code>field_name</code> <p>The field that failed range checking.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class RangeError(IdfKitError):\n    \"\"\"Raised when a field value is outside the valid range (eppy compatibility).\n\n    Attributes:\n        obj_type: The object type.\n        obj_name: The object name.\n        field_name: The field that failed range checking.\n    \"\"\"\n\n    def __init__(self, obj_type: str, obj_name: str, field_name: str, message: str) -&gt; None:\n        self.obj_type = obj_type\n        self.obj_name = obj_name\n        self.field_name = field_name\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.SchemaNotFoundError","title":"<code>SchemaNotFoundError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when the EpJSON schema file cannot be found.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class SchemaNotFoundError(IdfKitError):\n    \"\"\"Raised when the EpJSON schema file cannot be found.\"\"\"\n\n    def __init__(self, version: tuple[int, int, int], searched_paths: list[str] | None = None) -&gt; None:\n        self.version = version\n        self.searched_paths = searched_paths or []\n        version_str = f\"{version[0]}.{version[1]}.{version[2]}\"\n        msg = f\"Could not find Energy+.schema.epJSON for EnergyPlus {version_str}\"\n        if searched_paths:\n            msg += f\"\\nSearched in: {', '.join(searched_paths)}\"\n        super().__init__(msg)\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.SimulationError","title":"<code>SimulationError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when an EnergyPlus simulation fails.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class SimulationError(IdfKitError):\n    \"\"\"Raised when an EnergyPlus simulation fails.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        exit_code: int | None = None,\n        stderr: str | None = None,\n    ) -&gt; None:\n        self.exit_code = exit_code\n        self.stderr = stderr\n        msg = message\n        if exit_code is not None:\n            msg += f\" (exit code {exit_code})\"\n        if stderr:\n            trimmed = stderr.strip()[:500]\n            msg += f\"\\nstderr: {trimmed}\"\n        super().__init__(msg)\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.UnknownObjectTypeError","title":"<code>UnknownObjectTypeError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when an unknown object type is encountered.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class UnknownObjectTypeError(IdfKitError):\n    \"\"\"Raised when an unknown object type is encountered.\"\"\"\n\n    def __init__(self, obj_type: str) -&gt; None:\n        self.obj_type = obj_type\n        super().__init__(f\"Unknown object type: '{obj_type}'\")\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.ValidationFailedError","title":"<code>ValidationFailedError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when validation fails.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class ValidationFailedError(IdfKitError):\n    \"\"\"Raised when validation fails.\"\"\"\n\n    def __init__(self, errors: Sequence[object]) -&gt; None:\n        self.errors = list(errors)\n        msg = f\"Validation failed with {len(errors)} error(s):\\n\"\n        for i, err in enumerate(list(errors)[:5], 1):\n            msg += f\"  {i}. {err}\\n\"\n        if len(errors) &gt; 5:\n            msg += f\"  ... and {len(errors) - 5} more errors\"\n        super().__init__(msg)\n</code></pre>"},{"location":"api/exceptions/#idfkit.exceptions.VersionNotFoundError","title":"<code>VersionNotFoundError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when version cannot be detected from file.</p> Source code in <code>src/idfkit/exceptions.py</code> <pre><code>class VersionNotFoundError(IdfKitError):\n    \"\"\"Raised when version cannot be detected from file.\"\"\"\n\n    def __init__(self, filepath: str) -&gt; None:\n        self.filepath = filepath\n        super().__init__(f\"Could not detect EnergyPlus version in file: {filepath}\")\n</code></pre>"},{"location":"api/geometry/","title":"Geometry","text":"<p>Built-in 3D geometry primitives for surface area calculations, zone volume estimation, and coordinate transforms -- no external geometry dependencies required.</p> <p>Geometry utilities for IDF models.</p> <p>Provides coordinate handling and transformations without geomeppy dependency.</p>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D","title":"<code>Polygon3D</code>  <code>dataclass</code>","text":"<p>3D polygon defined by vertices.</p> <p>Computes geometric properties (area, normal, tilt, azimuth) and supports transformations (translate, rotate).</p> <p>Examples:</p> <p>A 5 m x 4 m ground-floor slab:</p> <pre><code>&gt;&gt;&gt; floor = Polygon3D([\n...     Vector3D(0, 0, 0), Vector3D(5, 0, 0),\n...     Vector3D(5, 4, 0), Vector3D(0, 4, 0),\n... ])\n&gt;&gt;&gt; floor.area\n20.0\n&gt;&gt;&gt; floor.is_horizontal\nTrue\n</code></pre> <p>A 10 m wide, 3 m high south-facing exterior wall:</p> <pre><code>&gt;&gt;&gt; south_wall = Polygon3D([\n...     Vector3D(0, 0, 0), Vector3D(10, 0, 0),\n...     Vector3D(10, 0, 3), Vector3D(0, 0, 3),\n... ])\n&gt;&gt;&gt; south_wall.area\n30.0\n&gt;&gt;&gt; south_wall.tilt\n90.0\n&gt;&gt;&gt; south_wall.azimuth\n180.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>@dataclass\nclass Polygon3D:\n    \"\"\"\n    3D polygon defined by vertices.\n\n    Computes geometric properties (area, normal, tilt, azimuth) and supports\n    transformations (translate, rotate).\n\n    Examples:\n        A 5 m x 4 m ground-floor slab:\n\n        &gt;&gt;&gt; floor = Polygon3D([\n        ...     Vector3D(0, 0, 0), Vector3D(5, 0, 0),\n        ...     Vector3D(5, 4, 0), Vector3D(0, 4, 0),\n        ... ])\n        &gt;&gt;&gt; floor.area\n        20.0\n        &gt;&gt;&gt; floor.is_horizontal\n        True\n\n        A 10 m wide, 3 m high south-facing exterior wall:\n\n        &gt;&gt;&gt; south_wall = Polygon3D([\n        ...     Vector3D(0, 0, 0), Vector3D(10, 0, 0),\n        ...     Vector3D(10, 0, 3), Vector3D(0, 0, 3),\n        ... ])\n        &gt;&gt;&gt; south_wall.area\n        30.0\n        &gt;&gt;&gt; south_wall.tilt\n        90.0\n        &gt;&gt;&gt; south_wall.azimuth\n        180.0\n    \"\"\"\n\n    vertices: list[Vector3D]\n\n    @property\n    def num_vertices(self) -&gt; int:\n        \"\"\"Number of vertices.\n\n        Examples:\n            &gt;&gt;&gt; Polygon3D([Vector3D(0,0,0), Vector3D(1,0,0), Vector3D(0,1,0)]).num_vertices\n            3\n        \"\"\"\n        return len(self.vertices)\n\n    @property\n    def normal(self) -&gt; Vector3D:\n        \"\"\"Surface normal vector.\n\n        Examples:\n            &gt;&gt;&gt; floor = Polygon3D([\n            ...     Vector3D(0, 0, 0), Vector3D(1, 0, 0),\n            ...     Vector3D(1, 1, 0), Vector3D(0, 1, 0),\n            ... ])\n            &gt;&gt;&gt; floor.normal\n            Vector3D(x=0.0, y=0.0, z=1.0)\n        \"\"\"\n        if self.num_vertices &lt; 3:\n            return Vector3D(0, 0, 1)\n\n        # Use Newell's method for robustness\n        n = Vector3D(0, 0, 0)\n        for i in range(self.num_vertices):\n            v1 = self.vertices[i]\n            v2 = self.vertices[(i + 1) % self.num_vertices]\n            n = Vector3D(\n                n.x + (v1.y - v2.y) * (v1.z + v2.z),\n                n.y + (v1.z - v2.z) * (v1.x + v2.x),\n                n.z + (v1.x - v2.x) * (v1.y + v2.y),\n            )\n        return n.normalize()\n\n    @property\n    def area(self) -&gt; float:\n        \"\"\"Surface area using cross product method.\n\n        Examples:\n            A 5 m x 5 m floor slab:\n\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(5,0,0),\n            ...     Vector3D(5,5,0), Vector3D(0,5,0),\n            ... ]).area\n            25.0\n        \"\"\"\n        if self.num_vertices &lt; 3:\n            return 0.0\n\n        # Triangulate and sum areas\n        total = Vector3D(0, 0, 0)\n        v0 = self.vertices[0]\n\n        for i in range(1, self.num_vertices - 1):\n            v1 = self.vertices[i]\n            v2 = self.vertices[i + 1]\n            edge1 = v1 - v0\n            edge2 = v2 - v0\n            cross = edge1.cross(edge2)\n            total = total + cross\n\n        return total.length() / 2.0\n\n    @property\n    def centroid(self) -&gt; Vector3D:\n        \"\"\"Geometric center.\n\n        Examples:\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(4,0,0),\n            ...     Vector3D(4,4,0), Vector3D(0,4,0),\n            ... ]).centroid\n            Vector3D(x=2.0, y=2.0, z=0.0)\n        \"\"\"\n        if not self.vertices:\n            return Vector3D.origin()\n\n        x = sum(v.x for v in self.vertices) / self.num_vertices\n        y = sum(v.y for v in self.vertices) / self.num_vertices\n        z = sum(v.z for v in self.vertices) / self.num_vertices\n        return Vector3D(x, y, z)\n\n    @property\n    def tilt(self) -&gt; float:\n        \"\"\"Surface tilt angle in degrees.\n\n        0 = facing up (horizontal roof/ceiling), 90 = vertical wall,\n        180 = facing down (horizontal floor).  Computed from the surface\n        normal using the same convention as EnergyPlus / eppy.\n\n        Examples:\n            Flat roof (tilt 0 = facing up):\n\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,3), Vector3D(5,0,3),\n            ...     Vector3D(5,5,3), Vector3D(0,5,3),\n            ... ]).tilt\n            0.0\n\n            Exterior wall (tilt 90 = vertical):\n\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(10,0,0),\n            ...     Vector3D(10,0,3), Vector3D(0,0,3),\n            ... ]).tilt\n            90.0\n        \"\"\"\n        n = self.normal\n        # Clamp to avoid floating-point issues with acos\n        clamped = max(-1.0, min(1.0, n.z))\n        return math.degrees(math.acos(clamped))\n\n    @property\n    def azimuth(self) -&gt; float:\n        \"\"\"Surface azimuth in degrees (0=north, 90=east, 180=south, 270=west).\n\n        Uses the same convention as EnergyPlus / eppy: the angle of the\n        outward normal projected onto the horizontal plane, measured\n        clockwise from north (+Y axis).\n\n        Returns 0.0 for perfectly horizontal surfaces (tilt 0 or 180).\n\n        Examples:\n            South-facing wall (normal points toward -Y):\n\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(10,0,0),\n            ...     Vector3D(10,0,3), Vector3D(0,0,3),\n            ... ]).azimuth\n            180.0\n\n            Horizontal surface has azimuth 0:\n\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(1,0,0),\n            ...     Vector3D(1,1,0), Vector3D(0,1,0),\n            ... ]).azimuth\n            0.0\n        \"\"\"\n        n = self.normal\n        # For horizontal surfaces the azimuth is undefined\n        if abs(n.x) &lt; 1e-10 and abs(n.y) &lt; 1e-10:\n            return 0.0\n        # atan2(x, y) gives the angle from +Y axis toward +X axis,\n        # which is clockwise from north -- exactly the convention we need.\n        angle = math.degrees(math.atan2(n.x, n.y))\n        if angle &lt; 0:\n            angle += 360.0\n        return angle\n\n    @property\n    def is_horizontal(self) -&gt; bool:\n        \"\"\"Check if polygon is horizontal (floor/ceiling).\n\n        Examples:\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(1,0,0),\n            ...     Vector3D(1,1,0), Vector3D(0,1,0),\n            ... ]).is_horizontal\n            True\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(1,0,0),\n            ...     Vector3D(1,0,1), Vector3D(0,0,1),\n            ... ]).is_horizontal\n            False\n        \"\"\"\n        n = self.normal\n        return abs(n.z) &gt; 0.99\n\n    @property\n    def is_vertical(self) -&gt; bool:\n        \"\"\"Check if polygon is vertical (wall).\n\n        Examples:\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(1,0,0),\n            ...     Vector3D(1,0,1), Vector3D(0,0,1),\n            ... ]).is_vertical\n            True\n            &gt;&gt;&gt; Polygon3D([\n            ...     Vector3D(0,0,0), Vector3D(1,0,0),\n            ...     Vector3D(1,1,0), Vector3D(0,1,0),\n            ... ]).is_vertical\n            False\n        \"\"\"\n        n = self.normal\n        return abs(n.z) &lt; 0.01\n\n    def translate(self, offset: Vector3D) -&gt; Polygon3D:\n        \"\"\"Return translated polygon.\n\n        Examples:\n            &gt;&gt;&gt; tri = Polygon3D([Vector3D(0,0,0), Vector3D(1,0,0), Vector3D(0,1,0)])\n            &gt;&gt;&gt; moved = tri.translate(Vector3D(10, 20, 0))\n            &gt;&gt;&gt; moved.centroid\n            Vector3D(x=10.333333333333334, y=20.333333333333332, z=0.0)\n        \"\"\"\n        return Polygon3D([v + offset for v in self.vertices])\n\n    def rotate_z(self, angle_deg: float, anchor: Vector3D | None = None) -&gt; Polygon3D:\n        \"\"\"Rotate around Z axis.\"\"\"\n        if anchor is None:\n            anchor = self.centroid\n\n        rotated: list[Vector3D] = []\n        for v in self.vertices:\n            # Translate to anchor, rotate, translate back\n            relative = v - anchor\n            rotated_rel = relative.rotate_z(angle_deg)\n            rotated.append(rotated_rel + anchor)\n\n        return Polygon3D(rotated)\n\n    def as_tuple_list(self) -&gt; list[tuple[float, float, float]]:\n        \"\"\"Return vertices as list of tuples.\n\n        Examples:\n            &gt;&gt;&gt; tri = Polygon3D([Vector3D(0,0,0), Vector3D(1,0,0), Vector3D(0,1,0)])\n            &gt;&gt;&gt; tri.as_tuple_list()\n            [(0, 0, 0), (1, 0, 0), (0, 1, 0)]\n        \"\"\"\n        return [v.as_tuple() for v in self.vertices]\n\n    @classmethod\n    def from_tuples(cls, coords: Sequence[Sequence[float]]) -&gt; Polygon3D:\n        \"\"\"Create from sequence of coordinate tuples.\n\n        Examples:\n            &gt;&gt;&gt; poly = Polygon3D.from_tuples([(0,0,0), (5,0,0), (5,5,0), (0,5,0)])\n            &gt;&gt;&gt; poly.area\n            25.0\n            &gt;&gt;&gt; poly.num_vertices\n            4\n        \"\"\"\n        return cls([Vector3D.from_tuple(c) for c in coords])\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.area","title":"<code>area</code>  <code>property</code>","text":"<p>Surface area using cross product method.</p> <p>Examples:</p> <p>A 5 m x 5 m floor slab:</p> <pre><code>&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(5,0,0),\n...     Vector3D(5,5,0), Vector3D(0,5,0),\n... ]).area\n25.0\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.azimuth","title":"<code>azimuth</code>  <code>property</code>","text":"<p>Surface azimuth in degrees (0=north, 90=east, 180=south, 270=west).</p> <p>Uses the same convention as EnergyPlus / eppy: the angle of the outward normal projected onto the horizontal plane, measured clockwise from north (+Y axis).</p> <p>Returns 0.0 for perfectly horizontal surfaces (tilt 0 or 180).</p> <p>Examples:</p> <p>South-facing wall (normal points toward -Y):</p> <pre><code>&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(10,0,0),\n...     Vector3D(10,0,3), Vector3D(0,0,3),\n... ]).azimuth\n180.0\n</code></pre> <p>Horizontal surface has azimuth 0:</p> <pre><code>&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(1,0,0),\n...     Vector3D(1,1,0), Vector3D(0,1,0),\n... ]).azimuth\n0.0\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.centroid","title":"<code>centroid</code>  <code>property</code>","text":"<p>Geometric center.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(4,0,0),\n...     Vector3D(4,4,0), Vector3D(0,4,0),\n... ]).centroid\nVector3D(x=2.0, y=2.0, z=0.0)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.is_horizontal","title":"<code>is_horizontal</code>  <code>property</code>","text":"<p>Check if polygon is horizontal (floor/ceiling).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(1,0,0),\n...     Vector3D(1,1,0), Vector3D(0,1,0),\n... ]).is_horizontal\nTrue\n&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(1,0,0),\n...     Vector3D(1,0,1), Vector3D(0,0,1),\n... ]).is_horizontal\nFalse\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.is_vertical","title":"<code>is_vertical</code>  <code>property</code>","text":"<p>Check if polygon is vertical (wall).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(1,0,0),\n...     Vector3D(1,0,1), Vector3D(0,0,1),\n... ]).is_vertical\nTrue\n&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(1,0,0),\n...     Vector3D(1,1,0), Vector3D(0,1,0),\n... ]).is_vertical\nFalse\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.normal","title":"<code>normal</code>  <code>property</code>","text":"<p>Surface normal vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; floor = Polygon3D([\n...     Vector3D(0, 0, 0), Vector3D(1, 0, 0),\n...     Vector3D(1, 1, 0), Vector3D(0, 1, 0),\n... ])\n&gt;&gt;&gt; floor.normal\nVector3D(x=0.0, y=0.0, z=1.0)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.num_vertices","title":"<code>num_vertices</code>  <code>property</code>","text":"<p>Number of vertices.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Polygon3D([Vector3D(0,0,0), Vector3D(1,0,0), Vector3D(0,1,0)]).num_vertices\n3\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.tilt","title":"<code>tilt</code>  <code>property</code>","text":"<p>Surface tilt angle in degrees.</p> <p>0 = facing up (horizontal roof/ceiling), 90 = vertical wall, 180 = facing down (horizontal floor).  Computed from the surface normal using the same convention as EnergyPlus / eppy.</p> <p>Examples:</p> <p>Flat roof (tilt 0 = facing up):</p> <pre><code>&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,3), Vector3D(5,0,3),\n...     Vector3D(5,5,3), Vector3D(0,5,3),\n... ]).tilt\n0.0\n</code></pre> <p>Exterior wall (tilt 90 = vertical):</p> <pre><code>&gt;&gt;&gt; Polygon3D([\n...     Vector3D(0,0,0), Vector3D(10,0,0),\n...     Vector3D(10,0,3), Vector3D(0,0,3),\n... ]).tilt\n90.0\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.as_tuple_list","title":"<code>as_tuple_list()</code>","text":"<p>Return vertices as list of tuples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tri = Polygon3D([Vector3D(0,0,0), Vector3D(1,0,0), Vector3D(0,1,0)])\n&gt;&gt;&gt; tri.as_tuple_list()\n[(0, 0, 0), (1, 0, 0), (0, 1, 0)]\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def as_tuple_list(self) -&gt; list[tuple[float, float, float]]:\n    \"\"\"Return vertices as list of tuples.\n\n    Examples:\n        &gt;&gt;&gt; tri = Polygon3D([Vector3D(0,0,0), Vector3D(1,0,0), Vector3D(0,1,0)])\n        &gt;&gt;&gt; tri.as_tuple_list()\n        [(0, 0, 0), (1, 0, 0), (0, 1, 0)]\n    \"\"\"\n    return [v.as_tuple() for v in self.vertices]\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.from_tuples","title":"<code>from_tuples(coords)</code>  <code>classmethod</code>","text":"<p>Create from sequence of coordinate tuples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; poly = Polygon3D.from_tuples([(0,0,0), (5,0,0), (5,5,0), (0,5,0)])\n&gt;&gt;&gt; poly.area\n25.0\n&gt;&gt;&gt; poly.num_vertices\n4\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>@classmethod\ndef from_tuples(cls, coords: Sequence[Sequence[float]]) -&gt; Polygon3D:\n    \"\"\"Create from sequence of coordinate tuples.\n\n    Examples:\n        &gt;&gt;&gt; poly = Polygon3D.from_tuples([(0,0,0), (5,0,0), (5,5,0), (0,5,0)])\n        &gt;&gt;&gt; poly.area\n        25.0\n        &gt;&gt;&gt; poly.num_vertices\n        4\n    \"\"\"\n    return cls([Vector3D.from_tuple(c) for c in coords])\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.rotate_z","title":"<code>rotate_z(angle_deg, anchor=None)</code>","text":"<p>Rotate around Z axis.</p> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def rotate_z(self, angle_deg: float, anchor: Vector3D | None = None) -&gt; Polygon3D:\n    \"\"\"Rotate around Z axis.\"\"\"\n    if anchor is None:\n        anchor = self.centroid\n\n    rotated: list[Vector3D] = []\n    for v in self.vertices:\n        # Translate to anchor, rotate, translate back\n        relative = v - anchor\n        rotated_rel = relative.rotate_z(angle_deg)\n        rotated.append(rotated_rel + anchor)\n\n    return Polygon3D(rotated)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Polygon3D.translate","title":"<code>translate(offset)</code>","text":"<p>Return translated polygon.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tri = Polygon3D([Vector3D(0,0,0), Vector3D(1,0,0), Vector3D(0,1,0)])\n&gt;&gt;&gt; moved = tri.translate(Vector3D(10, 20, 0))\n&gt;&gt;&gt; moved.centroid\nVector3D(x=10.333333333333334, y=20.333333333333332, z=0.0)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def translate(self, offset: Vector3D) -&gt; Polygon3D:\n    \"\"\"Return translated polygon.\n\n    Examples:\n        &gt;&gt;&gt; tri = Polygon3D([Vector3D(0,0,0), Vector3D(1,0,0), Vector3D(0,1,0)])\n        &gt;&gt;&gt; moved = tri.translate(Vector3D(10, 20, 0))\n        &gt;&gt;&gt; moved.centroid\n        Vector3D(x=10.333333333333334, y=20.333333333333332, z=0.0)\n    \"\"\"\n    return Polygon3D([v + offset for v in self.vertices])\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D","title":"<code>Vector3D</code>  <code>dataclass</code>","text":"<p>Immutable 3D vector.</p> <p>Supports arithmetic operations (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, unary <code>-</code>) and common vector operations (dot product, cross product, normalization).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; v = Vector3D(1.0, 2.0, 3.0)\n&gt;&gt;&gt; v.x, v.y, v.z\n(1.0, 2.0, 3.0)\n</code></pre> <p>Vectors support arithmetic:</p> <pre><code>&gt;&gt;&gt; Vector3D(1, 2, 3) + Vector3D(4, 5, 6)\nVector3D(x=5, y=7, z=9)\n&gt;&gt;&gt; Vector3D(3, 0, 0) * 2\nVector3D(x=6, y=0, z=0)\n&gt;&gt;&gt; -Vector3D(1, 0, 0)\nVector3D(x=-1, y=0, z=0)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass Vector3D:\n    \"\"\"\n    Immutable 3D vector.\n\n    Supports arithmetic operations (``+``, ``-``, ``*``, ``/``, unary ``-``)\n    and common vector operations (dot product, cross product, normalization).\n\n    Examples:\n        &gt;&gt;&gt; v = Vector3D(1.0, 2.0, 3.0)\n        &gt;&gt;&gt; v.x, v.y, v.z\n        (1.0, 2.0, 3.0)\n\n        Vectors support arithmetic:\n\n        &gt;&gt;&gt; Vector3D(1, 2, 3) + Vector3D(4, 5, 6)\n        Vector3D(x=5, y=7, z=9)\n        &gt;&gt;&gt; Vector3D(3, 0, 0) * 2\n        Vector3D(x=6, y=0, z=0)\n        &gt;&gt;&gt; -Vector3D(1, 0, 0)\n        Vector3D(x=-1, y=0, z=0)\n    \"\"\"\n\n    x: float\n    y: float\n    z: float\n\n    def __add__(self, other: Vector3D) -&gt; Vector3D:\n        return Vector3D(self.x + other.x, self.y + other.y, self.z + other.z)\n\n    def __sub__(self, other: Vector3D) -&gt; Vector3D:\n        return Vector3D(self.x - other.x, self.y - other.y, self.z - other.z)\n\n    def __mul__(self, scalar: float) -&gt; Vector3D:\n        return Vector3D(self.x * scalar, self.y * scalar, self.z * scalar)\n\n    def __rmul__(self, scalar: float) -&gt; Vector3D:\n        return self * scalar\n\n    def __truediv__(self, scalar: float) -&gt; Vector3D:\n        return Vector3D(self.x / scalar, self.y / scalar, self.z / scalar)\n\n    def __neg__(self) -&gt; Vector3D:\n        return Vector3D(-self.x, -self.y, -self.z)\n\n    def dot(self, other: Vector3D) -&gt; float:\n        \"\"\"Dot product.\n\n        Examples:\n            &gt;&gt;&gt; Vector3D(1, 0, 0).dot(Vector3D(0, 1, 0))\n            0\n            &gt;&gt;&gt; Vector3D(3.0, 0.0, 0.0).dot(Vector3D(4.0, 0.0, 0.0))\n            12.0\n        \"\"\"\n        return self.x * other.x + self.y * other.y + self.z * other.z\n\n    def cross(self, other: Vector3D) -&gt; Vector3D:\n        \"\"\"Cross product.\n\n        Examples:\n            &gt;&gt;&gt; Vector3D(1, 0, 0).cross(Vector3D(0, 1, 0))\n            Vector3D(x=0, y=0, z=1)\n            &gt;&gt;&gt; Vector3D(3, 0, 0).cross(Vector3D(0, 4, 0))\n            Vector3D(x=0, y=0, z=12)\n        \"\"\"\n        return Vector3D(\n            self.y * other.z - self.z * other.y,\n            self.z * other.x - self.x * other.z,\n            self.x * other.y - self.y * other.x,\n        )\n\n    def length(self) -&gt; float:\n        \"\"\"Vector magnitude.\n\n        Examples:\n            &gt;&gt;&gt; Vector3D(3, 4, 0).length()\n            5.0\n            &gt;&gt;&gt; Vector3D(1, 0, 0).length()\n            1.0\n        \"\"\"\n        return math.sqrt(self.x**2 + self.y**2 + self.z**2)\n\n    def normalize(self) -&gt; Vector3D:\n        \"\"\"Return unit vector.\n\n        Examples:\n            &gt;&gt;&gt; Vector3D(3, 0, 0).normalize()\n            Vector3D(x=1.0, y=0.0, z=0.0)\n            &gt;&gt;&gt; Vector3D(0, 0, 5).normalize()\n            Vector3D(x=0.0, y=0.0, z=1.0)\n        \"\"\"\n        mag = self.length()\n        if mag == 0:\n            return Vector3D(0, 0, 0)\n        return self / mag\n\n    def rotate_z(self, angle_deg: float) -&gt; Vector3D:\n        \"\"\"Rotate around Z axis by angle in degrees.\n\n        Examples:\n            &gt;&gt;&gt; v = Vector3D(1, 0, 0).rotate_z(90)\n            &gt;&gt;&gt; round(v.x, 10), round(v.y, 10)\n            (0.0, 1.0)\n            &gt;&gt;&gt; Vector3D(1, 0, 5).rotate_z(180)  # doctest: +SKIP\n            Vector3D(x=-1.0, y=0.0, z=5.0)\n        \"\"\"\n        angle_rad = math.radians(angle_deg)\n        cos_a = math.cos(angle_rad)\n        sin_a = math.sin(angle_rad)\n        return Vector3D(\n            self.x * cos_a - self.y * sin_a,\n            self.x * sin_a + self.y * cos_a,\n            self.z,\n        )\n\n    def as_tuple(self) -&gt; tuple[float, float, float]:\n        \"\"\"Return as tuple.\n\n        Examples:\n            &gt;&gt;&gt; Vector3D(1.0, 2.0, 3.0).as_tuple()\n            (1.0, 2.0, 3.0)\n            &gt;&gt;&gt; Vector3D.origin().as_tuple()\n            (0.0, 0.0, 0.0)\n        \"\"\"\n        return (self.x, self.y, self.z)\n\n    @classmethod\n    def from_tuple(cls, t: Sequence[float]) -&gt; Vector3D:\n        \"\"\"Create from tuple or list.\n\n        Examples:\n            &gt;&gt;&gt; Vector3D.from_tuple((1.0, 2.0, 3.0))\n            Vector3D(x=1.0, y=2.0, z=3.0)\n            &gt;&gt;&gt; Vector3D.from_tuple([0, 0, 0])\n            Vector3D(x=0.0, y=0.0, z=0.0)\n        \"\"\"\n        return cls(float(t[0]), float(t[1]), float(t[2]))\n\n    @classmethod\n    def origin(cls) -&gt; Vector3D:\n        \"\"\"Return origin vector.\n\n        Examples:\n            &gt;&gt;&gt; Vector3D.origin()\n            Vector3D(x=0.0, y=0.0, z=0.0)\n        \"\"\"\n        return cls(0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D.as_tuple","title":"<code>as_tuple()</code>","text":"<p>Return as tuple.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Vector3D(1.0, 2.0, 3.0).as_tuple()\n(1.0, 2.0, 3.0)\n&gt;&gt;&gt; Vector3D.origin().as_tuple()\n(0.0, 0.0, 0.0)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def as_tuple(self) -&gt; tuple[float, float, float]:\n    \"\"\"Return as tuple.\n\n    Examples:\n        &gt;&gt;&gt; Vector3D(1.0, 2.0, 3.0).as_tuple()\n        (1.0, 2.0, 3.0)\n        &gt;&gt;&gt; Vector3D.origin().as_tuple()\n        (0.0, 0.0, 0.0)\n    \"\"\"\n    return (self.x, self.y, self.z)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D.cross","title":"<code>cross(other)</code>","text":"<p>Cross product.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Vector3D(1, 0, 0).cross(Vector3D(0, 1, 0))\nVector3D(x=0, y=0, z=1)\n&gt;&gt;&gt; Vector3D(3, 0, 0).cross(Vector3D(0, 4, 0))\nVector3D(x=0, y=0, z=12)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def cross(self, other: Vector3D) -&gt; Vector3D:\n    \"\"\"Cross product.\n\n    Examples:\n        &gt;&gt;&gt; Vector3D(1, 0, 0).cross(Vector3D(0, 1, 0))\n        Vector3D(x=0, y=0, z=1)\n        &gt;&gt;&gt; Vector3D(3, 0, 0).cross(Vector3D(0, 4, 0))\n        Vector3D(x=0, y=0, z=12)\n    \"\"\"\n    return Vector3D(\n        self.y * other.z - self.z * other.y,\n        self.z * other.x - self.x * other.z,\n        self.x * other.y - self.y * other.x,\n    )\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D.dot","title":"<code>dot(other)</code>","text":"<p>Dot product.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Vector3D(1, 0, 0).dot(Vector3D(0, 1, 0))\n0\n&gt;&gt;&gt; Vector3D(3.0, 0.0, 0.0).dot(Vector3D(4.0, 0.0, 0.0))\n12.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def dot(self, other: Vector3D) -&gt; float:\n    \"\"\"Dot product.\n\n    Examples:\n        &gt;&gt;&gt; Vector3D(1, 0, 0).dot(Vector3D(0, 1, 0))\n        0\n        &gt;&gt;&gt; Vector3D(3.0, 0.0, 0.0).dot(Vector3D(4.0, 0.0, 0.0))\n        12.0\n    \"\"\"\n    return self.x * other.x + self.y * other.y + self.z * other.z\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D.from_tuple","title":"<code>from_tuple(t)</code>  <code>classmethod</code>","text":"<p>Create from tuple or list.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Vector3D.from_tuple((1.0, 2.0, 3.0))\nVector3D(x=1.0, y=2.0, z=3.0)\n&gt;&gt;&gt; Vector3D.from_tuple([0, 0, 0])\nVector3D(x=0.0, y=0.0, z=0.0)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>@classmethod\ndef from_tuple(cls, t: Sequence[float]) -&gt; Vector3D:\n    \"\"\"Create from tuple or list.\n\n    Examples:\n        &gt;&gt;&gt; Vector3D.from_tuple((1.0, 2.0, 3.0))\n        Vector3D(x=1.0, y=2.0, z=3.0)\n        &gt;&gt;&gt; Vector3D.from_tuple([0, 0, 0])\n        Vector3D(x=0.0, y=0.0, z=0.0)\n    \"\"\"\n    return cls(float(t[0]), float(t[1]), float(t[2]))\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D.length","title":"<code>length()</code>","text":"<p>Vector magnitude.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Vector3D(3, 4, 0).length()\n5.0\n&gt;&gt;&gt; Vector3D(1, 0, 0).length()\n1.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def length(self) -&gt; float:\n    \"\"\"Vector magnitude.\n\n    Examples:\n        &gt;&gt;&gt; Vector3D(3, 4, 0).length()\n        5.0\n        &gt;&gt;&gt; Vector3D(1, 0, 0).length()\n        1.0\n    \"\"\"\n    return math.sqrt(self.x**2 + self.y**2 + self.z**2)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D.normalize","title":"<code>normalize()</code>","text":"<p>Return unit vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Vector3D(3, 0, 0).normalize()\nVector3D(x=1.0, y=0.0, z=0.0)\n&gt;&gt;&gt; Vector3D(0, 0, 5).normalize()\nVector3D(x=0.0, y=0.0, z=1.0)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def normalize(self) -&gt; Vector3D:\n    \"\"\"Return unit vector.\n\n    Examples:\n        &gt;&gt;&gt; Vector3D(3, 0, 0).normalize()\n        Vector3D(x=1.0, y=0.0, z=0.0)\n        &gt;&gt;&gt; Vector3D(0, 0, 5).normalize()\n        Vector3D(x=0.0, y=0.0, z=1.0)\n    \"\"\"\n    mag = self.length()\n    if mag == 0:\n        return Vector3D(0, 0, 0)\n    return self / mag\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D.origin","title":"<code>origin()</code>  <code>classmethod</code>","text":"<p>Return origin vector.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Vector3D.origin()\nVector3D(x=0.0, y=0.0, z=0.0)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>@classmethod\ndef origin(cls) -&gt; Vector3D:\n    \"\"\"Return origin vector.\n\n    Examples:\n        &gt;&gt;&gt; Vector3D.origin()\n        Vector3D(x=0.0, y=0.0, z=0.0)\n    \"\"\"\n    return cls(0.0, 0.0, 0.0)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.Vector3D.rotate_z","title":"<code>rotate_z(angle_deg)</code>","text":"<p>Rotate around Z axis by angle in degrees.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; v = Vector3D(1, 0, 0).rotate_z(90)\n&gt;&gt;&gt; round(v.x, 10), round(v.y, 10)\n(0.0, 1.0)\n&gt;&gt;&gt; Vector3D(1, 0, 5).rotate_z(180)\nVector3D(x=-1.0, y=0.0, z=5.0)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def rotate_z(self, angle_deg: float) -&gt; Vector3D:\n    \"\"\"Rotate around Z axis by angle in degrees.\n\n    Examples:\n        &gt;&gt;&gt; v = Vector3D(1, 0, 0).rotate_z(90)\n        &gt;&gt;&gt; round(v.x, 10), round(v.y, 10)\n        (0.0, 1.0)\n        &gt;&gt;&gt; Vector3D(1, 0, 5).rotate_z(180)  # doctest: +SKIP\n        Vector3D(x=-1.0, y=0.0, z=5.0)\n    \"\"\"\n    angle_rad = math.radians(angle_deg)\n    cos_a = math.cos(angle_rad)\n    sin_a = math.sin(angle_rad)\n    return Vector3D(\n        self.x * cos_a - self.y * sin_a,\n        self.x * sin_a + self.y * cos_a,\n        self.z,\n    )\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.calculate_surface_area","title":"<code>calculate_surface_area(surface)</code>","text":"<p>Calculate the area of a surface in m\u00b2.</p> <p>Examples:</p> <p>Area of a 10 m wide, 3 m high exterior wall:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n...     validate=False)\n&gt;&gt;&gt; calculate_surface_area(wall)\n30.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def calculate_surface_area(surface: IDFObject) -&gt; float:\n    \"\"\"Calculate the area of a surface in m\u00b2.\n\n    Examples:\n        Area of a 10 m wide, 3 m high exterior wall:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n        ...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n        ...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n        ...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n        ...     validate=False)\n        &gt;&gt;&gt; calculate_surface_area(wall)\n        30.0\n    \"\"\"\n    coords = get_surface_coords(surface)\n    return coords.area if coords else 0.0\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.calculate_surface_azimuth","title":"<code>calculate_surface_azimuth(surface)</code>","text":"<p>Calculate the azimuth of a surface in degrees (eppy compatibility).</p> <p>0 = north, 90 = east, 180 = south, 270 = west.  Useful for identifying solar exposure for glazing and shading studies.</p> <p>Examples:</p> <p>Confirm a wall faces south (azimuth 180):</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"SouthWall\",\n...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n...     validate=False)\n&gt;&gt;&gt; calculate_surface_azimuth(wall)\n180.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def calculate_surface_azimuth(surface: IDFObject) -&gt; float:\n    \"\"\"Calculate the azimuth of a surface in degrees (eppy compatibility).\n\n    0 = north, 90 = east, 180 = south, 270 = west.  Useful for\n    identifying solar exposure for glazing and shading studies.\n\n    Examples:\n        Confirm a wall faces south (azimuth 180):\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"SouthWall\",\n        ...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n        ...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n        ...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n        ...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n        ...     validate=False)\n        &gt;&gt;&gt; calculate_surface_azimuth(wall)\n        180.0\n    \"\"\"\n    coords = get_surface_coords(surface)\n    return coords.azimuth if coords else 0.0\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.calculate_surface_tilt","title":"<code>calculate_surface_tilt(surface)</code>","text":"<p>Calculate the tilt of a surface in degrees (eppy compatibility).</p> <p>0 = facing up (roof), 90 = vertical (wall), 180 = facing down (floor).</p> <p>Examples:</p> <p>Verify that an exterior wall is vertical:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n...     validate=False)\n&gt;&gt;&gt; calculate_surface_tilt(wall)\n90.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def calculate_surface_tilt(surface: IDFObject) -&gt; float:\n    \"\"\"Calculate the tilt of a surface in degrees (eppy compatibility).\n\n    0 = facing up (roof), 90 = vertical (wall), 180 = facing down (floor).\n\n    Examples:\n        Verify that an exterior wall is vertical:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n        ...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n        ...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n        ...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n        ...     validate=False)\n        &gt;&gt;&gt; calculate_surface_tilt(wall)\n        90.0\n    \"\"\"\n    coords = get_surface_coords(surface)\n    return coords.tilt if coords else 0.0\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.calculate_zone_ceiling_area","title":"<code>calculate_zone_ceiling_area(doc, zone_name)</code>","text":"<p>Calculate the total ceiling/roof area of a zone (eppy compatibility).</p> <p>Sums the area of all surfaces whose <code>surface_type</code> is <code>\"Ceiling\"</code> or <code>\"Roof\"</code> in the given zone.</p> <p>Examples:</p> <p>Calculate the ceiling area of a 5 m x 4 m office at z=3 m:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Office\")\nZone('Office')\n&gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"Office_Ceiling\",\n...     surface_type=\"Ceiling\", construction_name=\"\", zone_name=\"Office\",\n...     outside_boundary_condition=\"Outdoors\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=4, vertex_2_z_coordinate=3,\n...     vertex_3_x_coordinate=5, vertex_3_y_coordinate=4, vertex_3_z_coordinate=3,\n...     vertex_4_x_coordinate=5, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n...     validate=False)\nBuildingSurface:Detailed('Office_Ceiling')\n&gt;&gt;&gt; calculate_zone_ceiling_area(model, \"Office\")\n20.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def calculate_zone_ceiling_area(doc: IDFDocument, zone_name: str) -&gt; float:\n    \"\"\"Calculate the total ceiling/roof area of a zone (eppy compatibility).\n\n    Sums the area of all surfaces whose ``surface_type`` is ``\"Ceiling\"``\n    or ``\"Roof\"`` in the given zone.\n\n    Examples:\n        Calculate the ceiling area of a 5 m x 4 m office at z=3 m:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n        Zone('Office')\n        &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"Office_Ceiling\",\n        ...     surface_type=\"Ceiling\", construction_name=\"\", zone_name=\"Office\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n        ...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=4, vertex_2_z_coordinate=3,\n        ...     vertex_3_x_coordinate=5, vertex_3_y_coordinate=4, vertex_3_z_coordinate=3,\n        ...     vertex_4_x_coordinate=5, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n        ...     validate=False)  # doctest: +ELLIPSIS\n        BuildingSurface:Detailed('Office_Ceiling')\n        &gt;&gt;&gt; calculate_zone_ceiling_area(model, \"Office\")\n        20.0\n    \"\"\"\n    total_area = 0.0\n\n    for surface in doc[\"BuildingSurface:Detailed\"]:\n        if (getattr(surface, \"zone_name\", None) or \"\").upper() != zone_name.upper():\n            continue\n\n        surface_type = getattr(surface, \"surface_type\", None) or \"\"\n        if surface_type and surface_type.lower() in (\"ceiling\", \"roof\"):\n            total_area += calculate_surface_area(surface)\n\n    return total_area\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.calculate_zone_floor_area","title":"<code>calculate_zone_floor_area(doc, zone_name)</code>","text":"<p>Calculate the total floor area of a zone.</p> <p>Sums the area of all <code>BuildingSurface:Detailed</code> objects whose <code>surface_type</code> is <code>\"Floor\"</code> and whose <code>zone_name</code> matches.</p> <p>Examples:</p> <p>Calculate the floor area of a 5 m x 4 m office:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Office\")\nZone('Office')\n&gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"Office_Floor\",\n...     surface_type=\"Floor\", construction_name=\"\", zone_name=\"Office\",\n...     outside_boundary_condition=\"Ground\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=0,\n...     vertex_2_x_coordinate=5, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n...     vertex_3_x_coordinate=5, vertex_3_y_coordinate=4, vertex_3_z_coordinate=0,\n...     vertex_4_x_coordinate=0, vertex_4_y_coordinate=4, vertex_4_z_coordinate=0,\n...     validate=False)\nBuildingSurface:Detailed('Office_Floor')\n&gt;&gt;&gt; calculate_zone_floor_area(model, \"Office\")\n20.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def calculate_zone_floor_area(doc: IDFDocument, zone_name: str) -&gt; float:\n    \"\"\"Calculate the total floor area of a zone.\n\n    Sums the area of all ``BuildingSurface:Detailed`` objects whose\n    ``surface_type`` is ``\"Floor\"`` and whose ``zone_name`` matches.\n\n    Examples:\n        Calculate the floor area of a 5 m x 4 m office:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n        Zone('Office')\n        &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"Office_Floor\",\n        ...     surface_type=\"Floor\", construction_name=\"\", zone_name=\"Office\",\n        ...     outside_boundary_condition=\"Ground\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=0,\n        ...     vertex_2_x_coordinate=5, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n        ...     vertex_3_x_coordinate=5, vertex_3_y_coordinate=4, vertex_3_z_coordinate=0,\n        ...     vertex_4_x_coordinate=0, vertex_4_y_coordinate=4, vertex_4_z_coordinate=0,\n        ...     validate=False)  # doctest: +ELLIPSIS\n        BuildingSurface:Detailed('Office_Floor')\n        &gt;&gt;&gt; calculate_zone_floor_area(model, \"Office\")\n        20.0\n    \"\"\"\n    total_area = 0.0\n\n    for surface in doc[\"BuildingSurface:Detailed\"]:\n        if (getattr(surface, \"zone_name\", None) or \"\").upper() != zone_name.upper():\n            continue\n\n        surface_type = getattr(surface, \"surface_type\", None) or \"\"\n        if surface_type and surface_type.lower() == \"floor\":\n            total_area += calculate_surface_area(surface)\n\n    return total_area\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.calculate_zone_height","title":"<code>calculate_zone_height(doc, zone_name)</code>","text":"<p>Calculate the height of a zone from its surfaces.</p> <p>Returns the difference between the maximum and minimum Z coordinates across all surfaces belonging to the zone.</p> <p>Examples:</p> <p>Determine the floor-to-ceiling height of a 3 m tall office:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Office\")\nZone('Office')\n&gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"Office\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n...     vertex_3_x_coordinate=5, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n...     vertex_4_x_coordinate=5, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n...     validate=False)\nBuildingSurface:Detailed('South_Wall')\n&gt;&gt;&gt; calculate_zone_height(model, \"Office\")\n3.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def calculate_zone_height(doc: IDFDocument, zone_name: str) -&gt; float:\n    \"\"\"Calculate the height of a zone from its surfaces.\n\n    Returns the difference between the maximum and minimum Z coordinates\n    across all surfaces belonging to the zone.\n\n    Examples:\n        Determine the floor-to-ceiling height of a 3 m tall office:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Office\")  # doctest: +ELLIPSIS\n        Zone('Office')\n        &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"Office\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n        ...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n        ...     vertex_3_x_coordinate=5, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n        ...     vertex_4_x_coordinate=5, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n        ...     validate=False)  # doctest: +ELLIPSIS\n        BuildingSurface:Detailed('South_Wall')\n        &gt;&gt;&gt; calculate_zone_height(model, \"Office\")\n        3.0\n    \"\"\"\n    z_min = float(\"inf\")\n    z_max = float(\"-inf\")\n\n    for surface in doc[\"BuildingSurface:Detailed\"]:\n        if (getattr(surface, \"zone_name\", None) or \"\").upper() != zone_name.upper():\n            continue\n\n        coords = get_surface_coords(surface)\n        if coords is None:\n            continue\n\n        for v in coords.vertices:\n            z_min = min(z_min, v.z)\n            z_max = max(z_max, v.z)\n\n    if z_min == float(\"inf\"):\n        return 0.0\n    return z_max - z_min\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.calculate_zone_volume","title":"<code>calculate_zone_volume(doc, zone_name)</code>","text":"<p>Calculate the volume of a zone from its surfaces.</p> <p>Uses the divergence theorem to compute volume from surface polygons. Returns 0.0 if the zone has no surfaces.</p> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def calculate_zone_volume(doc: IDFDocument, zone_name: str) -&gt; float:\n    \"\"\"\n    Calculate the volume of a zone from its surfaces.\n\n    Uses the divergence theorem to compute volume from surface polygons.\n    Returns 0.0 if the zone has no surfaces.\n    \"\"\"\n    volume = 0.0\n\n    for surface in doc[\"BuildingSurface:Detailed\"]:\n        if (getattr(surface, \"zone_name\", None) or \"\").upper() != zone_name.upper():\n            continue\n\n        coords = get_surface_coords(surface)\n        if coords is None or coords.num_vertices &lt; 3:\n            continue\n\n        # Contribution to volume using signed volume of tetrahedra\n        centroid = coords.centroid\n        for i in range(coords.num_vertices):\n            v1 = coords.vertices[i]\n            v2 = coords.vertices[(i + 1) % coords.num_vertices]\n\n            # Volume of tetrahedron with origin\n            volume += v1.dot(v2.cross(centroid)) / 6.0\n\n    return abs(volume)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.get_surface_coords","title":"<code>get_surface_coords(surface)</code>","text":"<p>Extract coordinates from a surface object.</p> <p>Works with BuildingSurface:Detailed, FenestrationSurface:Detailed, etc. Supports both field naming conventions:</p> <ul> <li>Classic/programmatic: <code>vertex_1_x_coordinate</code>, <code>vertex_2_x_coordinate</code>, ...</li> <li>epJSON schema: <code>vertex_x_coordinate</code>, <code>vertex_x_coordinate_2</code>, ...</li> </ul> <p>Examples:</p> <p>Extract geometry from a 10 m x 3 m south-facing exterior wall:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n...     validate=False)\n&gt;&gt;&gt; poly = get_surface_coords(wall)\n&gt;&gt;&gt; poly.area\n30.0\n&gt;&gt;&gt; poly.azimuth\n180.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def get_surface_coords(surface: IDFObject) -&gt; Polygon3D | None:\n    \"\"\"\n    Extract coordinates from a surface object.\n\n    Works with BuildingSurface:Detailed, FenestrationSurface:Detailed, etc.\n    Supports both field naming conventions:\n\n    - Classic/programmatic: ``vertex_1_x_coordinate``, ``vertex_2_x_coordinate``, ...\n    - epJSON schema: ``vertex_x_coordinate``, ``vertex_x_coordinate_2``, ...\n\n    Examples:\n        Extract geometry from a 10 m x 3 m south-facing exterior wall:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n        ...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n        ...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n        ...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n        ...     validate=False)\n        &gt;&gt;&gt; poly = get_surface_coords(wall)\n        &gt;&gt;&gt; poly.area\n        30.0\n        &gt;&gt;&gt; poly.azimuth\n        180.0\n    \"\"\"\n    vertices = _get_vertices_classic(surface)\n    if not vertices:\n        vertices = _get_vertices_schema(surface)\n    if len(vertices) &lt; 3:\n        return None\n    return Polygon3D(vertices)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.get_zone_origin","title":"<code>get_zone_origin(zone)</code>","text":"<p>Get the origin point of a zone.</p> <p>Examples:</p> <p>A second-floor zone offset 3.5 m above ground:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; zone = model.add(\"Zone\", \"Floor2_Office\",\n...     x_origin=10.0, y_origin=20.0, z_origin=3.5)\n&gt;&gt;&gt; get_zone_origin(zone)\nVector3D(x=10.0, y=20.0, z=3.5)\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def get_zone_origin(zone: IDFObject) -&gt; Vector3D:\n    \"\"\"Get the origin point of a zone.\n\n    Examples:\n        A second-floor zone offset 3.5 m above ground:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; zone = model.add(\"Zone\", \"Floor2_Office\",\n        ...     x_origin=10.0, y_origin=20.0, z_origin=3.5)\n        &gt;&gt;&gt; get_zone_origin(zone)\n        Vector3D(x=10.0, y=20.0, z=3.5)\n    \"\"\"\n    x = getattr(zone, \"x_origin\", 0) or 0\n    y = getattr(zone, \"y_origin\", 0) or 0\n    z = getattr(zone, \"z_origin\", 0) or 0\n    return Vector3D(float(x), float(y), float(z))\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.get_zone_rotation","title":"<code>get_zone_rotation(zone)</code>","text":"<p>Get the rotation angle of a zone in degrees.</p> <p>Examples:</p> <p>A zone rotated 45 degrees from true north (common for buildings aligned to a street grid):</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; zone = model.add(\"Zone\", \"Corner_Office\",\n...     direction_of_relative_north=45.0)\n&gt;&gt;&gt; get_zone_rotation(zone)\n45.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def get_zone_rotation(zone: IDFObject) -&gt; float:\n    \"\"\"Get the rotation angle of a zone in degrees.\n\n    Examples:\n        A zone rotated 45 degrees from true north (common for\n        buildings aligned to a street grid):\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; zone = model.add(\"Zone\", \"Corner_Office\",\n        ...     direction_of_relative_north=45.0)\n        &gt;&gt;&gt; get_zone_rotation(zone)\n        45.0\n    \"\"\"\n    angle = getattr(zone, \"direction_of_relative_north\", 0)\n    return float(angle) if angle else 0.0\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.intersect_match","title":"<code>intersect_match(doc)</code>","text":"<p>Match adjacent surfaces and set boundary conditions.</p> <p>Scans all <code>BuildingSurface:Detailed</code> walls and identifies pairs whose polygons are coincident (same plane, overlapping area).  For each matched pair, the boundary conditions are updated so the surfaces reference each other.</p> <p>This is the idfkit equivalent of geomeppy's <code>idf.intersect_match()</code>.</p> <p>The algorithm is O(n\u00b2) over exterior walls but uses normal-vector and centroid-distance filters to skip most comparisons quickly.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to modify in-place.</p> required <p>Note</p> <p>This implementation handles the common case of full-overlap matching (same-size surfaces on opposite sides of a shared wall).  Partial intersection and surface splitting are not implemented \u2014 use EnergyPlus' <code>ExpandObjects</code> preprocessor or manual surface definition for complex cases.</p> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def intersect_match(doc: IDFDocument) -&gt; None:  # noqa: C901\n    \"\"\"Match adjacent surfaces and set boundary conditions.\n\n    Scans all ``BuildingSurface:Detailed`` walls and identifies pairs\n    whose polygons are coincident (same plane, overlapping area).  For\n    each matched pair, the boundary conditions are updated so the\n    surfaces reference each other.\n\n    This is the idfkit equivalent of geomeppy's\n    ``idf.intersect_match()``.\n\n    The algorithm is O(n\u00b2) over exterior walls but uses normal-vector\n    and centroid-distance filters to skip most comparisons quickly.\n\n    Args:\n        doc: The document to modify in-place.\n\n    !!! note\n        This implementation handles the common case of full-overlap\n        matching (same-size surfaces on opposite sides of a shared\n        wall).  Partial intersection and surface splitting are **not**\n        implemented \u2014 use EnergyPlus' ``ExpandObjects`` preprocessor\n        or manual surface definition for complex cases.\n    \"\"\"\n    walls: list[IDFObject] = []\n    for surface in doc[\"BuildingSurface:Detailed\"]:\n        st = getattr(surface, \"surface_type\", None) or \"\"\n        if st.upper() == \"WALL\":\n            walls.append(surface)\n\n    matched: set[int] = set()\n\n    for i, wall_a in enumerate(walls):\n        if id(wall_a) in matched:\n            continue\n        coords_a = get_surface_coords(wall_a)\n        if coords_a is None:\n            continue\n\n        normal_a = coords_a.normal\n        centroid_a = coords_a.centroid\n\n        for j in range(i + 1, len(walls)):\n            wall_b = walls[j]\n            if id(wall_b) in matched:\n                continue\n            coords_b = get_surface_coords(wall_b)\n            if coords_b is None:\n                continue\n\n            # Quick filter: normals must be anti-parallel\n            normal_b = coords_b.normal\n            dot = normal_a.dot(normal_b)\n            if dot &gt; -0.99:\n                continue\n\n            # Quick filter: centroids must be close\n            centroid_b = coords_b.centroid\n            dist = (centroid_a - centroid_b).length()\n            if dist &gt; 1.0:  # Allow 1 m tolerance for thick walls\n                continue\n\n            # Check coplanarity: centroid_b must lie on plane of A\n            d = (centroid_b - centroid_a).dot(normal_a)\n            if abs(d) &gt; 0.5:  # Allow 0.5 m for wall thickness\n                continue\n\n            # Check area similarity\n            area_a = coords_a.area\n            area_b = coords_b.area\n            if area_a &lt; 1e-6:\n                continue\n            ratio = area_b / area_a\n            if ratio &lt; 0.9 or ratio &gt; 1.1:\n                continue\n\n            # Match found \u2014 update boundary conditions\n            wall_a.outside_boundary_condition = \"Surface\"\n            wall_a.outside_boundary_condition_object = wall_b.name\n            wall_a.sun_exposure = \"NoSun\"\n            wall_a.wind_exposure = \"NoWind\"\n\n            wall_b.outside_boundary_condition = \"Surface\"\n            wall_b.outside_boundary_condition_object = wall_a.name\n            wall_b.sun_exposure = \"NoSun\"\n            wall_b.wind_exposure = \"NoWind\"\n\n            matched.add(id(wall_a))\n            matched.add(id(wall_b))\n            break\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.rotate_building","title":"<code>rotate_building(doc, angle_deg, anchor=None)</code>","text":"<p>Rotate all building surfaces around the Z axis.</p> <p>Only vertex coordinates are modified; <code>Building.north_axis</code> and <code>Zone</code> rotation fields are not updated.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to modify in-place.</p> required <code>angle_deg</code> <code>float</code> <p>Rotation angle in degrees (positive = counter-clockwise when viewed from above).</p> required <code>anchor</code> <code>Vector3D | None</code> <p>Point to rotate around.  If <code>None</code>, the origin <code>(0, 0, 0)</code> is used.</p> <code>None</code> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def rotate_building(doc: IDFDocument, angle_deg: float, anchor: Vector3D | None = None) -&gt; None:\n    \"\"\"Rotate all building surfaces around the Z axis.\n\n    Only vertex coordinates are modified; ``Building.north_axis`` and\n    ``Zone`` rotation fields are **not** updated.\n\n    Args:\n        doc: The document to modify in-place.\n        angle_deg: Rotation angle in degrees (positive = counter-clockwise when\n            viewed from above).\n        anchor: Point to rotate around.  If ``None``, the origin ``(0, 0, 0)``\n            is used.\n    \"\"\"\n    if anchor is None:\n        anchor = Vector3D.origin()\n\n    for stype in VERTEX_SURFACE_TYPES:\n        for surface in doc[stype]:\n            coords = get_surface_coords(surface)\n            if coords is not None:\n                set_surface_coords(surface, coords.rotate_z(angle_deg, anchor=anchor))\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.set_surface_coords","title":"<code>set_surface_coords(surface, polygon)</code>","text":"<p>Set coordinates on a surface object.</p> <p>Updates vertex fields and number_of_vertices.</p> <p>Examples:</p> <p>Shorten a wall from 10 m to 5 m by replacing its vertices:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n...     validate=False)\n&gt;&gt;&gt; shorter = Polygon3D.from_tuples([(0,0,0),(5,0,0),(5,0,3),(0,0,3)])\n&gt;&gt;&gt; set_surface_coords(wall, shorter)\n&gt;&gt;&gt; get_surface_coords(wall).area\n15.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def set_surface_coords(surface: IDFObject, polygon: Polygon3D) -&gt; None:\n    \"\"\"\n    Set coordinates on a surface object.\n\n    Updates vertex fields and number_of_vertices.\n\n    Examples:\n        Shorten a wall from 10 m to 5 m by replacing its vertices:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n        ...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n        ...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n        ...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n        ...     validate=False)\n        &gt;&gt;&gt; shorter = Polygon3D.from_tuples([(0,0,0),(5,0,0),(5,0,3),(0,0,3)])\n        &gt;&gt;&gt; set_surface_coords(wall, shorter)\n        &gt;&gt;&gt; get_surface_coords(wall).area\n        15.0\n    \"\"\"\n    # Set number of vertices\n    surface.number_of_vertices = len(polygon.vertices)\n\n    # Set vertex coordinates\n    for i, vertex in enumerate(polygon.vertices, 1):\n        setattr(surface, f\"vertex_{i}_x_coordinate\", vertex.x)\n        setattr(surface, f\"vertex_{i}_y_coordinate\", vertex.y)\n        setattr(surface, f\"vertex_{i}_z_coordinate\", vertex.z)\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.set_wwr","title":"<code>set_wwr(doc, wwr, *, construction=None, surface_type='Wall', orientation=None, tolerance=10.0)</code>","text":"<p>Add or replace windows to achieve a target window-wall ratio.</p> <p>For each exterior wall matching the filter criteria, a single rectangular sub-surface (<code>FenestrationSurface:Detailed</code>) is created whose area equals <code>wwr * wall_area</code>.  Any existing <code>FenestrationSurface:Detailed</code> sub-surfaces on matching walls are removed first.</p> <p>This is the idfkit equivalent of geomeppy's <code>idf.set_wwr()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to modify in-place.</p> required <code>wwr</code> <code>float</code> <p>Target window-wall ratio in the range <code>(0, 1)</code>.</p> required <code>construction</code> <code>str | None</code> <p>Name of the window <code>Construction</code> to assign. If <code>None</code>, the field is left empty (EnergyPlus will require it to be set before simulation).</p> <code>None</code> <code>surface_type</code> <code>str</code> <p>Only walls whose <code>surface_type</code> matches (case- insensitive) are considered.  Defaults to <code>\"Wall\"</code>.</p> <code>'Wall'</code> <code>orientation</code> <code>str | None</code> <p>Optional cardinal direction filter \u2014 one of <code>\"north\"</code>, <code>\"south\"</code>, <code>\"east\"</code>, <code>\"west\"</code>.  Only walls within tolerance degrees of that azimuth are modified.</p> <code>None</code> <code>tolerance</code> <code>float</code> <p>Azimuth tolerance in degrees when orientation is given.  Defaults to 10\u00b0.</p> <code>10.0</code> <p>Returns:</p> Type Description <code>list[IDFObject]</code> <p>List of newly created <code>FenestrationSurface:Detailed</code> objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If wwr is not in <code>(0, 1)</code>.</p> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def set_wwr(  # noqa: C901\n    doc: IDFDocument,\n    wwr: float,\n    *,\n    construction: str | None = None,\n    surface_type: str = \"Wall\",\n    orientation: str | None = None,\n    tolerance: float = 10.0,\n) -&gt; list[IDFObject]:\n    \"\"\"Add or replace windows to achieve a target window-wall ratio.\n\n    For each exterior wall matching the filter criteria, a single\n    rectangular sub-surface (``FenestrationSurface:Detailed``) is\n    created whose area equals ``wwr * wall_area``.  Any existing\n    ``FenestrationSurface:Detailed`` sub-surfaces on matching walls are\n    removed first.\n\n    This is the idfkit equivalent of geomeppy's ``idf.set_wwr()``.\n\n    Args:\n        doc: The document to modify in-place.\n        wwr: Target window-wall ratio in the range ``(0, 1)``.\n        construction: Name of the window ``Construction`` to assign.\n            If ``None``, the field is left empty (EnergyPlus will\n            require it to be set before simulation).\n        surface_type: Only walls whose ``surface_type`` matches (case-\n            insensitive) are considered.  Defaults to ``\"Wall\"``.\n        orientation: Optional cardinal direction filter \u2014 one of\n            ``\"north\"``, ``\"south\"``, ``\"east\"``, ``\"west\"``.  Only\n            walls within *tolerance* degrees of that azimuth are modified.\n        tolerance: Azimuth tolerance in degrees when *orientation* is\n            given.  Defaults to 10\u00b0.\n\n    Returns:\n        List of newly created ``FenestrationSurface:Detailed`` objects.\n\n    Raises:\n        ValueError: If *wwr* is not in ``(0, 1)``.\n    \"\"\"\n    if not 0 &lt; wwr &lt; 1:\n        msg = f\"wwr must be between 0 and 1 (exclusive), got {wwr}\"\n        raise ValueError(msg)\n\n    azimuth_target = _orientation_to_azimuth(orientation) if orientation else None\n\n    # Remove existing fenestration on matching walls\n    existing_fen: list[IDFObject] = []\n    wall_names: set[str] = set()\n    for wall in doc[\"BuildingSurface:Detailed\"]:\n        if not _wall_matches(wall, surface_type, azimuth_target, tolerance):\n            continue\n        wall_names.add(wall.name.upper())\n\n    for fen in list(doc[\"FenestrationSurface:Detailed\"]):\n        bsn = getattr(fen, \"building_surface_name\", None) or \"\"\n        if bsn.upper() in wall_names:\n            existing_fen.append(fen)\n    for fen in existing_fen:\n        doc.removeidfobject(fen)\n\n    # Create new windows\n    new_windows: list[IDFObject] = []\n    for wall in doc[\"BuildingSurface:Detailed\"]:\n        if not _wall_matches(wall, surface_type, azimuth_target, tolerance):\n            continue\n        obc = getattr(wall, \"outside_boundary_condition\", None) or \"\"\n        if obc.upper() != \"OUTDOORS\":\n            continue\n\n        coords = get_surface_coords(wall)\n        if coords is None or coords.area &lt; 1e-6:\n            continue\n\n        window_poly = _inset_polygon(coords, wwr)\n        if window_poly is None:\n            continue\n\n        win_name = f\"{wall.name}_Window\"\n        win_data: dict[str, Any] = {\n            \"surface_type\": \"Window\",\n            \"building_surface_name\": wall.name,\n            \"number_of_vertices\": window_poly.num_vertices,\n        }\n        if construction is not None:\n            win_data[\"construction_name\"] = construction\n        for i, v in enumerate(window_poly.vertices, 1):\n            win_data[f\"vertex_{i}_x_coordinate\"] = round(v.x, 6)\n            win_data[f\"vertex_{i}_y_coordinate\"] = round(v.y, 6)\n            win_data[f\"vertex_{i}_z_coordinate\"] = round(v.z, 6)\n\n        win_obj = doc.add(\"FenestrationSurface:Detailed\", win_name, win_data, validate=False)\n        new_windows.append(win_obj)\n\n    return new_windows\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.translate_building","title":"<code>translate_building(doc, offset)</code>","text":"<p>Translate all building surfaces by the given offset vector.</p> <p>Modifies the document in-place, shifting every surface's vertices by offset.</p> <p>Note</p> <p>Only vertex coordinates are modified.  <code>Zone</code> origin fields and the <code>Building</code> object are not updated.  Use translate_to_world if you need to collapse zone-relative coordinates into world coordinates.</p> <p>Examples:</p> <p>Reposition a building on its site (e.g., from local to geo-referenced coordinates):</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     number_of_vertices=4,\n...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n...     validate=False)\n&gt;&gt;&gt; translate_building(model, Vector3D(100, 200, 0))\n&gt;&gt;&gt; wall.vertex_1_x_coordinate\n100.0\n</code></pre> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def translate_building(doc: IDFDocument, offset: Vector3D) -&gt; None:\n    \"\"\"Translate all building surfaces by the given offset vector.\n\n    Modifies the document in-place, shifting every surface's vertices\n    by *offset*.\n\n    !!! note\n        Only vertex coordinates are modified.  ``Zone`` origin fields\n        and the ``Building`` object are **not** updated.  Use\n        [translate_to_world][idfkit.geometry.translate_to_world] if you need to collapse zone-relative\n        coordinates into world coordinates.\n\n    Examples:\n        Reposition a building on its site (e.g., from local to\n        geo-referenced coordinates):\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; wall = model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\", zone_name=\"\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     number_of_vertices=4,\n        ...     vertex_1_x_coordinate=0, vertex_1_y_coordinate=0, vertex_1_z_coordinate=3,\n        ...     vertex_2_x_coordinate=0, vertex_2_y_coordinate=0, vertex_2_z_coordinate=0,\n        ...     vertex_3_x_coordinate=10, vertex_3_y_coordinate=0, vertex_3_z_coordinate=0,\n        ...     vertex_4_x_coordinate=10, vertex_4_y_coordinate=0, vertex_4_z_coordinate=3,\n        ...     validate=False)\n        &gt;&gt;&gt; translate_building(model, Vector3D(100, 200, 0))\n        &gt;&gt;&gt; wall.vertex_1_x_coordinate\n        100.0\n    \"\"\"\n    for stype in VERTEX_SURFACE_TYPES:\n        for surface in doc[stype]:\n            coords = get_surface_coords(surface)\n            if coords is not None:\n                set_surface_coords(surface, coords.translate(offset))\n</code></pre>"},{"location":"api/geometry/#idfkit.geometry.translate_to_world","title":"<code>translate_to_world(doc)</code>","text":"<p>Translate model from relative to world coordinates.</p> <p>Applies zone origins and rotations to surface coordinates.</p> Source code in <code>src/idfkit/geometry.py</code> <pre><code>def translate_to_world(doc: IDFDocument) -&gt; None:  # noqa: C901\n    \"\"\"\n    Translate model from relative to world coordinates.\n\n    Applies zone origins and rotations to surface coordinates.\n    \"\"\"\n    # Check coordinate system\n    geo_rules = doc[\"GlobalGeometryRules\"]\n    if geo_rules:\n        rules = geo_rules.first()\n        coord_system = getattr(rules, \"coordinate_system\", \"World\")\n        if coord_system and coord_system.lower() == \"world\":\n            return  # Already in world coordinates\n\n    # Get building north axis\n    building = doc[\"Building\"]\n    north_axis = 0.0\n    if building:\n        b = building.first()\n        north_axis = float(getattr(b, \"north_axis\", 0) or 0)\n\n    # Process each zone\n    for zone in doc[\"Zone\"]:\n        zone_origin = get_zone_origin(zone)\n        zone_rotation = get_zone_rotation(zone)\n        total_rotation = north_axis + zone_rotation\n\n        # Get surfaces in this zone\n        zone_name = zone.name\n        surfaces = list(doc.get_referencing(zone_name))\n\n        for surface in surfaces:\n            # Only process surfaces with coordinates\n            coords = get_surface_coords(surface)\n            if coords is None:\n                continue\n\n            # Apply rotation\n            if total_rotation != 0:\n                coords = coords.rotate_z(total_rotation)\n\n            # Apply translation\n            coords = coords.translate(zone_origin)\n\n            # Update surface\n            set_surface_coords(surface, coords)\n\n    # Update zone origins to zero\n    for zone in doc[\"Zone\"]:\n        zone.x_origin = 0.0\n        zone.y_origin = 0.0\n        zone.z_origin = 0.0\n        zone.direction_of_relative_north = 0.0\n\n    # Update building north axis\n    if building:\n        b = building.first()\n        if b is not None:\n            b.north_axis = 0.0\n\n    # Update coordinate system to World\n    if geo_rules:\n        rules = geo_rules.first()\n        if rules is not None:\n            rules.coordinate_system = \"World\"\n</code></pre>"},{"location":"api/geometry_builders/","title":"Geometry Builders","text":"<p>Geometry utility functions for EnergyPlus surface manipulation.  For creating building zones and surfaces, see Zoning.</p>"},{"location":"api/geometry_builders/#quick-start","title":"Quick Start","text":"<pre><code>from idfkit import new_document\nfrom idfkit.geometry_builders import add_shading_block\n\ndoc = new_document()\nadd_shading_block(doc, \"Neighbour\", [(30, 0), (50, 0), (50, 20), (30, 20)], height=25)\n\nprint(len(doc[\"Shading:Site:Detailed\"]))  # 5\n</code></pre>"},{"location":"api/geometry_builders/#shading-blocks","title":"Shading Blocks","text":"<p><code>add_shading_block</code> creates <code>Shading:Site:Detailed</code> surfaces -- opaque boxes that cast shadows but have no thermal zones.</p> <pre><code>from idfkit import new_document\nfrom idfkit.geometry_builders import add_shading_block\n\ndoc = new_document()\n\n# Neighbouring building\nadd_shading_block(doc, \"Neighbour\", [(30, 0), (50, 0), (50, 20), (30, 20)], height=25)\n\n# Elevated canopy\nadd_shading_block(doc, \"Canopy\", [(0, -3), (10, -3), (10, 0), (0, 0)], height=0.2, base_z=3)\n</code></pre> <p>Each call creates one wall surface per footprint edge plus a horizontal top cap.</p>"},{"location":"api/geometry_builders/#globalgeometryrules-convention","title":"GlobalGeometryRules Convention","text":"<p>All builder functions read the document's <code>GlobalGeometryRules</code> to determine the vertex ordering convention:</p> <ul> <li><code>starting_vertex_position</code> -- which corner is listed first for   walls (<code>UpperLeftCorner</code>, <code>LowerLeftCorner</code>, etc.)</li> <li><code>vertex_entry_direction</code> -- winding direction (<code>Counterclockwise</code>   or <code>Clockwise</code>)</li> </ul> <p>When no <code>GlobalGeometryRules</code> object exists, the EnergyPlus default of <code>UpperLeftCorner</code> / <code>Counterclockwise</code> is assumed.</p> <p>This means you can safely add geometry to an existing model that uses a non-default convention without having to rewrite all existing surfaces:</p> <pre><code>from idfkit import load_idf, create_building\n\n# Model uses Clockwise vertex convention\nmodel = load_idf(\"existing_building.idf\")\n\n# New surfaces will automatically use Clockwise ordering\n# to match the model's GlobalGeometryRules\ncreate_building(model, \"Addition\", [(20, 0), (30, 0), (30, 10), (20, 10)], floor_to_floor=3)\n</code></pre>"},{"location":"api/geometry_builders/#wall-vertex-order-by-convention","title":"Wall Vertex Order by Convention","text":"<p>For a wall between footprint vertices p1 and p2 (height z_bot to z_top), viewed from outside:</p> Starting Position Counterclockwise Clockwise UpperLeftCorner UL LL LR UR UL UR LR LL LowerLeftCorner LL LR UR UL LL UL UR LR LowerRightCorner LR UR UL LL LR LL UL UR UpperRightCorner UR UL LL LR UR LR LL UL <p>Where UL = (p1, z_top), LL = (p1, z_bot), LR = (p2, z_bot), UR = (p2, z_top).</p>"},{"location":"api/geometry_builders/#horizontal-surfaces","title":"Horizontal Surfaces","text":"<p>For floors and ceilings, the winding direction is adapted so that EnergyPlus computes the correct outward normal regardless of convention:</p> <ul> <li>Floor: outward normal points down (toward ground)</li> <li>Ceiling / Roof: outward normal points up (toward sky)</li> </ul>"},{"location":"api/geometry_builders/#utility-functions","title":"Utility Functions","text":""},{"location":"api/geometry_builders/#set_default_constructions","title":"<code>set_default_constructions</code>","text":"<p>Assigns a placeholder construction name to any surface that lacks one:</p> <pre><code>from idfkit.geometry_builders import set_default_constructions\n\ncount = set_default_constructions(doc, \"Generic Wall\")\nprint(f\"Updated {count} surfaces\")\n</code></pre>"},{"location":"api/geometry_builders/#bounding_box","title":"<code>bounding_box</code>","text":"<p>Returns the 2D axis-aligned bounding box of all <code>BuildingSurface:Detailed</code> objects:</p> <pre><code>from idfkit.geometry_builders import bounding_box\n\nbbox = bounding_box(doc)\nif bbox:\n    (min_x, min_y), (max_x, max_y) = bbox\n    print(f\"Footprint spans {max_x - min_x:.1f} x {max_y - min_y:.1f} m\")\n</code></pre>"},{"location":"api/geometry_builders/#scale_building","title":"<code>scale_building</code>","text":"<p>Scales all surface vertices around an anchor point:</p> <pre><code>from idfkit.geometry_builders import scale_building\nfrom idfkit.geometry import Vector3D\n\n# Double the building in all directions\nscale_building(doc, 2.0)\n\n# Stretch only the X axis\nscale_building(doc, (1.5, 1.0, 1.0))\n\n# Scale around the building centroid\nscale_building(doc, 0.5, anchor=Vector3D(15, 10, 0))\n</code></pre>"},{"location":"api/geometry_builders/#api-reference","title":"API Reference","text":"<p>Geometry utility functions for EnergyPlus surface manipulation.</p> <p>Provides shading block creation, default construction assignment, bounding box queries, building scaling, and <code>GlobalGeometryRules</code> vertex-ordering helpers.</p> <p>For building zone and surface creation, see zoning which provides create_building and ZonedBlock.</p>"},{"location":"api/geometry_builders/#idfkit.geometry_builders.add_shading_block","title":"<code>add_shading_block(doc, name, footprint, height, base_z=0.0)</code>","text":"<p>Create <code>Shading:Site:Detailed</code> surfaces from a 2D footprint.</p> <p>Creates one shading surface per footprint edge (walls) plus a horizontal top cap.  No zones or thermal surfaces are created.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to add objects to.</p> required <code>name</code> <code>str</code> <p>Base name for shading surfaces.</p> required <code>footprint</code> <code>Sequence[tuple[float, float]]</code> <p>2D footprint as <code>(x, y)</code> tuples (counter-clockwise).</p> required <code>height</code> <code>float</code> <p>Height of the shading block in metres.</p> required <code>base_z</code> <code>float</code> <p>Z-coordinate of the block base (default <code>0.0</code>). Use this to create elevated shading surfaces such as canopies.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[IDFObject]</code> <p>List of created <code>Shading:Site:Detailed</code> objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If footprint has fewer than 3 vertices or height &lt;= 0.</p> Source code in <code>src/idfkit/geometry_builders.py</code> <pre><code>def add_shading_block(\n    doc: IDFDocument,\n    name: str,\n    footprint: Sequence[tuple[float, float]],\n    height: float,\n    base_z: float = 0.0,\n) -&gt; list[IDFObject]:\n    \"\"\"Create ``Shading:Site:Detailed`` surfaces from a 2D footprint.\n\n    Creates one shading surface per footprint edge (walls) plus a\n    horizontal top cap.  No zones or thermal surfaces are created.\n\n    Args:\n        doc: The document to add objects to.\n        name: Base name for shading surfaces.\n        footprint: 2D footprint as ``(x, y)`` tuples (counter-clockwise).\n        height: Height of the shading block in metres.\n        base_z: Z-coordinate of the block base (default ``0.0``).\n            Use this to create elevated shading surfaces such as canopies.\n\n    Returns:\n        List of created ``Shading:Site:Detailed`` objects.\n\n    Raises:\n        ValueError: If footprint has fewer than 3 vertices or height &lt;= 0.\n    \"\"\"\n    fp = list(footprint)\n    if len(fp) &lt; 3:\n        msg = f\"Footprint must have at least 3 vertices, got {len(fp)}\"\n        raise ValueError(msg)\n    if height &lt;= 0:\n        msg = f\"Height must be positive, got {height}\"\n        raise ValueError(msg)\n\n    svp, clockwise = get_geometry_convention(doc)\n    wall_order = WALL_ORDER.get((svp, clockwise), (0, 1, 2, 3))\n\n    z_bot = base_z\n    z_top = base_z + height\n    created: list[IDFObject] = []\n    n = len(fp)\n\n    # Walls\n    for j in range(n):\n        p1 = fp[j]\n        p2 = fp[(j + 1) % n]\n        wall_name = f\"{name} Wall {j + 1}\"\n        corners = [\n            Vector3D(p1[0], p1[1], z_top),  # UL\n            Vector3D(p1[0], p1[1], z_bot),  # LL\n            Vector3D(p2[0], p2[1], z_bot),  # LR\n            Vector3D(p2[0], p2[1], z_top),  # UR\n        ]\n        poly = Polygon3D([corners[k] for k in wall_order])\n        obj = doc.add(\"Shading:Site:Detailed\", wall_name, validate=False)\n        set_surface_coords(obj, poly)\n        created.append(obj)\n\n    # Top cap \u2014 horizontal surface with normal pointing up\n    cap_name = f\"{name} Top\"\n    cap = doc.add(\"Shading:Site:Detailed\", cap_name, validate=False)\n    set_surface_coords(cap, horizontal_poly(fp, z_top, reverse=clockwise))\n    created.append(cap)\n\n    return created\n</code></pre>"},{"location":"api/geometry_builders/#idfkit.geometry_builders.bounding_box","title":"<code>bounding_box(doc)</code>","text":"<p>Return the 2D axis-aligned bounding box of all building surfaces.</p> <p>Scans all <code>BuildingSurface:Detailed</code> vertices and returns the bounding envelope projected onto the XY plane.</p> <p>Note</p> <p>Only <code>BuildingSurface:Detailed</code> objects are considered. Fenestration and shading surfaces are excluded because they are either coplanar with (windows) or outside (shading) the thermal envelope.</p> <p>Returns:</p> Type Description <code>tuple[tuple[float, float], tuple[float, float]] | None</code> <p><code>((min_x, min_y), (max_x, max_y))</code> or <code>None</code> if no</p> <code>tuple[tuple[float, float], tuple[float, float]] | None</code> <p>surfaces with valid coordinates exist.</p> Source code in <code>src/idfkit/geometry_builders.py</code> <pre><code>def bounding_box(doc: IDFDocument) -&gt; tuple[tuple[float, float], tuple[float, float]] | None:\n    \"\"\"Return the 2D axis-aligned bounding box of all building surfaces.\n\n    Scans all ``BuildingSurface:Detailed`` vertices and returns the\n    bounding envelope projected onto the XY plane.\n\n    !!! note\n        Only ``BuildingSurface:Detailed`` objects are considered.\n        Fenestration and shading surfaces are excluded because they\n        are either coplanar with (windows) or outside (shading) the\n        thermal envelope.\n\n    Returns:\n        ``((min_x, min_y), (max_x, max_y))`` or ``None`` if no\n        surfaces with valid coordinates exist.\n    \"\"\"\n    min_x = float(\"inf\")\n    min_y = float(\"inf\")\n    max_x = float(\"-inf\")\n    max_y = float(\"-inf\")\n    found = False\n\n    for srf in doc[\"BuildingSurface:Detailed\"]:\n        coords = get_surface_coords(srf)\n        if coords is None:\n            continue\n        for v in coords.vertices:\n            min_x = min(min_x, v.x)\n            min_y = min(min_y, v.y)\n            max_x = max(max_x, v.x)\n            max_y = max(max_y, v.y)\n            found = True\n\n    if not found:\n        return None\n    return ((min_x, min_y), (max_x, max_y))\n</code></pre>"},{"location":"api/geometry_builders/#idfkit.geometry_builders.get_geometry_convention","title":"<code>get_geometry_convention(doc)</code>","text":"<p>Read the vertex ordering convention from <code>GlobalGeometryRules</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p><code>(starting_vertex_position, clockwise)</code> where clockwise is</p> <code>bool</code> <p><code>True</code> when <code>vertex_entry_direction</code> is <code>\"Clockwise\"</code>.</p> <code>tuple[str, bool]</code> <p>Defaults to <code>(\"UpperLeftCorner\", False)</code> if no rules exist.</p> Source code in <code>src/idfkit/geometry_builders.py</code> <pre><code>def get_geometry_convention(doc: IDFDocument) -&gt; tuple[str, bool]:\n    \"\"\"Read the vertex ordering convention from ``GlobalGeometryRules``.\n\n    Returns:\n        ``(starting_vertex_position, clockwise)`` where *clockwise* is\n        ``True`` when ``vertex_entry_direction`` is ``\"Clockwise\"``.\n        Defaults to ``(\"UpperLeftCorner\", False)`` if no rules exist.\n    \"\"\"\n    geo_rules = doc[\"GlobalGeometryRules\"]\n    if not geo_rules:\n        return (\"UpperLeftCorner\", False)\n    rules = geo_rules.first()\n    if rules is None:\n        return (\"UpperLeftCorner\", False)\n    svp = getattr(rules, \"starting_vertex_position\", None) or \"UpperLeftCorner\"\n    ved = getattr(rules, \"vertex_entry_direction\", None) or \"Counterclockwise\"\n    return (str(svp), str(ved).lower() == \"clockwise\")\n</code></pre>"},{"location":"api/geometry_builders/#idfkit.geometry_builders.horizontal_poly","title":"<code>horizontal_poly(footprint, z, *, reverse)</code>","text":"<p>Build a horizontal polygon at height z.</p> <p>When reverse is <code>True</code> the footprint is reversed, flipping the polygon normal.  Used to produce floor and ceiling polygons in the correct winding for the active <code>GlobalGeometryRules</code> convention.</p> Source code in <code>src/idfkit/geometry_builders.py</code> <pre><code>def horizontal_poly(footprint: list[tuple[float, float]], z: float, *, reverse: bool) -&gt; Polygon3D:\n    \"\"\"Build a horizontal polygon at height *z*.\n\n    When *reverse* is ``True`` the footprint is reversed, flipping the\n    polygon normal.  Used to produce floor and ceiling polygons in the\n    correct winding for the active ``GlobalGeometryRules`` convention.\n    \"\"\"\n    pts = reversed(footprint) if reverse else footprint\n    return Polygon3D([Vector3D(p[0], p[1], z) for p in pts])\n</code></pre>"},{"location":"api/geometry_builders/#idfkit.geometry_builders.scale_building","title":"<code>scale_building(doc, factor, anchor=None)</code>","text":"<p>Scale all surface vertices around an anchor point.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to modify in-place.</p> required <code>factor</code> <code>float | tuple[float, float, float]</code> <p>Scale factor.  A single <code>float</code> applies uniform scaling; a <code>(fx, fy, fz)</code> tuple scales each axis independently (e.g. <code>(2.0, 1.0, 1.0)</code> doubles X only).</p> required <code>anchor</code> <code>Vector3D | None</code> <p>Point to scale around.  If <code>None</code>, the origin <code>(0, 0, 0)</code> is used.</p> <code>None</code> Source code in <code>src/idfkit/geometry_builders.py</code> <pre><code>def scale_building(\n    doc: IDFDocument,\n    factor: float | tuple[float, float, float],\n    anchor: Vector3D | None = None,\n) -&gt; None:\n    \"\"\"Scale all surface vertices around an anchor point.\n\n    Args:\n        doc: The document to modify in-place.\n        factor: Scale factor.  A single ``float`` applies uniform scaling;\n            a ``(fx, fy, fz)`` tuple scales each axis independently\n            (e.g. ``(2.0, 1.0, 1.0)`` doubles X only).\n        anchor: Point to scale around.  If ``None``, the origin\n            ``(0, 0, 0)`` is used.\n    \"\"\"\n    if isinstance(factor, tuple):\n        fx, fy, fz = factor\n    else:\n        fx = fy = fz = factor\n\n    ax, ay, az = (anchor.x, anchor.y, anchor.z) if anchor else (0.0, 0.0, 0.0)\n\n    for stype in VERTEX_SURFACE_TYPES:\n        for srf in doc[stype]:\n            coords = get_surface_coords(srf)\n            if coords is None:\n                continue\n            new_vertices = [\n                Vector3D(\n                    ax + (v.x - ax) * fx,\n                    ay + (v.y - ay) * fy,\n                    az + (v.z - az) * fz,\n                )\n                for v in coords.vertices\n            ]\n            set_surface_coords(srf, Polygon3D(new_vertices))\n</code></pre>"},{"location":"api/geometry_builders/#idfkit.geometry_builders.set_default_constructions","title":"<code>set_default_constructions(doc, construction_name='Default Construction')</code>","text":"<p>Assign a placeholder construction to surfaces that lack one.</p> <p>Iterates all <code>BuildingSurface:Detailed</code> and <code>FenestrationSurface:Detailed</code> objects and sets <code>construction_name</code> for any whose current value is empty or <code>None</code>.</p> <p>Does not create the <code>Construction</code> object itself \u2014 the caller is responsible for ensuring it exists.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to modify.</p> required <code>construction_name</code> <code>str</code> <p>Name of the construction to assign.</p> <code>'Default Construction'</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of surfaces updated.</p> Source code in <code>src/idfkit/geometry_builders.py</code> <pre><code>def set_default_constructions(doc: IDFDocument, construction_name: str = \"Default Construction\") -&gt; int:\n    \"\"\"Assign a placeholder construction to surfaces that lack one.\n\n    Iterates all ``BuildingSurface:Detailed`` and\n    ``FenestrationSurface:Detailed`` objects and sets\n    ``construction_name`` for any whose current value is empty or ``None``.\n\n    Does **not** create the ``Construction`` object itself \u2014 the caller\n    is responsible for ensuring it exists.\n\n    Args:\n        doc: The document to modify.\n        construction_name: Name of the construction to assign.\n\n    Returns:\n        Number of surfaces updated.\n    \"\"\"\n    count = 0\n    for stype in (\"BuildingSurface:Detailed\", \"FenestrationSurface:Detailed\"):\n        for srf in doc[stype]:\n            if not srf.get(\"Construction Name\"):\n                srf.construction_name = construction_name\n                count += 1\n    return count\n</code></pre>"},{"location":"api/geometry_builders/#see-also","title":"See Also","text":"<ul> <li>Zoning -- <code>create_building</code>, core-perimeter zoning, footprint   helpers, and multi-zone building generation</li> <li>Geometry -- Lower-level 3D primitives, coordinate transforms,   and surface intersection</li> <li>Visualization -- 3D rendering of building geometry</li> <li>Thermal -- R/U-value calculations for constructions</li> </ul>"},{"location":"api/io/","title":"I/O -- Parsers &amp; Writers","text":"<p>Functions and classes for reading and writing EnergyPlus models in IDF (text) and epJSON (JSON) formats.</p>"},{"location":"api/io/#idf-parser","title":"IDF Parser","text":"<p>Streaming IDF parser - parses EnergyPlus IDF files into IDFDocument.</p> <p>Features: - Memory-efficient streaming for large files - Regex-based tokenization - Direct parsing into IDFDocument (no intermediate structures) - Type coercion based on schema</p>"},{"location":"api/io/#idfkit.idf_parser.IDFParser","title":"<code>IDFParser</code>","text":"<p>Streaming parser for IDF files.</p> <p>Uses memory mapping for large files and regex for tokenization.</p> Source code in <code>src/idfkit/idf_parser.py</code> <pre><code>class IDFParser:\n    \"\"\"\n    Streaming parser for IDF files.\n\n    Uses memory mapping for large files and regex for tokenization.\n    \"\"\"\n\n    __slots__ = (\"_content\", \"_encoding\", \"_filepath\", \"_schema\")\n\n    _filepath: Path\n    _schema: EpJSONSchema | None\n    _encoding: str\n    _content: bytes | None\n\n    def __init__(\n        self,\n        filepath: Path,\n        schema: EpJSONSchema | None = None,\n        encoding: str = \"latin-1\",\n    ):\n        self._filepath = filepath\n        self._schema = schema\n        self._encoding = encoding\n        self._content: bytes | None = None\n\n    def parse(self, version: tuple[int, int, int] | None = None) -&gt; IDFDocument:\n        \"\"\"\n        Parse the IDF file into an IDFDocument.\n\n        Args:\n            version: Optional version override\n\n        Returns:\n            Parsed IDFDocument\n        \"\"\"\n        # Load content (with mmap for large files)\n        content = self._load_content()\n\n        # Detect version if not provided\n        if version is None:\n            version = self._detect_version(content)\n\n        # Load schema if not provided\n        schema = self._schema\n        if schema is None:\n            from .schema import get_schema\n\n            schema = get_schema(version)\n\n        # Create document\n        doc = IDFDocument(version=version, schema=schema, filepath=self._filepath)\n\n        # Parse objects\n        self._parse_objects(content, doc, schema)\n\n        return doc\n\n    def _load_content(self) -&gt; bytes:\n        \"\"\"Load file content, using mmap for large files.\"\"\"\n        file_size = self._filepath.stat().st_size\n\n        if file_size &gt; _MMAP_THRESHOLD:\n            # Use memory mapping for large files\n            with open(self._filepath, \"rb\") as f:\n                mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n                content = bytes(mm)\n                mm.close()\n        else:\n            with open(self._filepath, \"rb\") as f:\n                content = f.read()\n\n        return content\n\n    def _detect_version(self, content: bytes) -&gt; tuple[int, int, int]:\n        \"\"\"Detect EnergyPlus version from file content.\"\"\"\n        # Only search first 10KB for version\n        header = content[:10240]\n\n        match = _VERSION_PATTERN.search(header)\n        if match:\n            major = int(match.group(1))\n            minor = int(match.group(2))\n            patch = int(match.group(3)) if match.group(3) else 0\n            return (major, minor, patch)\n\n        raise VersionNotFoundError(str(self._filepath))\n\n    def _parse_objects(\n        self,\n        content: bytes,\n        doc: IDFDocument,\n        schema: EpJSONSchema | None,\n    ) -&gt; None:\n        \"\"\"Parse all objects from content into document.\"\"\"\n        # Strip comments before matching to prevent phantom objects\n        # (e.g. \"!- X,Y,Z Origin\" matching \"X,\" as an object type)\n        content = _COMMENT_PATTERN.sub(b\"\", content)\n\n        # Local per-type cache avoids repeated schema lookups\n        type_cache: dict[str, ParsingCache | None] = {}\n        encoding = self._encoding\n        addidfobject = doc.addidfobject\n\n        for match in _OBJECT_PATTERN.finditer(content):\n            try:\n                obj_type = match.group(1).decode(encoding).strip()\n\n                # Skip version object (handled separately)\n                if obj_type.upper() == \"VERSION\":\n                    continue\n\n                # Get or build per-type cache\n                if schema is not None:\n                    pc = type_cache.get(obj_type)\n                    if pc is None and obj_type not in type_cache:\n                        pc = schema.get_parsing_cache(obj_type)\n                        type_cache[obj_type] = pc\n                    if pc is None:\n                        continue  # Unknown object type\n                else:\n                    pc = None\n\n                obj = self._parse_object_cached(match, pc, encoding)\n                if obj:\n                    addidfobject(obj)\n            except Exception:  # noqa: S110\n                # Log parse errors but continue\n                pass\n\n    def _parse_object_cached(\n        self,\n        match: re.Match[bytes],\n        pc: ParsingCache | None,\n        encoding: str,\n    ) -&gt; IDFObject | None:\n        \"\"\"Parse a single object from regex match using cached metadata.\"\"\"\n        obj_type = match.group(1).decode(encoding).strip()\n        fields_raw = match.group(2).decode(encoding)\n\n        fields = self._parse_fields(fields_raw)\n        if not fields:\n            return None\n\n        if pc is not None:\n            has_name = pc.has_name\n            # Mutable copy since extensible parsing appends to it\n            field_names: list[str] = list(pc.field_names) if has_name else list(pc.all_field_names)\n\n            name, remaining_fields = (fields[0], fields[1:]) if has_name else (\"\", fields)\n\n            data = self._build_data_dict_cached(remaining_fields, field_names, pc)\n\n            return IDFObject(\n                obj_type=obj_type,\n                name=name,\n                data=data,\n                schema=pc.obj_schema,\n                field_order=field_names,\n                ref_fields=pc.ref_fields,\n            )\n\n        # No-schema fallback\n        name = fields[0] if fields else \"\"\n        remaining = fields[1:]\n        data: dict[str, Any] = {}\n        for i, value in enumerate(remaining):\n            if value:\n                data[f\"field_{i + 1}\"] = value\n        return IDFObject(obj_type=obj_type, name=name, data=data)\n\n    def _build_data_dict_cached(\n        self,\n        remaining_fields: list[str],\n        field_names: list[str],\n        pc: ParsingCache,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Build the data dict using pre-computed field types from the cache.\"\"\"\n        data: dict[str, Any] = {}\n        field_types = pc.field_types\n        num_named = len(field_names)\n\n        for i, value in enumerate(remaining_fields):\n            if i &lt; num_named:\n                field_name = field_names[i]\n                if value:\n                    data[field_name] = _coerce_value_fast(field_types.get(field_name), value)\n                else:\n                    data[field_name] = \"\"\n\n        # Handle extensible fields\n        if pc.extensible and num_named &lt; len(remaining_fields):\n            ext_size = pc.ext_size\n            ext_names = pc.ext_field_names\n            num_ext = len(ext_names)\n            extra = remaining_fields[num_named:]\n            for group_idx in range(0, len(extra), ext_size):\n                group = extra[group_idx : group_idx + ext_size]\n                suffix = \"\" if group_idx == 0 else f\"_{group_idx // ext_size + 1}\"\n                for j, value in enumerate(group):\n                    if j &lt; num_ext:\n                        ext_field = f\"{ext_names[j]}{suffix}\"\n                        if value:\n                            data[ext_field] = _coerce_value_fast(field_types.get(ext_names[j]), value)\n                        else:\n                            data[ext_field] = \"\"\n                        field_names.append(ext_field)\n\n        return data\n\n    def _parse_fields(self, fields_raw: str) -&gt; list[str]:\n        \"\"\"Parse and clean field values from raw string.\"\"\"\n        # Fast path: comments already stripped by _COMMENT_PATTERN in _parse_objects\n        if \"!\" not in fields_raw:\n            return [part.strip() for part in fields_raw.split(\",\")]\n\n        # Slow path: strip inline comments (safety fallback)\n        lines: list[str] = []\n        for line in fields_raw.split(\"\\n\"):\n            idx = line.find(\"!\")\n            if idx &gt;= 0:\n                line = line[:idx]\n            lines.append(line)\n        clean_text = \" \".join(lines)\n        return [part.strip() for part in clean_text.split(\",\")]\n</code></pre>"},{"location":"api/io/#idfkit.idf_parser.IDFParser.parse","title":"<code>parse(version=None)</code>","text":"<p>Parse the IDF file into an IDFDocument.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>tuple[int, int, int] | None</code> <p>Optional version override</p> <code>None</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>Parsed IDFDocument</p> Source code in <code>src/idfkit/idf_parser.py</code> <pre><code>def parse(self, version: tuple[int, int, int] | None = None) -&gt; IDFDocument:\n    \"\"\"\n    Parse the IDF file into an IDFDocument.\n\n    Args:\n        version: Optional version override\n\n    Returns:\n        Parsed IDFDocument\n    \"\"\"\n    # Load content (with mmap for large files)\n    content = self._load_content()\n\n    # Detect version if not provided\n    if version is None:\n        version = self._detect_version(content)\n\n    # Load schema if not provided\n    schema = self._schema\n    if schema is None:\n        from .schema import get_schema\n\n        schema = get_schema(version)\n\n    # Create document\n    doc = IDFDocument(version=version, schema=schema, filepath=self._filepath)\n\n    # Parse objects\n    self._parse_objects(content, doc, schema)\n\n    return doc\n</code></pre>"},{"location":"api/io/#idfkit.idf_parser.get_idf_version","title":"<code>get_idf_version(filepath)</code>","text":"<p>Quick version detection without full parsing.</p> <p>Only reads the first 10 KB of the file, making it very fast even for large models.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to IDF file</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>Version tuple (major, minor, patch)</p> <p>Raises:</p> Type Description <code>VersionNotFoundError</code> <p>If version cannot be detected</p> <p>Examples:</p> <p>Check which EnergyPlus version a model was created for (reads only the first 10 KB for speed):</p> <pre><code>```python\nfrom idfkit import get_idf_version\n\nversion = get_idf_version(\"5ZoneAirCooled.idf\")\nprint(f\"EnergyPlus v{version[0]}.{version[1]}.{version[2]}\")\n```\n</code></pre> Source code in <code>src/idfkit/idf_parser.py</code> <pre><code>def get_idf_version(filepath: Path | str) -&gt; tuple[int, int, int]:\n    \"\"\"\n    Quick version detection without full parsing.\n\n    Only reads the first 10 KB of the file, making it very fast\n    even for large models.\n\n    Args:\n        filepath: Path to IDF file\n\n    Returns:\n        Version tuple (major, minor, patch)\n\n    Raises:\n        VersionNotFoundError: If version cannot be detected\n\n    Examples:\n        Check which EnergyPlus version a model was created for\n        (reads only the first 10 KB for speed):\n\n            ```python\n            from idfkit import get_idf_version\n\n            version = get_idf_version(\"5ZoneAirCooled.idf\")\n            print(f\"EnergyPlus v{version[0]}.{version[1]}.{version[2]}\")\n            ```\n    \"\"\"\n    filepath = Path(filepath)\n\n    with open(filepath, \"rb\") as f:\n        header = f.read(10240)\n\n    match = _VERSION_PATTERN.search(header)\n    if match:\n        major = int(match.group(1))\n        minor = int(match.group(2))\n        patch = int(match.group(3)) if match.group(3) else 0\n        return (major, minor, patch)\n\n    raise VersionNotFoundError(str(filepath))\n</code></pre>"},{"location":"api/io/#idfkit.idf_parser.iter_idf_objects","title":"<code>iter_idf_objects(filepath, encoding='latin-1')</code>","text":"<p>Iterate over objects in an IDF file without loading into document.</p> <p>Yields:</p> Type Description <code>tuple[str, str, list[str]]</code> <p>Tuples of (object_type, name, [field_values])</p> <p>This is useful for quick scanning or filtering without full parsing.</p> <p>Examples:</p> <p>Count thermal zones without loading the full document (useful for quickly sizing batch runs):</p> <pre><code>```python\nfrom idfkit import iter_idf_objects\n\nzone_count = sum(\n    1 for obj_type, name, _\n    in iter_idf_objects(\"5ZoneAirCooled.idf\")\n    if obj_type == \"Zone\"\n)\n```\n</code></pre> <p>Collect all material names for an audit report:</p> <pre><code>```python\nmaterials = [\n    name for obj_type, name, _\n    in iter_idf_objects(\"LargeOffice.idf\")\n    if obj_type == \"Material\"\n]\n```\n</code></pre> Source code in <code>src/idfkit/idf_parser.py</code> <pre><code>def iter_idf_objects(\n    filepath: Path | str,\n    encoding: str = \"latin-1\",\n) -&gt; Iterator[tuple[str, str, list[str]]]:\n    \"\"\"\n    Iterate over objects in an IDF file without loading into document.\n\n    Yields:\n        Tuples of (object_type, name, [field_values])\n\n    This is useful for quick scanning or filtering without full parsing.\n\n    Examples:\n        Count thermal zones without loading the full document\n        (useful for quickly sizing batch runs):\n\n            ```python\n            from idfkit import iter_idf_objects\n\n            zone_count = sum(\n                1 for obj_type, name, _\n                in iter_idf_objects(\"5ZoneAirCooled.idf\")\n                if obj_type == \"Zone\"\n            )\n            ```\n\n        Collect all material names for an audit report:\n\n            ```python\n            materials = [\n                name for obj_type, name, _\n                in iter_idf_objects(\"LargeOffice.idf\")\n                if obj_type == \"Material\"\n            ]\n            ```\n    \"\"\"\n    filepath = Path(filepath)\n\n    with open(filepath, \"rb\") as f:\n        content = f.read()\n\n    # Strip comments before matching to prevent phantom objects\n    content = _COMMENT_PATTERN.sub(b\"\", content)\n\n    for match in _OBJECT_PATTERN.finditer(content):\n        obj_type = match.group(1).decode(encoding).strip()\n        fields_raw = match.group(2).decode(encoding)\n\n        # Split and clean fields\n        fields: list[str] = []\n        for part in fields_raw.split(\",\"):\n            if \"!\" in part:\n                part = part[: part.index(\"!\")]\n            fields.append(part.strip())\n\n        obj_name = fields[0] if fields else \"\"\n        yield (obj_type, obj_name, fields[1:])\n</code></pre>"},{"location":"api/io/#idfkit.idf_parser.parse_idf","title":"<code>parse_idf(filepath, schema=None, version=None, encoding='latin-1')</code>","text":"<p>Parse an IDF file into an IDFDocument.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the IDF file</p> required <code>schema</code> <code>EpJSONSchema | None</code> <p>Optional EpJSONSchema for field ordering and type coercion</p> <code>None</code> <code>version</code> <code>tuple[int, int, int] | None</code> <p>Optional version override (auto-detected if not provided)</p> <code>None</code> <code>encoding</code> <code>str</code> <p>File encoding (default: latin-1 for compatibility)</p> <code>'latin-1'</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>Parsed IDFDocument</p> <p>Raises:</p> Type Description <code>VersionNotFoundError</code> <p>If version cannot be detected</p> <code>IdfKitError</code> <p>If parsing fails</p> <p>Examples:</p> <p>Load and inspect a DOE reference building:</p> <pre><code>```python\nfrom idfkit import parse_idf\n\nmodel = parse_idf(\"RefBldgSmallOfficeNew2004.idf\")\nfor zone in model[\"Zone\"]:\n    print(zone.name, zone.x_origin)\n```\n</code></pre> <p>Force a specific EnergyPlus version when auto-detection fails (e.g., a pre-v8.9 file that was manually upgraded):</p> <pre><code>```python\nmodel = parse_idf(\"legacy_building.idf\", version=(9, 6, 0))\n```\n</code></pre> Source code in <code>src/idfkit/idf_parser.py</code> <pre><code>def parse_idf(\n    filepath: Path | str,\n    schema: EpJSONSchema | None = None,\n    version: tuple[int, int, int] | None = None,\n    encoding: str = \"latin-1\",\n) -&gt; IDFDocument:\n    \"\"\"\n    Parse an IDF file into an IDFDocument.\n\n    Args:\n        filepath: Path to the IDF file\n        schema: Optional EpJSONSchema for field ordering and type coercion\n        version: Optional version override (auto-detected if not provided)\n        encoding: File encoding (default: latin-1 for compatibility)\n\n    Returns:\n        Parsed IDFDocument\n\n    Raises:\n        VersionNotFoundError: If version cannot be detected\n        IdfKitError: If parsing fails\n\n    Examples:\n        Load and inspect a DOE reference building:\n\n            ```python\n            from idfkit import parse_idf\n\n            model = parse_idf(\"RefBldgSmallOfficeNew2004.idf\")\n            for zone in model[\"Zone\"]:\n                print(zone.name, zone.x_origin)\n            ```\n\n        Force a specific EnergyPlus version when auto-detection fails\n        (e.g., a pre-v8.9 file that was manually upgraded):\n\n            ```python\n            model = parse_idf(\"legacy_building.idf\", version=(9, 6, 0))\n            ```\n    \"\"\"\n    filepath = Path(filepath)\n\n    if not filepath.exists():\n        raise FileNotFoundError(f\"IDF file not found: {filepath}\")  # noqa: TRY003\n\n    parser = IDFParser(filepath, schema, encoding)\n    return parser.parse(version)\n</code></pre>"},{"location":"api/io/#epjson-parser","title":"epJSON Parser","text":"<p>epJSON parser - parses EnergyPlus epJSON files into IDFDocument.</p> <p>The epJSON format is the native JSON representation of EnergyPlus models. Parsing is straightforward since it's already structured JSON.</p>"},{"location":"api/io/#idfkit.epjson_parser.EpJSONParser","title":"<code>EpJSONParser</code>","text":"<p>Parser for epJSON files.</p> <p>epJSON is the native JSON format for EnergyPlus models, making parsing straightforward - just json.load() and transform.</p> Source code in <code>src/idfkit/epjson_parser.py</code> <pre><code>class EpJSONParser:\n    \"\"\"\n    Parser for epJSON files.\n\n    epJSON is the native JSON format for EnergyPlus models, making\n    parsing straightforward - just json.load() and transform.\n    \"\"\"\n\n    __slots__ = (\"_filepath\", \"_schema\")\n\n    def __init__(\n        self,\n        filepath: Path,\n        schema: EpJSONSchema | None = None,\n    ):\n        self._filepath = filepath\n        self._schema = schema\n\n    def parse(self, version: tuple[int, int, int] | None = None) -&gt; IDFDocument:\n        \"\"\"\n        Parse the epJSON file into an IDFDocument.\n\n        Args:\n            version: Optional version override\n\n        Returns:\n            Parsed IDFDocument\n        \"\"\"\n        # Load JSON\n        with open(self._filepath, encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        # Detect version if not provided\n        if version is None:\n            version = self._detect_version(data)\n\n        # Load schema if not provided\n        schema = self._schema\n        if schema is None:\n            from .schema import get_schema\n\n            schema = get_schema(version)\n\n        # Create document\n        doc = IDFDocument(version=version, schema=schema, filepath=self._filepath)\n\n        # Parse objects\n        self._parse_objects(data, doc, schema)\n\n        return doc\n\n    def _detect_version(self, data: dict[str, Any]) -&gt; tuple[int, int, int]:\n        \"\"\"Detect EnergyPlus version from epJSON data.\"\"\"\n        # Version is in the \"Version\" object\n        version_obj = data.get(\"Version\")\n\n        if version_obj:\n            # epJSON format: {\"Version\": {\"Version 1\": {\"version_identifier\": \"23.2\"}}}\n            for _name, fields in version_obj.items():\n                version_str: str = fields.get(\"version_identifier\", \"\")\n                if version_str:\n                    return self._parse_version_string(version_str)\n\n        raise VersionNotFoundError(str(self._filepath))\n\n    @staticmethod\n    def _parse_version_string(version_str: str) -&gt; tuple[int, int, int]:\n        \"\"\"Parse version string like '23.2' or '9.2.0'.\"\"\"\n        parts = version_str.split(\".\")\n        major = int(parts[0]) if len(parts) &gt; 0 else 0\n        minor = int(parts[1]) if len(parts) &gt; 1 else 0\n        patch = int(parts[2]) if len(parts) &gt; 2 else 0\n        return (major, minor, patch)\n\n    def _parse_objects(\n        self,\n        data: dict[str, Any],\n        doc: IDFDocument,\n        schema: EpJSONSchema | None,\n    ) -&gt; None:\n        \"\"\"Parse all objects from epJSON data into document.\"\"\"\n        addidfobject = doc.addidfobject\n\n        for obj_type, objects in data.items():\n            # Skip Version (handled separately)\n            if obj_type == \"Version\":\n                continue\n\n            if not isinstance(objects, dict):\n                continue\n\n            # Get schema info from parsing cache\n            pc: ParsingCache | None = None\n            obj_schema: dict[str, Any] | None = None\n            base_field_names: tuple[str, ...] | None = None\n            ref_fields: frozenset[str] | None = None\n            has_name = True\n            if schema:\n                pc = schema.get_parsing_cache(obj_type)\n                if pc is not None:\n                    obj_schema = pc.obj_schema\n                    has_name = pc.has_name\n                    base_field_names = pc.field_names if has_name else pc.all_field_names\n                    ref_fields = pc.ref_fields\n\n            # epJSON format: {\"ObjectType\": {\"obj_name\": {fields...}, ...}}\n            objects_dict = cast(dict[str, Any], objects)\n            for obj_name, fields in objects_dict.items():\n                if not isinstance(fields, dict):\n                    continue\n\n                # Nameless objects: use empty string instead of epJSON dict key\n                name = obj_name if has_name else \"\"\n\n                # Create per-object field_order copy so extensible fields can be added\n                fields_dict = cast(dict[str, Any], fields)\n                field_order = self._build_field_order(base_field_names, fields_dict, pc)\n\n                obj = IDFObject(\n                    obj_type=obj_type,\n                    name=name,\n                    data=dict(fields_dict),  # Copy the fields dict\n                    schema=obj_schema,\n                    field_order=field_order,\n                    ref_fields=ref_fields,\n                )\n\n                addidfobject(obj)\n\n    @staticmethod\n    def _build_field_order(\n        base_field_names: tuple[str, ...] | None,\n        fields_dict: dict[str, Any],\n        pc: ParsingCache | None,\n    ) -&gt; list[str] | None:\n        \"\"\"Build field_order including extensible fields present in the data.\"\"\"\n        if base_field_names is None:\n            return None\n\n        # Start with a copy of the base schema field names\n        field_order = list(base_field_names)\n        base_set = set(base_field_names)\n\n        # Find extensible fields in the data that aren't in the base field list\n        if pc is not None and pc.extensible and pc.ext_field_names:\n            ext_names = pc.ext_field_names\n            group_idx = 0\n            while True:\n                suffix = \"\" if group_idx == 0 else f\"_{group_idx + 1}\"\n                group_fields = [f\"{name}{suffix}\" for name in ext_names]\n\n                if not any(f in fields_dict for f in group_fields):\n                    break\n\n                for f in group_fields:\n                    if f not in base_set:\n                        field_order.append(f)\n\n                group_idx += 1\n\n        return field_order\n</code></pre>"},{"location":"api/io/#idfkit.epjson_parser.EpJSONParser.parse","title":"<code>parse(version=None)</code>","text":"<p>Parse the epJSON file into an IDFDocument.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>tuple[int, int, int] | None</code> <p>Optional version override</p> <code>None</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>Parsed IDFDocument</p> Source code in <code>src/idfkit/epjson_parser.py</code> <pre><code>def parse(self, version: tuple[int, int, int] | None = None) -&gt; IDFDocument:\n    \"\"\"\n    Parse the epJSON file into an IDFDocument.\n\n    Args:\n        version: Optional version override\n\n    Returns:\n        Parsed IDFDocument\n    \"\"\"\n    # Load JSON\n    with open(self._filepath, encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    # Detect version if not provided\n    if version is None:\n        version = self._detect_version(data)\n\n    # Load schema if not provided\n    schema = self._schema\n    if schema is None:\n        from .schema import get_schema\n\n        schema = get_schema(version)\n\n    # Create document\n    doc = IDFDocument(version=version, schema=schema, filepath=self._filepath)\n\n    # Parse objects\n    self._parse_objects(data, doc, schema)\n\n    return doc\n</code></pre>"},{"location":"api/io/#idfkit.epjson_parser.get_epjson_version","title":"<code>get_epjson_version(filepath)</code>","text":"<p>Quick version detection from epJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to epJSON file</p> required <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>Version tuple (major, minor, patch)</p> <p>Raises:</p> Type Description <code>VersionNotFoundError</code> <p>If version cannot be detected</p> <p>Examples:</p> <p>Detect the EnergyPlus version of an epJSON file:</p> <pre><code>```python\nfrom idfkit.epjson_parser import get_epjson_version\n\nversion = get_epjson_version(\"SmallOffice.epJSON\")\nprint(f\"EnergyPlus v{version[0]}.{version[1]}\")\n```\n</code></pre> Source code in <code>src/idfkit/epjson_parser.py</code> <pre><code>def get_epjson_version(filepath: Path | str) -&gt; tuple[int, int, int]:\n    \"\"\"\n    Quick version detection from epJSON file.\n\n    Args:\n        filepath: Path to epJSON file\n\n    Returns:\n        Version tuple (major, minor, patch)\n\n    Raises:\n        VersionNotFoundError: If version cannot be detected\n\n    Examples:\n        Detect the EnergyPlus version of an epJSON file:\n\n            ```python\n            from idfkit.epjson_parser import get_epjson_version\n\n            version = get_epjson_version(\"SmallOffice.epJSON\")\n            print(f\"EnergyPlus v{version[0]}.{version[1]}\")\n            ```\n    \"\"\"\n    filepath = Path(filepath)\n\n    with open(filepath, encoding=\"utf-8\") as f:\n        # Parse just enough to get version\n        data = json.load(f)\n\n    version_obj = data.get(\"Version\")\n    if version_obj:\n        for _name, fields in version_obj.items():\n            version_str: str = fields.get(\"version_identifier\", \"\")\n            if version_str:\n                parts = version_str.split(\".\")\n                major = int(parts[0]) if len(parts) &gt; 0 else 0\n                minor = int(parts[1]) if len(parts) &gt; 1 else 0\n                patch = int(parts[2]) if len(parts) &gt; 2 else 0\n                return (major, minor, patch)\n\n    raise VersionNotFoundError(str(filepath))\n</code></pre>"},{"location":"api/io/#idfkit.epjson_parser.load_epjson","title":"<code>load_epjson(filepath)</code>","text":"<p>Load raw epJSON data without parsing into document.</p> <p>Useful for quick inspection or manipulation when you need the raw JSON dict rather than an IDFDocument.</p> <p>Examples:</p> <p>Grab the raw JSON dict for custom post-processing:</p> <pre><code>```python\nfrom idfkit.epjson_parser import load_epjson\n\ndata = load_epjson(\"SmallOffice.epJSON\")\nzone_names = list(data.get(\"Zone\", {}).keys())\n```\n</code></pre> Source code in <code>src/idfkit/epjson_parser.py</code> <pre><code>def load_epjson(filepath: Path | str) -&gt; dict[str, Any]:\n    \"\"\"\n    Load raw epJSON data without parsing into document.\n\n    Useful for quick inspection or manipulation when you need the\n    raw JSON dict rather than an [IDFDocument][idfkit.document.IDFDocument].\n\n    Examples:\n        Grab the raw JSON dict for custom post-processing:\n\n            ```python\n            from idfkit.epjson_parser import load_epjson\n\n            data = load_epjson(\"SmallOffice.epJSON\")\n            zone_names = list(data.get(\"Zone\", {}).keys())\n            ```\n    \"\"\"\n    filepath = Path(filepath)\n    with open(filepath, encoding=\"utf-8\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/io/#idfkit.epjson_parser.parse_epjson","title":"<code>parse_epjson(filepath, schema=None, version=None)</code>","text":"<p>Parse an epJSON file into an IDFDocument.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Path | str</code> <p>Path to the epJSON file</p> required <code>schema</code> <code>EpJSONSchema | None</code> <p>Optional EpJSONSchema for validation</p> <code>None</code> <code>version</code> <code>tuple[int, int, int] | None</code> <p>Optional version override (auto-detected if not provided)</p> <code>None</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>Parsed IDFDocument</p> <p>Raises:</p> Type Description <code>VersionNotFoundError</code> <p>If version cannot be detected</p> <code>IdfKitError</code> <p>If parsing fails</p> <p>Examples:</p> <p>Load an epJSON model and list its thermal zones:</p> <pre><code>```python\nfrom idfkit import parse_epjson\n\nmodel = parse_epjson(\"SmallOffice.epJSON\")\nfor zone in model[\"Zone\"]:\n    print(zone.name)\n```\n</code></pre> Source code in <code>src/idfkit/epjson_parser.py</code> <pre><code>def parse_epjson(\n    filepath: Path | str,\n    schema: EpJSONSchema | None = None,\n    version: tuple[int, int, int] | None = None,\n) -&gt; IDFDocument:\n    \"\"\"\n    Parse an epJSON file into an IDFDocument.\n\n    Args:\n        filepath: Path to the epJSON file\n        schema: Optional EpJSONSchema for validation\n        version: Optional version override (auto-detected if not provided)\n\n    Returns:\n        Parsed IDFDocument\n\n    Raises:\n        VersionNotFoundError: If version cannot be detected\n        IdfKitError: If parsing fails\n\n    Examples:\n        Load an epJSON model and list its thermal zones:\n\n            ```python\n            from idfkit import parse_epjson\n\n            model = parse_epjson(\"SmallOffice.epJSON\")\n            for zone in model[\"Zone\"]:\n                print(zone.name)\n            ```\n    \"\"\"\n    filepath = Path(filepath)\n\n    if not filepath.exists():\n        raise FileNotFoundError(f\"epJSON file not found: {filepath}\")  # noqa: TRY003\n\n    parser = EpJSONParser(filepath, schema)\n    return parser.parse(version)\n</code></pre>"},{"location":"api/io/#writers","title":"Writers","text":"<p>Writers for IDF and epJSON formats.</p> <p>Provides serialization of IDFDocument to both formats.</p> <p>The write_idf function accepts an output_type parameter that mirrors eppy's <code>idf.outputtype</code> options:</p> <ul> <li><code>\"standard\"</code> (default): field comments included (<code>!- Field Name</code>).</li> <li><code>\"nocomment\"</code>: no field comments, one field per line.</li> <li><code>\"compressed\"</code>: entire object on a single line (minimal whitespace).</li> </ul>"},{"location":"api/io/#idfkit.writers.EpJSONWriter","title":"<code>EpJSONWriter</code>","text":"<p>Writes IDFDocument to epJSON format.</p> <p>The epJSON format is: <pre><code>{\n  \"Version\": {\n    \"Version 1\": {\n      \"version_identifier\": \"23.2\"\n    }\n  },\n  \"Zone\": {\n    \"Zone 1\": {\n      \"direction_of_relative_north\": 0.0,\n      ...\n    }\n  }\n}\n</code></pre></p> Source code in <code>src/idfkit/writers.py</code> <pre><code>class EpJSONWriter:\n    \"\"\"\n    Writes IDFDocument to epJSON format.\n\n    The epJSON format is:\n    ```json\n    {\n      \"Version\": {\n        \"Version 1\": {\n          \"version_identifier\": \"23.2\"\n        }\n      },\n      \"Zone\": {\n        \"Zone 1\": {\n          \"direction_of_relative_north\": 0.0,\n          ...\n        }\n      }\n    }\n    ```\n    \"\"\"\n\n    def __init__(self, doc: IDFDocument):\n        self._doc = doc\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert document to epJSON dict.\"\"\"\n        result: dict[str, Any] = {}\n\n        # Add Version\n        version = self._doc.version\n        result[\"Version\"] = {\"Version 1\": {\"version_identifier\": f\"{version[0]}.{version[1]}\"}}\n\n        # Add objects by type\n        for obj_type, collection in self._doc.collections.items():\n            if not collection:\n                continue\n\n            result[obj_type] = {}\n            nameless_counter = 0\n            for obj in collection:\n                obj_data = self._object_to_dict(obj)\n                if obj.name:\n                    key = obj.name\n                else:\n                    # Generate unique key for nameless objects (e.g. Output:Variable)\n                    nameless_counter += 1\n                    key = f\"{obj_type} {nameless_counter}\"\n                result[obj_type][key] = obj_data\n\n        return result\n\n    def _object_to_dict(self, obj: IDFObject) -&gt; dict[str, Any]:\n        \"\"\"Convert object to epJSON dict (excluding name).\"\"\"\n        result: dict[str, Any] = {}\n\n        for field_name, value in obj.data.items():\n            if value is not None and value != \"\":\n                result[field_name] = self._format_value(value)\n\n        return result\n\n    def _format_value(self, value: Any) -&gt; Any:\n        \"\"\"Format a field value for epJSON output.\"\"\"\n        # epJSON uses native JSON types\n        if isinstance(value, str):\n            # Check for special values\n            lower = value.lower()\n            if lower == \"autocalculate\":\n                return \"Autocalculate\"\n            if lower == \"autosize\":\n                return \"Autosize\"\n            if lower == \"yes\":\n                return \"Yes\"\n            if lower == \"no\":\n                return \"No\"\n        return value\n\n    def write_to_file(self, filepath: Path | str, indent: int = 2) -&gt; None:\n        \"\"\"Write to file.\"\"\"\n        data = self.to_dict()\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=indent)\n</code></pre>"},{"location":"api/io/#idfkit.writers.EpJSONWriter.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert document to epJSON dict.</p> Source code in <code>src/idfkit/writers.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert document to epJSON dict.\"\"\"\n    result: dict[str, Any] = {}\n\n    # Add Version\n    version = self._doc.version\n    result[\"Version\"] = {\"Version 1\": {\"version_identifier\": f\"{version[0]}.{version[1]}\"}}\n\n    # Add objects by type\n    for obj_type, collection in self._doc.collections.items():\n        if not collection:\n            continue\n\n        result[obj_type] = {}\n        nameless_counter = 0\n        for obj in collection:\n            obj_data = self._object_to_dict(obj)\n            if obj.name:\n                key = obj.name\n            else:\n                # Generate unique key for nameless objects (e.g. Output:Variable)\n                nameless_counter += 1\n                key = f\"{obj_type} {nameless_counter}\"\n            result[obj_type][key] = obj_data\n\n    return result\n</code></pre>"},{"location":"api/io/#idfkit.writers.EpJSONWriter.write_to_file","title":"<code>write_to_file(filepath, indent=2)</code>","text":"<p>Write to file.</p> Source code in <code>src/idfkit/writers.py</code> <pre><code>def write_to_file(self, filepath: Path | str, indent: int = 2) -&gt; None:\n    \"\"\"Write to file.\"\"\"\n    data = self.to_dict()\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, indent=indent)\n</code></pre>"},{"location":"api/io/#idfkit.writers.IDFWriter","title":"<code>IDFWriter</code>","text":"<p>Writes IDFDocument to IDF text format.</p> <p>The IDF format is: <pre><code>ObjectType,\n  field1,    !- Field 1 Name\n  field2,    !- Field 2 Name\n  field3;    !- Field 3 Name\n</code></pre></p> <p>Supports three output_type modes mirroring eppy's <code>idf.outputtype</code>:</p> <ul> <li><code>\"standard\"</code> \u2014 full comments (default).</li> <li><code>\"nocomment\"</code> \u2014 no field comments, one field per line.</li> <li><code>\"compressed\"</code> \u2014 each object on a single line.</li> </ul> Source code in <code>src/idfkit/writers.py</code> <pre><code>class IDFWriter:\n    \"\"\"\n    Writes IDFDocument to IDF text format.\n\n    The IDF format is:\n    ```\n    ObjectType,\n      field1,    !- Field 1 Name\n      field2,    !- Field 2 Name\n      field3;    !- Field 3 Name\n    ```\n\n    Supports three *output_type* modes mirroring eppy's\n    ``idf.outputtype``:\n\n    - ``\"standard\"`` \u2014 full comments (default).\n    - ``\"nocomment\"`` \u2014 no field comments, one field per line.\n    - ``\"compressed\"`` \u2014 each object on a single line.\n    \"\"\"\n\n    def __init__(self, doc: IDFDocument, output_type: OutputType = \"standard\"):\n        self._doc = doc\n        self._output_type = output_type\n\n    def to_string(self) -&gt; str:\n        \"\"\"Convert document to IDF string.\"\"\"\n        lines: list[str] = []\n\n        if self._output_type != \"compressed\":\n            # Write header comment\n            lines.append(\"!-Generator archetypal\")\n            lines.append(\"!-Option SortedOrder\")\n            lines.append(\"\")\n\n        # Write Version first\n        version = self._doc.version\n        if self._output_type == \"compressed\":\n            lines.append(f\"Version,{version[0]}.{version[1]};\")\n        else:\n            lines.append(\"Version,\")\n            if self._output_type == \"standard\":\n                lines.append(f\"  {version[0]}.{version[1]};                    !- Version Identifier\")\n            else:\n                lines.append(f\"  {version[0]}.{version[1]};\")\n            lines.append(\"\")\n\n        # Write objects grouped by type\n        for obj_type in sorted(self._doc.collections.keys()):\n            collection = self._doc.collections[obj_type]\n            if not collection:\n                continue\n\n            for obj in collection:\n                obj_str = self._object_to_string(obj)\n                lines.append(obj_str)\n                if self._output_type != \"compressed\":\n                    lines.append(\"\")\n\n        return \"\\n\".join(lines)\n\n    def _get_field_values_and_comments(self, obj: IDFObject) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Get the ordered field values and comment labels for *obj*.\"\"\"\n        obj_type = obj.obj_type\n        schema = self._doc.schema\n\n        obj_has_name = True\n        if schema:\n            obj_has_name = schema.has_name(obj_type)\n\n        if obj.field_order:\n            if obj_has_name:\n                field_names: list[str] = [\"name\", *list(obj.field_order)]\n            else:\n                field_names = list(obj.field_order)\n        elif schema:\n            field_names = schema.get_all_field_names(obj_type)\n        else:\n            field_names = [\"name\", *list(obj.data.keys())] if obj_has_name else list(obj.data.keys())\n\n        values: list[str] = []\n        comments: list[str] = []\n\n        for field_name in field_names:\n            if field_name == \"name\":\n                values.append(obj.name or \"\")\n                comments.append(\"Name\")\n            else:\n                value = obj.data.get(field_name)\n                values.append(self._format_value(value))\n                comment = field_name.replace(\"_\", \" \").title()\n                comments.append(comment)\n\n        # Trim trailing empty fields\n        while len(values) &gt; 1 and values[-1] == \"\":\n            values.pop()\n            comments.pop()\n\n        return values, comments\n\n    def _object_to_string(self, obj: IDFObject) -&gt; str:\n        \"\"\"Convert a single object to IDF string.\"\"\"\n        values, comments = self._get_field_values_and_comments(obj)\n        obj_type = obj.obj_type\n\n        if self._output_type == \"compressed\":\n            parts = \",\".join(values)\n            return f\"{obj_type},{parts};\"\n\n        lines: list[str] = [f\"{obj_type},\"]\n        for i, (value, comment) in enumerate(zip(values, comments, strict=False)):\n            is_last = i == len(values) - 1\n            terminator = \";\" if is_last else \",\"\n\n            if self._output_type == \"standard\":\n                field_str = f\"  {value}{terminator}\"\n                field_str = field_str.ljust(30)\n                field_str += f\"!- {comment}\"\n            else:\n                # nocomment\n                field_str = f\"  {value}{terminator}\"\n\n            lines.append(field_str)\n\n        return \"\\n\".join(lines)\n\n    def _format_value(self, value: Any) -&gt; str:\n        \"\"\"Format a field value for IDF output.\"\"\"\n        if value is None:\n            return \"\"\n        if isinstance(value, bool):\n            return \"Yes\" if value else \"No\"\n        if isinstance(value, float):\n            # Avoid scientific notation for small numbers\n            abs_val = abs(value)\n            if abs_val &lt; 1e-10:\n                return \"0\"\n            if abs_val &gt;= 1e10 or abs_val &lt; 0.0001:\n                return f\"{value:.6e}\"\n            return f\"{value:g}\"\n        if isinstance(value, list):\n            # Handle vertex lists etc.\n            items = cast(list[Any], value)\n            return \", \".join(str(v) for v in items)\n        return str(value)\n\n    def write_to_file(self, filepath: Path | str, encoding: str = \"latin-1\") -&gt; None:\n        \"\"\"Write to file.\"\"\"\n        content = self.to_string()\n        with open(filepath, \"w\", encoding=encoding) as f:\n            f.write(content)\n</code></pre>"},{"location":"api/io/#idfkit.writers.IDFWriter.to_string","title":"<code>to_string()</code>","text":"<p>Convert document to IDF string.</p> Source code in <code>src/idfkit/writers.py</code> <pre><code>def to_string(self) -&gt; str:\n    \"\"\"Convert document to IDF string.\"\"\"\n    lines: list[str] = []\n\n    if self._output_type != \"compressed\":\n        # Write header comment\n        lines.append(\"!-Generator archetypal\")\n        lines.append(\"!-Option SortedOrder\")\n        lines.append(\"\")\n\n    # Write Version first\n    version = self._doc.version\n    if self._output_type == \"compressed\":\n        lines.append(f\"Version,{version[0]}.{version[1]};\")\n    else:\n        lines.append(\"Version,\")\n        if self._output_type == \"standard\":\n            lines.append(f\"  {version[0]}.{version[1]};                    !- Version Identifier\")\n        else:\n            lines.append(f\"  {version[0]}.{version[1]};\")\n        lines.append(\"\")\n\n    # Write objects grouped by type\n    for obj_type in sorted(self._doc.collections.keys()):\n        collection = self._doc.collections[obj_type]\n        if not collection:\n            continue\n\n        for obj in collection:\n            obj_str = self._object_to_string(obj)\n            lines.append(obj_str)\n            if self._output_type != \"compressed\":\n                lines.append(\"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/io/#idfkit.writers.IDFWriter.write_to_file","title":"<code>write_to_file(filepath, encoding='latin-1')</code>","text":"<p>Write to file.</p> Source code in <code>src/idfkit/writers.py</code> <pre><code>def write_to_file(self, filepath: Path | str, encoding: str = \"latin-1\") -&gt; None:\n    \"\"\"Write to file.\"\"\"\n    content = self.to_string()\n    with open(filepath, \"w\", encoding=encoding) as f:\n        f.write(content)\n</code></pre>"},{"location":"api/io/#idfkit.writers.convert_epjson_to_idf","title":"<code>convert_epjson_to_idf(epjson_path, idf_path=None)</code>","text":"<p>Convert an epJSON file to IDF format.</p> <p>Parameters:</p> Name Type Description Default <code>epjson_path</code> <code>Path | str</code> <p>Input epJSON file path</p> required <code>idf_path</code> <code>Path | str | None</code> <p>Output IDF path (default: same name with .idf extension)</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the output file</p> <p>Examples:</p> <p>Convert an epJSON model back to classic IDF format:</p> <pre><code>```python\noutput = convert_epjson_to_idf(\"5ZoneAirCooled.epJSON\")\n# Creates 5ZoneAirCooled.idf\n\nconvert_epjson_to_idf(\"modern_model.epJSON\", \"classic_model.idf\")\n```\n</code></pre> Source code in <code>src/idfkit/writers.py</code> <pre><code>def convert_epjson_to_idf(\n    epjson_path: Path | str,\n    idf_path: Path | str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Convert an epJSON file to IDF format.\n\n    Args:\n        epjson_path: Input epJSON file path\n        idf_path: Output IDF path (default: same name with .idf extension)\n\n    Returns:\n        Path to the output file\n\n    Examples:\n        Convert an epJSON model back to classic IDF format:\n\n            ```python\n            output = convert_epjson_to_idf(\"5ZoneAirCooled.epJSON\")\n            # Creates 5ZoneAirCooled.idf\n\n            convert_epjson_to_idf(\"modern_model.epJSON\", \"classic_model.idf\")\n            ```\n    \"\"\"\n    from .epjson_parser import parse_epjson\n\n    epjson_path = Path(epjson_path)\n\n    idf_path = epjson_path.with_suffix(\".idf\") if idf_path is None else Path(idf_path)\n\n    doc = parse_epjson(epjson_path)\n    write_idf(doc, idf_path)\n\n    return idf_path\n</code></pre>"},{"location":"api/io/#idfkit.writers.convert_idf_to_epjson","title":"<code>convert_idf_to_epjson(idf_path, epjson_path=None)</code>","text":"<p>Convert an IDF file to epJSON format.</p> <p>Parameters:</p> Name Type Description Default <code>idf_path</code> <code>Path | str</code> <p>Input IDF file path</p> required <code>epjson_path</code> <code>Path | str | None</code> <p>Output epJSON path (default: same name with .epJSON extension)</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the output file</p> <p>Examples:</p> <p>Convert an IDF model to native JSON format:</p> <pre><code>```python\noutput = convert_idf_to_epjson(\"5ZoneAirCooled.idf\")\n# Creates 5ZoneAirCooled.epJSON\n\nconvert_idf_to_epjson(\"legacy_model.idf\", \"modern_model.epJSON\")\n```\n</code></pre> Source code in <code>src/idfkit/writers.py</code> <pre><code>def convert_idf_to_epjson(\n    idf_path: Path | str,\n    epjson_path: Path | str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Convert an IDF file to epJSON format.\n\n    Args:\n        idf_path: Input IDF file path\n        epjson_path: Output epJSON path (default: same name with .epJSON extension)\n\n    Returns:\n        Path to the output file\n\n    Examples:\n        Convert an IDF model to native JSON format:\n\n            ```python\n            output = convert_idf_to_epjson(\"5ZoneAirCooled.idf\")\n            # Creates 5ZoneAirCooled.epJSON\n\n            convert_idf_to_epjson(\"legacy_model.idf\", \"modern_model.epJSON\")\n            ```\n    \"\"\"\n    from .idf_parser import parse_idf\n\n    idf_path = Path(idf_path)\n\n    epjson_path = idf_path.with_suffix(\".epJSON\") if epjson_path is None else Path(epjson_path)\n\n    doc = parse_idf(idf_path)\n    write_epjson(doc, epjson_path)\n\n    return epjson_path\n</code></pre>"},{"location":"api/io/#idfkit.writers.write_epjson","title":"<code>write_epjson(doc, filepath=None, indent=2)</code>","text":"<p>Write document to epJSON format.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to write</p> required <code>filepath</code> <code>Path | str | None</code> <p>Output path (if None, returns string)</p> <code>None</code> <code>indent</code> <code>int</code> <p>JSON indentation</p> <code>2</code> <p>Returns:</p> Type Description <code>str | None</code> <p>JSON string if filepath is None, otherwise None</p> <p>Examples:</p> <p>Serialize the model to epJSON for use with EnergyPlus v9.3+:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document, write_epjson\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; json_str = write_epjson(model)\n&gt;&gt;&gt; '\"Zone\"' in json_str\nTrue\n</code></pre> <p>Write to disk:</p> <pre><code>```python\nwrite_epjson(model, \"in.epJSON\")\n```\n</code></pre> Source code in <code>src/idfkit/writers.py</code> <pre><code>def write_epjson(\n    doc: IDFDocument,\n    filepath: Path | str | None = None,\n    indent: int = 2,\n) -&gt; str | None:\n    \"\"\"\n    Write document to epJSON format.\n\n    Args:\n        doc: The document to write\n        filepath: Output path (if None, returns string)\n        indent: JSON indentation\n\n    Returns:\n        JSON string if filepath is None, otherwise None\n\n    Examples:\n        Serialize the model to epJSON for use with EnergyPlus v9.3+:\n\n        &gt;&gt;&gt; from idfkit import new_document, write_epjson\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; json_str = write_epjson(model)\n        &gt;&gt;&gt; '\"Zone\"' in json_str\n        True\n\n        Write to disk:\n\n            ```python\n            write_epjson(model, \"in.epJSON\")\n            ```\n    \"\"\"\n    writer = EpJSONWriter(doc)\n    data = writer.to_dict()\n\n    if filepath:\n        filepath = Path(filepath)\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=indent)\n        return None\n\n    return json.dumps(data, indent=indent)\n</code></pre>"},{"location":"api/io/#idfkit.writers.write_idf","title":"<code>write_idf(doc, filepath=None, encoding='latin-1', output_type='standard')</code>","text":"<p>Write document to IDF format.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to write.</p> required <code>filepath</code> <code>Path | str | None</code> <p>Output path (if <code>None</code>, returns a string).</p> <code>None</code> <code>encoding</code> <code>str</code> <p>Output encoding.</p> <code>'latin-1'</code> <code>output_type</code> <code>OutputType</code> <p>Output formatting mode \u2014 <code>\"standard\"</code> (with comments), <code>\"nocomment\"</code> (no comments), or <code>\"compressed\"</code> (single-line objects).  Mirrors eppy's <code>idf.outputtype</code>.</p> <code>'standard'</code> <p>Returns:</p> Type Description <code>str | None</code> <p>IDF string if filepath is <code>None</code>, otherwise <code>None</code>.</p> <p>Examples:</p> <p>Serialize the model to an IDF string for inspection:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document, write_idf\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; idf_str = write_idf(model)\n&gt;&gt;&gt; \"Zone,\" in idf_str\nTrue\n</code></pre> <p>Write to disk for EnergyPlus simulation:</p> <pre><code>```python\nwrite_idf(model, \"in.idf\")\n```\n</code></pre> <p>Use compressed format for batch parametric runs:</p> <pre><code>&gt;&gt;&gt; compressed = write_idf(model, output_type=\"compressed\")\n&gt;&gt;&gt; \"\\n\" not in compressed.split(\"Zone\")[1].split(\";\")[0]\nTrue\n</code></pre> Source code in <code>src/idfkit/writers.py</code> <pre><code>def write_idf(\n    doc: IDFDocument,\n    filepath: Path | str | None = None,\n    encoding: str = \"latin-1\",\n    output_type: OutputType = \"standard\",\n) -&gt; str | None:\n    \"\"\"\n    Write document to IDF format.\n\n    Args:\n        doc: The document to write.\n        filepath: Output path (if ``None``, returns a string).\n        encoding: Output encoding.\n        output_type: Output formatting mode \u2014 ``\"standard\"`` (with\n            comments), ``\"nocomment\"`` (no comments), or\n            ``\"compressed\"`` (single-line objects).  Mirrors eppy's\n            ``idf.outputtype``.\n\n    Returns:\n        IDF string if *filepath* is ``None``, otherwise ``None``.\n\n    Examples:\n        Serialize the model to an IDF string for inspection:\n\n        &gt;&gt;&gt; from idfkit import new_document, write_idf\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; idf_str = write_idf(model)\n        &gt;&gt;&gt; \"Zone,\" in idf_str\n        True\n\n        Write to disk for EnergyPlus simulation:\n\n            ```python\n            write_idf(model, \"in.idf\")\n            ```\n\n        Use compressed format for batch parametric runs:\n\n        &gt;&gt;&gt; compressed = write_idf(model, output_type=\"compressed\")\n        &gt;&gt;&gt; \"\\\\n\" not in compressed.split(\"Zone\")[1].split(\";\")[0]\n        True\n    \"\"\"\n    writer = IDFWriter(doc, output_type=output_type)\n    content = writer.to_string()\n\n    if filepath:\n        filepath = Path(filepath)\n        with open(filepath, \"w\", encoding=encoding) as f:\n            f.write(content)\n        return None\n\n    return content\n</code></pre>"},{"location":"api/objects/","title":"Objects","text":"<p><code>IDFObject</code> represents a single EnergyPlus object (e.g. a Zone or a Material). Fields are accessed as snake_case Python attributes.</p> <p><code>IDFCollection</code> is a name-indexed container of objects that share the same EnergyPlus type, providing O(1) lookup by name, iteration, and filtering.</p> <p>Core object classes for IDF representation.</p> <p>IDFObject: Thin wrapper around a dict with attribute access. IDFCollection: Indexed collection of IDFObjects with O(1) lookup.</p>"},{"location":"api/objects/#idfkit.objects.IDFCollection","title":"<code>IDFCollection</code>","text":"<p>Indexed collection of IDFObjects with O(1) lookup by name.</p> <p>Provides list-like iteration and dict-like access by name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")\nZone('Core_ZN')\n&gt;&gt;&gt; zones = model[\"Zone\"]\n&gt;&gt;&gt; len(zones)\n2\n</code></pre> <p>O(1) lookup by name:</p> <pre><code>&gt;&gt;&gt; zones[\"Perimeter_ZN_1\"].name\n'Perimeter_ZN_1'\n&gt;&gt;&gt; zones[0].name\n'Perimeter_ZN_1'\n</code></pre> <p>Attributes:</p> Name Type Description <code>_type</code> <code>str</code> <p>The object type this collection holds</p> <code>_by_name</code> <code>dict[str, IDFObject]</code> <p>Dict mapping uppercase names to objects</p> <code>_items</code> <code>list[IDFObject]</code> <p>Ordered list of objects</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>class IDFCollection:\n    \"\"\"\n    Indexed collection of IDFObjects with O(1) lookup by name.\n\n    Provides list-like iteration and dict-like access by name.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")  # doctest: +ELLIPSIS\n        Zone('Core_ZN')\n        &gt;&gt;&gt; zones = model[\"Zone\"]\n        &gt;&gt;&gt; len(zones)\n        2\n\n        O(1) lookup by name:\n\n        &gt;&gt;&gt; zones[\"Perimeter_ZN_1\"].name\n        'Perimeter_ZN_1'\n        &gt;&gt;&gt; zones[0].name\n        'Perimeter_ZN_1'\n\n    Attributes:\n        _type: The object type this collection holds\n        _by_name: Dict mapping uppercase names to objects\n        _items: Ordered list of objects\n    \"\"\"\n\n    __slots__ = (\"_by_name\", \"_items\", \"_type\")\n\n    _type: str\n    _by_name: dict[str, IDFObject]\n    _items: list[IDFObject]\n\n    def __init__(self, obj_type: str) -&gt; None:\n        self._type = obj_type\n        self._by_name: dict[str, IDFObject] = {}\n        self._items: list[IDFObject] = []\n\n    @property\n    def obj_type(self) -&gt; str:\n        \"\"\"The object type this collection holds.\"\"\"\n        return self._type\n\n    @property\n    def by_name(self) -&gt; dict[str, IDFObject]:\n        \"\"\"Dict mapping uppercase names to objects.\"\"\"\n        return self._by_name\n\n    def add(self, obj: IDFObject) -&gt; IDFObject:\n        \"\"\"\n        Add an object to the collection.\n\n        Args:\n            obj: The IDFObject to add\n\n        Returns:\n            The added object\n\n        Raises:\n            DuplicateObjectError: If an object with the same name exists\n        \"\"\"\n        from .exceptions import DuplicateObjectError\n\n        key = obj.name.upper() if obj.name else \"\"\n        if key and key in self._by_name:\n            raise DuplicateObjectError(self._type, obj.name)\n\n        if key:\n            self._by_name[key] = obj\n        self._items.append(obj)\n        return obj\n\n    def remove(self, obj: IDFObject) -&gt; None:\n        \"\"\"Remove an object from the collection.\"\"\"\n        key = obj.name.upper() if obj.name else \"\"\n        if key in self._by_name:\n            del self._by_name[key]\n        if obj in self._items:\n            self._items.remove(obj)\n\n    def __getitem__(self, key: str | int) -&gt; IDFObject:\n        \"\"\"Get object by name or index.\"\"\"\n        if isinstance(key, int):\n            return self._items[key]\n        result = self._by_name.get(key.upper())\n        if result is None:\n            raise KeyError(f\"No {self._type} with name '{key}'\")  # noqa: TRY003\n        return result\n\n    def __iter__(self) -&gt; Iterator[IDFObject]:\n        return iter(self._items)\n\n    def __len__(self) -&gt; int:\n        return len(self._items)\n\n    def __contains__(self, key: str | IDFObject) -&gt; bool:\n        if isinstance(key, IDFObject):\n            return key in self._items\n        return key.upper() in self._by_name\n\n    def __bool__(self) -&gt; bool:\n        return len(self._items) &gt; 0\n\n    def __repr__(self) -&gt; str:\n        return f\"IDFCollection({self._type}, count={len(self._items)})\"\n\n    def get(self, name: str, default: IDFObject | None = None) -&gt; IDFObject | None:\n        \"\"\"Get object by name with default.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; model[\"Zone\"].get(\"Perimeter_ZN_1\").name\n            'Perimeter_ZN_1'\n            &gt;&gt;&gt; model[\"Zone\"].get(\"NonExistent\") is None\n            True\n        \"\"\"\n        return self._by_name.get(name.upper(), default)\n\n    def first(self) -&gt; IDFObject | None:\n        \"\"\"Get the first object or None.\n\n        Examples:\n            Quickly grab a singleton like Building or SimulationControl:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")  # doctest: +ELLIPSIS\n            Zone('Core_ZN')\n            &gt;&gt;&gt; model[\"Zone\"].first().name\n            'Core_ZN'\n            &gt;&gt;&gt; model[\"Material\"].first() is None\n            True\n        \"\"\"\n        return self._items[0] if self._items else None\n\n    def to_list(self) -&gt; list[IDFObject]:\n        \"\"\"Convert to list.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")  # doctest: +ELLIPSIS\n            Zone('Core_ZN')\n            &gt;&gt;&gt; [z.name for z in model[\"Zone\"].to_list()]\n            ['Perimeter_ZN_1', 'Core_ZN']\n        \"\"\"\n        return list(self._items)\n\n    def to_dict(self) -&gt; list[dict[str, Any]]:\n        \"\"\"Convert all objects to list of dicts (eppy compatibility).\n\n        Useful for feeding zone/material data into pandas or other\n        analysis tools.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\", x_origin=0.0)  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; dicts = model[\"Zone\"].to_dict()\n            &gt;&gt;&gt; dicts[0][\"name\"]\n            'Perimeter_ZN_1'\n        \"\"\"\n        return [obj.to_dict() for obj in self._items]\n\n    def filter(self, predicate: Callable[[IDFObject], bool]) -&gt; list[IDFObject]:\n        \"\"\"Filter objects by predicate function.\n\n        Examples:\n            Find zones on upper floors of a multi-story building:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Ground_Office\", z_origin=0.0)  # doctest: +ELLIPSIS\n            Zone('Ground_Office')\n            &gt;&gt;&gt; model.add(\"Zone\", \"Floor2_Office\", z_origin=3.5)  # doctest: +ELLIPSIS\n            Zone('Floor2_Office')\n            &gt;&gt;&gt; upper = model[\"Zone\"].filter(lambda z: (z.z_origin or 0) &gt; 0)\n            &gt;&gt;&gt; [z.name for z in upper]\n            ['Floor2_Office']\n        \"\"\"\n        return [obj for obj in self._items if predicate(obj)]\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFCollection.by_name","title":"<code>by_name</code>  <code>property</code>","text":"<p>Dict mapping uppercase names to objects.</p>"},{"location":"api/objects/#idfkit.objects.IDFCollection.obj_type","title":"<code>obj_type</code>  <code>property</code>","text":"<p>The object type this collection holds.</p>"},{"location":"api/objects/#idfkit.objects.IDFCollection.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get object by name or index.</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def __getitem__(self, key: str | int) -&gt; IDFObject:\n    \"\"\"Get object by name or index.\"\"\"\n    if isinstance(key, int):\n        return self._items[key]\n    result = self._by_name.get(key.upper())\n    if result is None:\n        raise KeyError(f\"No {self._type} with name '{key}'\")  # noqa: TRY003\n    return result\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFCollection.add","title":"<code>add(obj)</code>","text":"<p>Add an object to the collection.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>IDFObject</code> <p>The IDFObject to add</p> required <p>Returns:</p> Type Description <code>IDFObject</code> <p>The added object</p> <p>Raises:</p> Type Description <code>DuplicateObjectError</code> <p>If an object with the same name exists</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def add(self, obj: IDFObject) -&gt; IDFObject:\n    \"\"\"\n    Add an object to the collection.\n\n    Args:\n        obj: The IDFObject to add\n\n    Returns:\n        The added object\n\n    Raises:\n        DuplicateObjectError: If an object with the same name exists\n    \"\"\"\n    from .exceptions import DuplicateObjectError\n\n    key = obj.name.upper() if obj.name else \"\"\n    if key and key in self._by_name:\n        raise DuplicateObjectError(self._type, obj.name)\n\n    if key:\n        self._by_name[key] = obj\n    self._items.append(obj)\n    return obj\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFCollection.filter","title":"<code>filter(predicate)</code>","text":"<p>Filter objects by predicate function.</p> <p>Examples:</p> <p>Find zones on upper floors of a multi-story building:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Ground_Office\", z_origin=0.0)\nZone('Ground_Office')\n&gt;&gt;&gt; model.add(\"Zone\", \"Floor2_Office\", z_origin=3.5)\nZone('Floor2_Office')\n&gt;&gt;&gt; upper = model[\"Zone\"].filter(lambda z: (z.z_origin or 0) &gt; 0)\n&gt;&gt;&gt; [z.name for z in upper]\n['Floor2_Office']\n</code></pre> Source code in <code>src/idfkit/objects.py</code> <pre><code>def filter(self, predicate: Callable[[IDFObject], bool]) -&gt; list[IDFObject]:\n    \"\"\"Filter objects by predicate function.\n\n    Examples:\n        Find zones on upper floors of a multi-story building:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Ground_Office\", z_origin=0.0)  # doctest: +ELLIPSIS\n        Zone('Ground_Office')\n        &gt;&gt;&gt; model.add(\"Zone\", \"Floor2_Office\", z_origin=3.5)  # doctest: +ELLIPSIS\n        Zone('Floor2_Office')\n        &gt;&gt;&gt; upper = model[\"Zone\"].filter(lambda z: (z.z_origin or 0) &gt; 0)\n        &gt;&gt;&gt; [z.name for z in upper]\n        ['Floor2_Office']\n    \"\"\"\n    return [obj for obj in self._items if predicate(obj)]\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFCollection.first","title":"<code>first()</code>","text":"<p>Get the first object or None.</p> <p>Examples:</p> <p>Quickly grab a singleton like Building or SimulationControl:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")\nZone('Core_ZN')\n&gt;&gt;&gt; model[\"Zone\"].first().name\n'Core_ZN'\n&gt;&gt;&gt; model[\"Material\"].first() is None\nTrue\n</code></pre> Source code in <code>src/idfkit/objects.py</code> <pre><code>def first(self) -&gt; IDFObject | None:\n    \"\"\"Get the first object or None.\n\n    Examples:\n        Quickly grab a singleton like Building or SimulationControl:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")  # doctest: +ELLIPSIS\n        Zone('Core_ZN')\n        &gt;&gt;&gt; model[\"Zone\"].first().name\n        'Core_ZN'\n        &gt;&gt;&gt; model[\"Material\"].first() is None\n        True\n    \"\"\"\n    return self._items[0] if self._items else None\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFCollection.get","title":"<code>get(name, default=None)</code>","text":"<p>Get object by name with default.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; model[\"Zone\"].get(\"Perimeter_ZN_1\").name\n'Perimeter_ZN_1'\n&gt;&gt;&gt; model[\"Zone\"].get(\"NonExistent\") is None\nTrue\n</code></pre> Source code in <code>src/idfkit/objects.py</code> <pre><code>def get(self, name: str, default: IDFObject | None = None) -&gt; IDFObject | None:\n    \"\"\"Get object by name with default.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; model[\"Zone\"].get(\"Perimeter_ZN_1\").name\n        'Perimeter_ZN_1'\n        &gt;&gt;&gt; model[\"Zone\"].get(\"NonExistent\") is None\n        True\n    \"\"\"\n    return self._by_name.get(name.upper(), default)\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFCollection.remove","title":"<code>remove(obj)</code>","text":"<p>Remove an object from the collection.</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def remove(self, obj: IDFObject) -&gt; None:\n    \"\"\"Remove an object from the collection.\"\"\"\n    key = obj.name.upper() if obj.name else \"\"\n    if key in self._by_name:\n        del self._by_name[key]\n    if obj in self._items:\n        self._items.remove(obj)\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFCollection.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert all objects to list of dicts (eppy compatibility).</p> <p>Useful for feeding zone/material data into pandas or other analysis tools.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\", x_origin=0.0)\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; dicts = model[\"Zone\"].to_dict()\n&gt;&gt;&gt; dicts[0][\"name\"]\n'Perimeter_ZN_1'\n</code></pre> Source code in <code>src/idfkit/objects.py</code> <pre><code>def to_dict(self) -&gt; list[dict[str, Any]]:\n    \"\"\"Convert all objects to list of dicts (eppy compatibility).\n\n    Useful for feeding zone/material data into pandas or other\n    analysis tools.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\", x_origin=0.0)  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; dicts = model[\"Zone\"].to_dict()\n        &gt;&gt;&gt; dicts[0][\"name\"]\n        'Perimeter_ZN_1'\n    \"\"\"\n    return [obj.to_dict() for obj in self._items]\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFCollection.to_list","title":"<code>to_list()</code>","text":"<p>Convert to list.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")\nZone('Core_ZN')\n&gt;&gt;&gt; [z.name for z in model[\"Zone\"].to_list()]\n['Perimeter_ZN_1', 'Core_ZN']\n</code></pre> Source code in <code>src/idfkit/objects.py</code> <pre><code>def to_list(self) -&gt; list[IDFObject]:\n    \"\"\"Convert to list.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; model.add(\"Zone\", \"Core_ZN\")  # doctest: +ELLIPSIS\n        Zone('Core_ZN')\n        &gt;&gt;&gt; [z.name for z in model[\"Zone\"].to_list()]\n        ['Perimeter_ZN_1', 'Core_ZN']\n    \"\"\"\n    return list(self._items)\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject","title":"<code>IDFObject</code>","text":"<p>               Bases: <code>EppyObjectMixin</code></p> <p>Lightweight wrapper around a dict representing an EnergyPlus object.</p> <p>Uses slots for memory efficiency - each object is ~200 bytes. Provides attribute access to fields via getattr/setattr.</p> <p>Examples:</p> <p>Create a rigid insulation material and access its properties:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; insulation = model.add(\"Material\", \"XPS_50mm\",\n...     roughness=\"Rough\", thickness=0.05,\n...     conductivity=0.034, density=35.0, specific_heat=1400.0)\n</code></pre> <p>Read thermal properties as attributes:</p> <pre><code>&gt;&gt;&gt; insulation.conductivity\n0.034\n&gt;&gt;&gt; insulation.thickness\n0.05\n</code></pre> <p>Modify for parametric analysis (double the insulation):</p> <pre><code>&gt;&gt;&gt; insulation.thickness = 0.1\n&gt;&gt;&gt; insulation.thickness\n0.1\n</code></pre> <p>Export to a dictionary for use with external tools:</p> <pre><code>&gt;&gt;&gt; d = insulation.to_dict()\n&gt;&gt;&gt; d[\"conductivity\"]\n0.034\n</code></pre> <p>Attributes:</p> Name Type Description <code>_type</code> <code>str</code> <p>The IDF object type (e.g., \"Zone\", \"Material\")</p> <code>_name</code> <code>str</code> <p>The object's name (first field)</p> <code>_data</code> <code>dict[str, Any]</code> <p>Dict of field_name -&gt; value</p> <code>_schema</code> <code>dict[str, Any] | None</code> <p>Optional schema dict for validation</p> <code>_document</code> <code>IDFDocument | None</code> <p>Reference to parent document (for reference resolution)</p> <code>_field_order</code> <code>list[str] | None</code> <p>Ordered list of field names from schema</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>class IDFObject(EppyObjectMixin):\n    \"\"\"\n    Lightweight wrapper around a dict representing an EnergyPlus object.\n\n    Uses __slots__ for memory efficiency - each object is ~200 bytes.\n    Provides attribute access to fields via __getattr__/__setattr__.\n\n    Examples:\n        Create a rigid insulation material and access its properties:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; insulation = model.add(\"Material\", \"XPS_50mm\",\n        ...     roughness=\"Rough\", thickness=0.05,\n        ...     conductivity=0.034, density=35.0, specific_heat=1400.0)\n\n        Read thermal properties as attributes:\n\n        &gt;&gt;&gt; insulation.conductivity\n        0.034\n        &gt;&gt;&gt; insulation.thickness\n        0.05\n\n        Modify for parametric analysis (double the insulation):\n\n        &gt;&gt;&gt; insulation.thickness = 0.1\n        &gt;&gt;&gt; insulation.thickness\n        0.1\n\n        Export to a dictionary for use with external tools:\n\n        &gt;&gt;&gt; d = insulation.to_dict()\n        &gt;&gt;&gt; d[\"conductivity\"]\n        0.034\n\n    Attributes:\n        _type: The IDF object type (e.g., \"Zone\", \"Material\")\n        _name: The object's name (first field)\n        _data: Dict of field_name -&gt; value\n        _schema: Optional schema dict for validation\n        _document: Reference to parent document (for reference resolution)\n        _field_order: Ordered list of field names from schema\n    \"\"\"\n\n    __slots__ = (\n        \"__weakref__\",\n        \"_data\",\n        \"_document\",\n        \"_field_order\",\n        \"_name\",\n        \"_ref_fields\",\n        \"_schema\",\n        \"_type\",\n        \"_version\",\n    )\n\n    _type: str\n    _name: str\n    _data: dict[str, Any]\n    _schema: dict[str, Any] | None\n    _document: IDFDocument | None\n    _field_order: list[str] | None\n    _ref_fields: frozenset[str] | None\n\n    def __init__(\n        self,\n        obj_type: str,\n        name: str,\n        data: dict[str, Any] | None = None,\n        schema: dict[str, Any] | None = None,\n        document: IDFDocument | None = None,\n        field_order: list[str] | None = None,\n        ref_fields: frozenset[str] | None = None,\n    ) -&gt; None:\n        object.__setattr__(self, \"_type\", obj_type)\n        object.__setattr__(self, \"_name\", name)\n        object.__setattr__(self, \"_data\", data if data is not None else {})\n        object.__setattr__(self, \"_schema\", schema)\n        object.__setattr__(self, \"_document\", document)\n        object.__setattr__(self, \"_field_order\", field_order)\n        object.__setattr__(self, \"_ref_fields\", ref_fields)\n        object.__setattr__(self, \"_version\", 0)\n\n    @property\n    def obj_type(self) -&gt; str:\n        \"\"\"The IDF object type (e.g., 'Zone', 'Material').\"\"\"\n        return self._type\n\n    @property\n    def mutation_version(self) -&gt; int:\n        \"\"\"Monotonically increasing counter bumped on every field write.\n\n        Useful for caches that need to detect whether an object has been\n        modified since a cached value was computed.\n        \"\"\"\n        return self._version\n\n    @property\n    def data(self) -&gt; dict[str, Any]:\n        \"\"\"The field data dictionary.\"\"\"\n        return self._data\n\n    @property\n    def schema_dict(self) -&gt; dict[str, Any] | None:\n        \"\"\"The schema dict for this object type.\"\"\"\n        return self._schema\n\n    @property\n    def field_order(self) -&gt; list[str] | None:\n        \"\"\"Ordered list of field names from schema.\"\"\"\n        return self._field_order\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"The object's name.\"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value: str) -&gt; None:\n        \"\"\"Set the object's name.\"\"\"\n        self._set_name(value)\n\n    def __getattr__(self, key: str) -&gt; Any:\n        \"\"\"Get field value by attribute name.\n\n        When the parent document has ``strict=True``, accessing a field\n        name that is neither present in the data dict nor recognised by\n        the schema raises ``AttributeError`` instead of returning\n        ``None``.  This catches typos during migration.\n        \"\"\"\n        if key.startswith(\"_\"):\n            raise AttributeError(key)\n\n        # Try exact match first\n        data = object.__getattribute__(self, \"_data\")\n        if key in data:\n            return data[key]\n\n        # Try lowercase version\n        key_lower = key.lower()\n        if key_lower in data:\n            return data[key_lower]\n\n        # Try python name conversion\n        python_key = to_python_name(key)\n        if python_key in data:\n            return data[python_key]\n\n        # Field not found \u2014 check strict mode\n        doc = object.__getattribute__(self, \"_document\")\n        if doc is not None and getattr(doc, \"_strict\", False):\n            # In strict mode, only allow known schema fields\n            field_order = object.__getattribute__(self, \"_field_order\")\n            if field_order is not None and python_key not in field_order:\n                obj_type = object.__getattribute__(self, \"_type\")\n                raise AttributeError(  # noqa: TRY003\n                    f\"'{obj_type}' object has no field '{key}'. \"\n                    f\"Known fields: {', '.join(field_order[:10])}{'...' if len(field_order) &gt; 10 else ''}\"\n                )\n\n        # Default: return None (eppy behaviour)\n        return None\n\n    def __setattr__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set field value by attribute name.\"\"\"\n        if key.startswith(\"_\"):\n            object.__setattr__(self, key, value)\n        elif key.lower() == \"name\":\n            self._set_name(value)\n        else:\n            # Normalize key to python style\n            python_key = to_python_name(key)\n            self._set_field(python_key, value)\n\n    def __getitem__(self, key: str | int) -&gt; Any:\n        \"\"\"Get field value by name or index.\"\"\"\n        if isinstance(key, int):\n            if key == 0:\n                return self._name\n            if self._field_order and 0 &lt; key &lt;= len(self._field_order):\n                field_name = self._field_order[key - 1]\n                return self._data.get(field_name)\n            raise IndexError(f\"Field index {key} out of range\")  # noqa: TRY003\n        return getattr(self, key)\n\n    def __setitem__(self, key: str | int, value: Any) -&gt; None:\n        \"\"\"Set field value by name or index.\"\"\"\n        if isinstance(key, int):\n            if key == 0:\n                self._set_name(value)\n            elif self._field_order and 0 &lt; key &lt;= len(self._field_order):\n                field_name = self._field_order[key - 1]\n                self._set_field(field_name, value)\n            else:\n                raise IndexError(f\"Field index {key} out of range\")  # noqa: TRY003\n        else:\n            setattr(self, key, value)\n\n    def __repr__(self) -&gt; str:\n        return f\"{self._type}('{self._name}')\"\n\n    def __str__(self) -&gt; str:\n        return f\"{self._type}: {self._name}\"\n\n    def __eq__(self, other: object) -&gt; bool:\n        if not isinstance(other, IDFObject):\n            return NotImplemented\n        return self._type == other._type and self._name == other._name and self._data == other._data\n\n    def __hash__(self) -&gt; int:\n        return id(self)\n\n    def _set_name(self, value: str) -&gt; None:\n        \"\"\"Centralized name-change logic with document notification.\"\"\"\n        old = self._name\n        if old == value:\n            return\n        object.__setattr__(self, \"_name\", value)\n        object.__setattr__(self, \"_version\", self._version + 1)\n        doc = self._document\n        if doc is not None:\n            doc.notify_name_change(self, old, value)\n\n    def _set_field(self, python_key: str, value: Any) -&gt; None:\n        \"\"\"Centralized data-field write with reference graph notification.\"\"\"\n        doc = self._document\n        ref_fields = self._ref_fields\n        if doc is not None and ref_fields is not None and python_key in ref_fields:\n            old = self._data.get(python_key)\n            self._data[python_key] = value\n            if old != value:\n                doc.notify_reference_change(self, python_key, old, value)\n        else:\n            self._data[python_key] = value\n        object.__setattr__(self, \"_version\", self._version + 1)\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert to dictionary representation.\n\n        Useful for serializing EnergyPlus objects to JSON, CSV, or\n        DataFrames for post-processing.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; mat = model.add(\"Material\", \"Concrete_200mm\",\n            ...     roughness=\"MediumRough\", thickness=0.2,\n            ...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n            &gt;&gt;&gt; d = mat.to_dict()\n            &gt;&gt;&gt; d[\"name\"], d[\"thickness\"], d[\"conductivity\"]\n            ('Concrete_200mm', 0.2, 1.4)\n        \"\"\"\n        return {\"name\": self._name, **self._data}\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get field value with default.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; mat = model.add(\"Material\", \"Concrete_200mm\",\n            ...     roughness=\"MediumRough\", thickness=0.2,\n            ...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n            &gt;&gt;&gt; mat.get(\"conductivity\")\n            1.4\n            &gt;&gt;&gt; mat.get(\"thermal_absorptance\", 0.9)\n            0.9\n        \"\"\"\n        value = getattr(self, key)\n        return value if value is not None else default\n\n    def copy(self) -&gt; IDFObject:\n        \"\"\"Create a copy of this object.\"\"\"\n        return IDFObject(\n            obj_type=self._type,\n            name=self._name,\n            data=dict(self._data),\n            schema=self._schema,\n            document=None,  # Don't copy document reference\n            field_order=self._field_order,\n            ref_fields=self._ref_fields,\n        )\n\n    def __dir__(self) -&gt; list[str]:\n        \"\"\"Return attributes for tab completion (includes schema field names).\"\"\"\n        attrs = [\n            \"obj_type\",\n            \"name\",\n            \"data\",\n            \"key\",\n            \"Name\",\n            \"fieldnames\",\n            \"fieldvalues\",\n            \"theidf\",\n            \"schema_dict\",\n            \"field_order\",\n            \"to_dict\",\n            \"get\",\n            \"copy\",\n            \"get_field_idd\",\n            \"get_referenced_object\",\n            \"getfieldidd\",\n            \"getfieldidd_item\",\n            \"getrange\",\n            \"checkrange\",\n            \"getreferingobjs\",\n        ]\n        field_order = object.__getattribute__(self, \"_field_order\")\n        if field_order:\n            attrs.extend(field_order)\n        else:\n            data = object.__getattribute__(self, \"_data\")\n            attrs.extend(data.keys())\n        return attrs\n\n    def _repr_svg_(self) -&gt; str | None:\n        \"\"\"Return SVG representation for Jupyter/IPython display.\n\n        Currently supports Construction objects, rendering a cross-section\n        diagram showing layer sequence, thicknesses, and thermal properties.\n\n        Returns:\n            SVG string for Construction objects, None for other types.\n        \"\"\"\n        if self._type != \"Construction\":\n            return None\n\n        if self._document is None:\n            # Need document to resolve material references\n            return None\n\n        try:\n            from .visualization.svg import construction_to_svg\n\n            return construction_to_svg(self)\n        except Exception:\n            # Fail gracefully - fall back to text repr\n            return None\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject.data","title":"<code>data</code>  <code>property</code>","text":"<p>The field data dictionary.</p>"},{"location":"api/objects/#idfkit.objects.IDFObject.field_order","title":"<code>field_order</code>  <code>property</code>","text":"<p>Ordered list of field names from schema.</p>"},{"location":"api/objects/#idfkit.objects.IDFObject.mutation_version","title":"<code>mutation_version</code>  <code>property</code>","text":"<p>Monotonically increasing counter bumped on every field write.</p> <p>Useful for caches that need to detect whether an object has been modified since a cached value was computed.</p>"},{"location":"api/objects/#idfkit.objects.IDFObject.name","title":"<code>name</code>  <code>property</code> <code>writable</code>","text":"<p>The object's name.</p>"},{"location":"api/objects/#idfkit.objects.IDFObject.obj_type","title":"<code>obj_type</code>  <code>property</code>","text":"<p>The IDF object type (e.g., 'Zone', 'Material').</p>"},{"location":"api/objects/#idfkit.objects.IDFObject.schema_dict","title":"<code>schema_dict</code>  <code>property</code>","text":"<p>The schema dict for this object type.</p>"},{"location":"api/objects/#idfkit.objects.IDFObject.__dir__","title":"<code>__dir__()</code>","text":"<p>Return attributes for tab completion (includes schema field names).</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def __dir__(self) -&gt; list[str]:\n    \"\"\"Return attributes for tab completion (includes schema field names).\"\"\"\n    attrs = [\n        \"obj_type\",\n        \"name\",\n        \"data\",\n        \"key\",\n        \"Name\",\n        \"fieldnames\",\n        \"fieldvalues\",\n        \"theidf\",\n        \"schema_dict\",\n        \"field_order\",\n        \"to_dict\",\n        \"get\",\n        \"copy\",\n        \"get_field_idd\",\n        \"get_referenced_object\",\n        \"getfieldidd\",\n        \"getfieldidd_item\",\n        \"getrange\",\n        \"checkrange\",\n        \"getreferingobjs\",\n    ]\n    field_order = object.__getattribute__(self, \"_field_order\")\n    if field_order:\n        attrs.extend(field_order)\n    else:\n        data = object.__getattribute__(self, \"_data\")\n        attrs.extend(data.keys())\n    return attrs\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject.__getattr__","title":"<code>__getattr__(key)</code>","text":"<p>Get field value by attribute name.</p> <p>When the parent document has <code>strict=True</code>, accessing a field name that is neither present in the data dict nor recognised by the schema raises <code>AttributeError</code> instead of returning <code>None</code>.  This catches typos during migration.</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def __getattr__(self, key: str) -&gt; Any:\n    \"\"\"Get field value by attribute name.\n\n    When the parent document has ``strict=True``, accessing a field\n    name that is neither present in the data dict nor recognised by\n    the schema raises ``AttributeError`` instead of returning\n    ``None``.  This catches typos during migration.\n    \"\"\"\n    if key.startswith(\"_\"):\n        raise AttributeError(key)\n\n    # Try exact match first\n    data = object.__getattribute__(self, \"_data\")\n    if key in data:\n        return data[key]\n\n    # Try lowercase version\n    key_lower = key.lower()\n    if key_lower in data:\n        return data[key_lower]\n\n    # Try python name conversion\n    python_key = to_python_name(key)\n    if python_key in data:\n        return data[python_key]\n\n    # Field not found \u2014 check strict mode\n    doc = object.__getattribute__(self, \"_document\")\n    if doc is not None and getattr(doc, \"_strict\", False):\n        # In strict mode, only allow known schema fields\n        field_order = object.__getattribute__(self, \"_field_order\")\n        if field_order is not None and python_key not in field_order:\n            obj_type = object.__getattribute__(self, \"_type\")\n            raise AttributeError(  # noqa: TRY003\n                f\"'{obj_type}' object has no field '{key}'. \"\n                f\"Known fields: {', '.join(field_order[:10])}{'...' if len(field_order) &gt; 10 else ''}\"\n            )\n\n    # Default: return None (eppy behaviour)\n    return None\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get field value by name or index.</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def __getitem__(self, key: str | int) -&gt; Any:\n    \"\"\"Get field value by name or index.\"\"\"\n    if isinstance(key, int):\n        if key == 0:\n            return self._name\n        if self._field_order and 0 &lt; key &lt;= len(self._field_order):\n            field_name = self._field_order[key - 1]\n            return self._data.get(field_name)\n        raise IndexError(f\"Field index {key} out of range\")  # noqa: TRY003\n    return getattr(self, key)\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject.__setattr__","title":"<code>__setattr__(key, value)</code>","text":"<p>Set field value by attribute name.</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def __setattr__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set field value by attribute name.\"\"\"\n    if key.startswith(\"_\"):\n        object.__setattr__(self, key, value)\n    elif key.lower() == \"name\":\n        self._set_name(value)\n    else:\n        # Normalize key to python style\n        python_key = to_python_name(key)\n        self._set_field(python_key, value)\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set field value by name or index.</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def __setitem__(self, key: str | int, value: Any) -&gt; None:\n    \"\"\"Set field value by name or index.\"\"\"\n    if isinstance(key, int):\n        if key == 0:\n            self._set_name(value)\n        elif self._field_order and 0 &lt; key &lt;= len(self._field_order):\n            field_name = self._field_order[key - 1]\n            self._set_field(field_name, value)\n        else:\n            raise IndexError(f\"Field index {key} out of range\")  # noqa: TRY003\n    else:\n        setattr(self, key, value)\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject.copy","title":"<code>copy()</code>","text":"<p>Create a copy of this object.</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def copy(self) -&gt; IDFObject:\n    \"\"\"Create a copy of this object.\"\"\"\n    return IDFObject(\n        obj_type=self._type,\n        name=self._name,\n        data=dict(self._data),\n        schema=self._schema,\n        document=None,  # Don't copy document reference\n        field_order=self._field_order,\n        ref_fields=self._ref_fields,\n    )\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject.get","title":"<code>get(key, default=None)</code>","text":"<p>Get field value with default.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; mat = model.add(\"Material\", \"Concrete_200mm\",\n...     roughness=\"MediumRough\", thickness=0.2,\n...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n&gt;&gt;&gt; mat.get(\"conductivity\")\n1.4\n&gt;&gt;&gt; mat.get(\"thermal_absorptance\", 0.9)\n0.9\n</code></pre> Source code in <code>src/idfkit/objects.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get field value with default.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; mat = model.add(\"Material\", \"Concrete_200mm\",\n        ...     roughness=\"MediumRough\", thickness=0.2,\n        ...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n        &gt;&gt;&gt; mat.get(\"conductivity\")\n        1.4\n        &gt;&gt;&gt; mat.get(\"thermal_absorptance\", 0.9)\n        0.9\n    \"\"\"\n    value = getattr(self, key)\n    return value if value is not None else default\n</code></pre>"},{"location":"api/objects/#idfkit.objects.IDFObject.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Useful for serializing EnergyPlus objects to JSON, CSV, or DataFrames for post-processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; mat = model.add(\"Material\", \"Concrete_200mm\",\n...     roughness=\"MediumRough\", thickness=0.2,\n...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n&gt;&gt;&gt; d = mat.to_dict()\n&gt;&gt;&gt; d[\"name\"], d[\"thickness\"], d[\"conductivity\"]\n('Concrete_200mm', 0.2, 1.4)\n</code></pre> Source code in <code>src/idfkit/objects.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert to dictionary representation.\n\n    Useful for serializing EnergyPlus objects to JSON, CSV, or\n    DataFrames for post-processing.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; mat = model.add(\"Material\", \"Concrete_200mm\",\n        ...     roughness=\"MediumRough\", thickness=0.2,\n        ...     conductivity=1.4, density=2240.0, specific_heat=900.0)\n        &gt;&gt;&gt; d = mat.to_dict()\n        &gt;&gt;&gt; d[\"name\"], d[\"thickness\"], d[\"conductivity\"]\n        ('Concrete_200mm', 0.2, 1.4)\n    \"\"\"\n    return {\"name\": self._name, **self._data}\n</code></pre>"},{"location":"api/objects/#idfkit.objects.to_idf_name","title":"<code>to_idf_name(python_name)</code>","text":"<p>Convert Python name back to IDF-style name.</p> <p>'direction_of_relative_north' -&gt; 'Direction of Relative North'</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def to_idf_name(python_name: str) -&gt; str:\n    \"\"\"Convert Python name back to IDF-style name.\n\n    'direction_of_relative_north' -&gt; 'Direction of Relative North'\n    \"\"\"\n    return \" \".join(word.capitalize() for word in python_name.split(\"_\"))\n</code></pre>"},{"location":"api/objects/#idfkit.objects.to_python_name","title":"<code>to_python_name(idf_name)</code>","text":"<p>Convert IDF field name to Python-friendly name.</p> <p>'Direction of Relative North' -&gt; 'direction_of_relative_north' 'X Origin' -&gt; 'x_origin'</p> Source code in <code>src/idfkit/objects.py</code> <pre><code>def to_python_name(idf_name: str) -&gt; str:\n    \"\"\"Convert IDF field name to Python-friendly name.\n\n    'Direction of Relative North' -&gt; 'direction_of_relative_north'\n    'X Origin' -&gt; 'x_origin'\n    \"\"\"\n    return _FIELD_NAME_PATTERN.sub(\"_\", idf_name.lower()).strip(\"_\")\n</code></pre>"},{"location":"api/references/","title":"References","text":"<p><code>ReferenceGraph</code> maintains a bidirectional index of cross-object references. It powers <code>doc.get_referencing()</code> and <code>doc.get_references()</code> and is automatically kept in sync as objects are added, removed, or renamed.</p> <p>Reference graph for tracking object dependencies.</p> <p>Provides O(1) lookups for: - What objects reference a given name? - What names does an object reference? - Validation of reference integrity</p>"},{"location":"api/references/#idfkit.references.ReferenceGraph","title":"<code>ReferenceGraph</code>","text":"<p>Tracks object references for instant dependency queries.</p> <p>The graph maintains two indexes: - _referenced_by: name -&gt; set of objects that reference it - _references: object -&gt; set of names it references</p> <p>This enables O(1) lookups for common operations like: - Finding all surfaces in a zone - Finding all objects using a construction - Detecting dangling references</p> <p>The reference graph is automatically maintained by IDFDocument when objects are added, removed, or when reference fields are modified.</p> <p>Examples:</p> <p>The reference graph automatically tracks which objects point to which names.  For instance, when a surface references a zone, that link is available for instant queries:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\",\n...     zone_name=\"Perimeter_ZN_1\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     validate=False)\nBuildingSurface:Detailed('South_Wall')\n&gt;&gt;&gt; model.references.is_referenced(\"Perimeter_ZN_1\")\nTrue\n&gt;&gt;&gt; stats = model.references.stats()\n&gt;&gt;&gt; stats[\"total_references\"] &gt;= 1\nTrue\n</code></pre> Source code in <code>src/idfkit/references.py</code> <pre><code>class ReferenceGraph:\n    \"\"\"\n    Tracks object references for instant dependency queries.\n\n    The graph maintains two indexes:\n    - _referenced_by: name -&gt; set of objects that reference it\n    - _references: object -&gt; set of names it references\n\n    This enables O(1) lookups for common operations like:\n    - Finding all surfaces in a zone\n    - Finding all objects using a construction\n    - Detecting dangling references\n\n    The reference graph is automatically maintained by\n    [IDFDocument][idfkit.document.IDFDocument] when objects are added,\n    removed, or when reference fields are modified.\n\n    Examples:\n        The reference graph automatically tracks which objects point\n        to which names.  For instance, when a surface references a\n        zone, that link is available for instant queries:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\",\n        ...     zone_name=\"Perimeter_ZN_1\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     validate=False)  # doctest: +ELLIPSIS\n        BuildingSurface:Detailed('South_Wall')\n        &gt;&gt;&gt; model.references.is_referenced(\"Perimeter_ZN_1\")\n        True\n        &gt;&gt;&gt; stats = model.references.stats()\n        &gt;&gt;&gt; stats[\"total_references\"] &gt;= 1\n        True\n    \"\"\"\n\n    __slots__ = (\"_object_lists\", \"_referenced_by\", \"_references\")\n\n    def __init__(self) -&gt; None:\n        # name (uppercase) -&gt; set of (object, field_name) tuples that reference it\n        self._referenced_by: dict[str, set[tuple[IDFObject, str]]] = defaultdict(set)\n        # object -&gt; set of (name_uppercase, field_name) tuples it references\n        self._references: dict[IDFObject, set[tuple[str, str]]] = defaultdict(set)\n        # object_list name -&gt; set of object types that provide names for it\n        self._object_lists: dict[str, set[str]] = defaultdict(set)\n\n    def register_object_list(self, list_name: str, obj_type: str) -&gt; None:\n        \"\"\"Register that an object type provides names for an object-list.\"\"\"\n        self._object_lists[list_name].add(obj_type)\n\n    def register(self, obj: IDFObject, field_name: str, referenced_name: str) -&gt; None:\n        \"\"\"\n        Register that an object references another name.\n\n        Args:\n            obj: The object that contains the reference\n            field_name: The field that contains the reference\n            referenced_name: The name being referenced\n        \"\"\"\n        if not referenced_name:\n            return\n\n        name_upper = referenced_name.upper()\n        self._referenced_by[name_upper].add((obj, field_name))\n        self._references[obj].add((name_upper, field_name))\n\n    def unregister(self, obj: IDFObject) -&gt; None:\n        \"\"\"Remove all reference tracking for an object.\"\"\"\n        if obj in self._references:\n            # Remove from referenced_by\n            for name_upper, field_name in self._references[obj]:\n                if name_upper in self._referenced_by:\n                    self._referenced_by[name_upper].discard((obj, field_name))\n                    if not self._referenced_by[name_upper]:\n                        del self._referenced_by[name_upper]\n            del self._references[obj]\n\n        # Also remove any references TO this object\n        obj_name_upper = obj.name.upper() if obj.name else \"\"\n        if obj_name_upper in self._referenced_by:\n            del self._referenced_by[obj_name_upper]\n\n    def get_referencing(self, name: str) -&gt; set[IDFObject]:\n        \"\"\"\n        O(1): Get all objects that reference a given name.\n\n        Args:\n            name: The name to look up\n\n        Returns:\n            Set of IDFObjects that reference this name\n\n        Examples:\n            Find all surfaces assigned to a zone (O(1)):\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n            ...     surface_type=\"Wall\", construction_name=\"\",\n            ...     zone_name=\"Perimeter_ZN_1\",\n            ...     outside_boundary_condition=\"Outdoors\",\n            ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n            ...     validate=False)  # doctest: +ELLIPSIS\n            BuildingSurface:Detailed('South_Wall')\n            &gt;&gt;&gt; len(model.references.get_referencing(\"Perimeter_ZN_1\"))\n            1\n        \"\"\"\n        refs = self._referenced_by.get(name.upper(), set())\n        return {obj for obj, _ in refs}\n\n    def get_referencing_with_fields(self, name: str) -&gt; set[tuple[IDFObject, str]]:\n        \"\"\"\n        O(1): Get all (object, field_name) pairs that reference a given name.\n\n        Args:\n            name: The name to look up\n\n        Returns:\n            Set of (IDFObject, field_name) tuples\n        \"\"\"\n        return self._referenced_by.get(name.upper(), set()).copy()\n\n    def get_references(self, obj: IDFObject) -&gt; set[str]:\n        \"\"\"\n        O(1): Get all names that an object references.\n\n        Args:\n            obj: The object to look up\n\n        Returns:\n            Set of names (uppercase) that this object references\n        \"\"\"\n        refs = self._references.get(obj, set())\n        return {name for name, _ in refs}\n\n    def get_references_with_fields(self, obj: IDFObject) -&gt; set[tuple[str, str]]:\n        \"\"\"\n        O(1): Get all (name, field_name) pairs that an object references.\n\n        Args:\n            obj: The object to look up\n\n        Returns:\n            Set of (name, field_name) tuples\n        \"\"\"\n        return self._references.get(obj, set()).copy()\n\n    def is_referenced(self, name: str) -&gt; bool:\n        \"\"\"Check if a name is referenced by any object.\n\n        Examples:\n            Check whether a zone is used by any surface before deleting it:\n\n            &gt;&gt;&gt; from idfkit import new_document\n            &gt;&gt;&gt; model = new_document()\n            &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n            Zone('Perimeter_ZN_1')\n            &gt;&gt;&gt; model.references.is_referenced(\"Perimeter_ZN_1\")\n            False\n            &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n            ...     surface_type=\"Wall\", construction_name=\"\",\n            ...     zone_name=\"Perimeter_ZN_1\",\n            ...     outside_boundary_condition=\"Outdoors\",\n            ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n            ...     validate=False)  # doctest: +ELLIPSIS\n            BuildingSurface:Detailed('South_Wall')\n            &gt;&gt;&gt; model.references.is_referenced(\"Perimeter_ZN_1\")\n            True\n        \"\"\"\n        return name.upper() in self._referenced_by\n\n    def get_dangling_references(self, valid_names: set[str]) -&gt; Iterator[tuple[IDFObject, str, str]]:\n        \"\"\"\n        Find all references to non-existent objects.\n\n        Args:\n            valid_names: Set of valid object names (uppercase)\n\n        Yields:\n            Tuples of (source_object, field_name, referenced_name)\n        \"\"\"\n        valid_upper = {n.upper() for n in valid_names}\n\n        for obj, refs in self._references.items():\n            for name_upper, field_name in refs:\n                if name_upper not in valid_upper:\n                    yield (obj, field_name, name_upper)\n\n    def rename_target(self, old_name: str, new_name: str) -&gt; None:\n        \"\"\"\n        Update indexes when a referenced target is renamed.\n\n        Moves _referenced_by[OLD] -&gt; _referenced_by[NEW] and updates\n        corresponding _references entries for all affected objects.\n\n        Args:\n            old_name: The old target name\n            new_name: The new target name\n        \"\"\"\n        old_upper = old_name.upper()\n        new_upper = new_name.upper()\n        if old_upper == new_upper:\n            return\n\n        referrers = self._referenced_by.pop(old_upper, set())\n        if not referrers:\n            return\n\n        # Update _references for each referring object\n        for obj, field_name in referrers:\n            obj_refs = self._references.get(obj)\n            if obj_refs is not None:\n                obj_refs.discard((old_upper, field_name))\n                obj_refs.add((new_upper, field_name))\n\n        # Merge into new key (there may already be refs to new_name)\n        if new_upper in self._referenced_by:\n            self._referenced_by[new_upper].update(referrers)\n        else:\n            self._referenced_by[new_upper] = referrers\n\n    def update_reference(self, obj: IDFObject, field_name: str, old_value: str | None, new_value: str | None) -&gt; None:\n        \"\"\"\n        Update indexes when an object's reference field changes.\n\n        Removes the old entry from both indexes and adds the new entry.\n\n        Args:\n            obj: The object whose field changed\n            field_name: The field that changed\n            old_value: The previous referenced name (or None)\n            new_value: The new referenced name (or None)\n        \"\"\"\n        # Remove old\n        if old_value:\n            old_upper = old_value.upper()\n            refs_set = self._referenced_by.get(old_upper)\n            if refs_set is not None:\n                refs_set.discard((obj, field_name))\n                if not refs_set:\n                    del self._referenced_by[old_upper]\n            obj_refs = self._references.get(obj)\n            if obj_refs is not None:\n                obj_refs.discard((old_upper, field_name))\n\n        # Add new\n        if new_value and new_value.strip():\n            new_upper = new_value.upper()\n            self._referenced_by[new_upper].add((obj, field_name))\n            self._references[obj].add((new_upper, field_name))\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all reference tracking.\"\"\"\n        self._referenced_by.clear()\n        self._references.clear()\n        self._object_lists.clear()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return total number of references tracked.\"\"\"\n        return sum(len(refs) for refs in self._references.values())\n\n    def stats(self) -&gt; dict[str, int]:\n        \"\"\"Return statistics about the reference graph.\"\"\"\n        return {\n            \"total_references\": len(self),\n            \"objects_with_references\": len(self._references),\n            \"names_referenced\": len(self._referenced_by),\n            \"object_lists\": len(self._object_lists),\n        }\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.__len__","title":"<code>__len__()</code>","text":"<p>Return total number of references tracked.</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return total number of references tracked.\"\"\"\n    return sum(len(refs) for refs in self._references.values())\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.clear","title":"<code>clear()</code>","text":"<p>Clear all reference tracking.</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all reference tracking.\"\"\"\n    self._referenced_by.clear()\n    self._references.clear()\n    self._object_lists.clear()\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.get_dangling_references","title":"<code>get_dangling_references(valid_names)</code>","text":"<p>Find all references to non-existent objects.</p> <p>Parameters:</p> Name Type Description Default <code>valid_names</code> <code>set[str]</code> <p>Set of valid object names (uppercase)</p> required <p>Yields:</p> Type Description <code>tuple[IDFObject, str, str]</code> <p>Tuples of (source_object, field_name, referenced_name)</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def get_dangling_references(self, valid_names: set[str]) -&gt; Iterator[tuple[IDFObject, str, str]]:\n    \"\"\"\n    Find all references to non-existent objects.\n\n    Args:\n        valid_names: Set of valid object names (uppercase)\n\n    Yields:\n        Tuples of (source_object, field_name, referenced_name)\n    \"\"\"\n    valid_upper = {n.upper() for n in valid_names}\n\n    for obj, refs in self._references.items():\n        for name_upper, field_name in refs:\n            if name_upper not in valid_upper:\n                yield (obj, field_name, name_upper)\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.get_references","title":"<code>get_references(obj)</code>","text":"<p>O(1): Get all names that an object references.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>IDFObject</code> <p>The object to look up</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of names (uppercase) that this object references</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def get_references(self, obj: IDFObject) -&gt; set[str]:\n    \"\"\"\n    O(1): Get all names that an object references.\n\n    Args:\n        obj: The object to look up\n\n    Returns:\n        Set of names (uppercase) that this object references\n    \"\"\"\n    refs = self._references.get(obj, set())\n    return {name for name, _ in refs}\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.get_references_with_fields","title":"<code>get_references_with_fields(obj)</code>","text":"<p>O(1): Get all (name, field_name) pairs that an object references.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>IDFObject</code> <p>The object to look up</p> required <p>Returns:</p> Type Description <code>set[tuple[str, str]]</code> <p>Set of (name, field_name) tuples</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def get_references_with_fields(self, obj: IDFObject) -&gt; set[tuple[str, str]]:\n    \"\"\"\n    O(1): Get all (name, field_name) pairs that an object references.\n\n    Args:\n        obj: The object to look up\n\n    Returns:\n        Set of (name, field_name) tuples\n    \"\"\"\n    return self._references.get(obj, set()).copy()\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.get_referencing","title":"<code>get_referencing(name)</code>","text":"<p>O(1): Get all objects that reference a given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to look up</p> required <p>Returns:</p> Type Description <code>set[IDFObject]</code> <p>Set of IDFObjects that reference this name</p> <p>Examples:</p> <p>Find all surfaces assigned to a zone (O(1)):</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\",\n...     zone_name=\"Perimeter_ZN_1\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     validate=False)\nBuildingSurface:Detailed('South_Wall')\n&gt;&gt;&gt; len(model.references.get_referencing(\"Perimeter_ZN_1\"))\n1\n</code></pre> Source code in <code>src/idfkit/references.py</code> <pre><code>def get_referencing(self, name: str) -&gt; set[IDFObject]:\n    \"\"\"\n    O(1): Get all objects that reference a given name.\n\n    Args:\n        name: The name to look up\n\n    Returns:\n        Set of IDFObjects that reference this name\n\n    Examples:\n        Find all surfaces assigned to a zone (O(1)):\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\",\n        ...     zone_name=\"Perimeter_ZN_1\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     validate=False)  # doctest: +ELLIPSIS\n        BuildingSurface:Detailed('South_Wall')\n        &gt;&gt;&gt; len(model.references.get_referencing(\"Perimeter_ZN_1\"))\n        1\n    \"\"\"\n    refs = self._referenced_by.get(name.upper(), set())\n    return {obj for obj, _ in refs}\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.get_referencing_with_fields","title":"<code>get_referencing_with_fields(name)</code>","text":"<p>O(1): Get all (object, field_name) pairs that reference a given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to look up</p> required <p>Returns:</p> Type Description <code>set[tuple[IDFObject, str]]</code> <p>Set of (IDFObject, field_name) tuples</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def get_referencing_with_fields(self, name: str) -&gt; set[tuple[IDFObject, str]]:\n    \"\"\"\n    O(1): Get all (object, field_name) pairs that reference a given name.\n\n    Args:\n        name: The name to look up\n\n    Returns:\n        Set of (IDFObject, field_name) tuples\n    \"\"\"\n    return self._referenced_by.get(name.upper(), set()).copy()\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.is_referenced","title":"<code>is_referenced(name)</code>","text":"<p>Check if a name is referenced by any object.</p> <p>Examples:</p> <p>Check whether a zone is used by any surface before deleting it:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; model.references.is_referenced(\"Perimeter_ZN_1\")\nFalse\n&gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n...     surface_type=\"Wall\", construction_name=\"\",\n...     zone_name=\"Perimeter_ZN_1\",\n...     outside_boundary_condition=\"Outdoors\",\n...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n...     validate=False)\nBuildingSurface:Detailed('South_Wall')\n&gt;&gt;&gt; model.references.is_referenced(\"Perimeter_ZN_1\")\nTrue\n</code></pre> Source code in <code>src/idfkit/references.py</code> <pre><code>def is_referenced(self, name: str) -&gt; bool:\n    \"\"\"Check if a name is referenced by any object.\n\n    Examples:\n        Check whether a zone is used by any surface before deleting it:\n\n        &gt;&gt;&gt; from idfkit import new_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; model.references.is_referenced(\"Perimeter_ZN_1\")\n        False\n        &gt;&gt;&gt; model.add(\"BuildingSurface:Detailed\", \"South_Wall\",\n        ...     surface_type=\"Wall\", construction_name=\"\",\n        ...     zone_name=\"Perimeter_ZN_1\",\n        ...     outside_boundary_condition=\"Outdoors\",\n        ...     sun_exposure=\"SunExposed\", wind_exposure=\"WindExposed\",\n        ...     validate=False)  # doctest: +ELLIPSIS\n        BuildingSurface:Detailed('South_Wall')\n        &gt;&gt;&gt; model.references.is_referenced(\"Perimeter_ZN_1\")\n        True\n    \"\"\"\n    return name.upper() in self._referenced_by\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.register","title":"<code>register(obj, field_name, referenced_name)</code>","text":"<p>Register that an object references another name.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>IDFObject</code> <p>The object that contains the reference</p> required <code>field_name</code> <code>str</code> <p>The field that contains the reference</p> required <code>referenced_name</code> <code>str</code> <p>The name being referenced</p> required Source code in <code>src/idfkit/references.py</code> <pre><code>def register(self, obj: IDFObject, field_name: str, referenced_name: str) -&gt; None:\n    \"\"\"\n    Register that an object references another name.\n\n    Args:\n        obj: The object that contains the reference\n        field_name: The field that contains the reference\n        referenced_name: The name being referenced\n    \"\"\"\n    if not referenced_name:\n        return\n\n    name_upper = referenced_name.upper()\n    self._referenced_by[name_upper].add((obj, field_name))\n    self._references[obj].add((name_upper, field_name))\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.register_object_list","title":"<code>register_object_list(list_name, obj_type)</code>","text":"<p>Register that an object type provides names for an object-list.</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def register_object_list(self, list_name: str, obj_type: str) -&gt; None:\n    \"\"\"Register that an object type provides names for an object-list.\"\"\"\n    self._object_lists[list_name].add(obj_type)\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.rename_target","title":"<code>rename_target(old_name, new_name)</code>","text":"<p>Update indexes when a referenced target is renamed.</p> <p>Moves _referenced_by[OLD] -&gt; _referenced_by[NEW] and updates corresponding _references entries for all affected objects.</p> <p>Parameters:</p> Name Type Description Default <code>old_name</code> <code>str</code> <p>The old target name</p> required <code>new_name</code> <code>str</code> <p>The new target name</p> required Source code in <code>src/idfkit/references.py</code> <pre><code>def rename_target(self, old_name: str, new_name: str) -&gt; None:\n    \"\"\"\n    Update indexes when a referenced target is renamed.\n\n    Moves _referenced_by[OLD] -&gt; _referenced_by[NEW] and updates\n    corresponding _references entries for all affected objects.\n\n    Args:\n        old_name: The old target name\n        new_name: The new target name\n    \"\"\"\n    old_upper = old_name.upper()\n    new_upper = new_name.upper()\n    if old_upper == new_upper:\n        return\n\n    referrers = self._referenced_by.pop(old_upper, set())\n    if not referrers:\n        return\n\n    # Update _references for each referring object\n    for obj, field_name in referrers:\n        obj_refs = self._references.get(obj)\n        if obj_refs is not None:\n            obj_refs.discard((old_upper, field_name))\n            obj_refs.add((new_upper, field_name))\n\n    # Merge into new key (there may already be refs to new_name)\n    if new_upper in self._referenced_by:\n        self._referenced_by[new_upper].update(referrers)\n    else:\n        self._referenced_by[new_upper] = referrers\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.stats","title":"<code>stats()</code>","text":"<p>Return statistics about the reference graph.</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def stats(self) -&gt; dict[str, int]:\n    \"\"\"Return statistics about the reference graph.\"\"\"\n    return {\n        \"total_references\": len(self),\n        \"objects_with_references\": len(self._references),\n        \"names_referenced\": len(self._referenced_by),\n        \"object_lists\": len(self._object_lists),\n    }\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.unregister","title":"<code>unregister(obj)</code>","text":"<p>Remove all reference tracking for an object.</p> Source code in <code>src/idfkit/references.py</code> <pre><code>def unregister(self, obj: IDFObject) -&gt; None:\n    \"\"\"Remove all reference tracking for an object.\"\"\"\n    if obj in self._references:\n        # Remove from referenced_by\n        for name_upper, field_name in self._references[obj]:\n            if name_upper in self._referenced_by:\n                self._referenced_by[name_upper].discard((obj, field_name))\n                if not self._referenced_by[name_upper]:\n                    del self._referenced_by[name_upper]\n        del self._references[obj]\n\n    # Also remove any references TO this object\n    obj_name_upper = obj.name.upper() if obj.name else \"\"\n    if obj_name_upper in self._referenced_by:\n        del self._referenced_by[obj_name_upper]\n</code></pre>"},{"location":"api/references/#idfkit.references.ReferenceGraph.update_reference","title":"<code>update_reference(obj, field_name, old_value, new_value)</code>","text":"<p>Update indexes when an object's reference field changes.</p> <p>Removes the old entry from both indexes and adds the new entry.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>IDFObject</code> <p>The object whose field changed</p> required <code>field_name</code> <code>str</code> <p>The field that changed</p> required <code>old_value</code> <code>str | None</code> <p>The previous referenced name (or None)</p> required <code>new_value</code> <code>str | None</code> <p>The new referenced name (or None)</p> required Source code in <code>src/idfkit/references.py</code> <pre><code>def update_reference(self, obj: IDFObject, field_name: str, old_value: str | None, new_value: str | None) -&gt; None:\n    \"\"\"\n    Update indexes when an object's reference field changes.\n\n    Removes the old entry from both indexes and adds the new entry.\n\n    Args:\n        obj: The object whose field changed\n        field_name: The field that changed\n        old_value: The previous referenced name (or None)\n        new_value: The new referenced name (or None)\n    \"\"\"\n    # Remove old\n    if old_value:\n        old_upper = old_value.upper()\n        refs_set = self._referenced_by.get(old_upper)\n        if refs_set is not None:\n            refs_set.discard((obj, field_name))\n            if not refs_set:\n                del self._referenced_by[old_upper]\n        obj_refs = self._references.get(obj)\n        if obj_refs is not None:\n            obj_refs.discard((old_upper, field_name))\n\n    # Add new\n    if new_value and new_value.strip():\n        new_upper = new_value.upper()\n        self._referenced_by[new_upper].add((obj, field_name))\n        self._references[obj].add((new_upper, field_name))\n</code></pre>"},{"location":"api/schema/","title":"Schema","text":"<p><code>EpJSONSchema</code> wraps the official Energy+.schema.epJSON file and exposes field metadata -- types, defaults, ranges, reference lists, and extensible group info.</p> <p><code>SchemaManager</code> handles version discovery, caching, and lazy loading of schema files.</p> <p>EpJSON Schema loader and manager.</p> <p>Handles loading and caching of Energy+.schema.epJSON files for different EnergyPlus versions. Supports both uncompressed and gzip-compressed schema files.</p>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema","title":"<code>EpJSONSchema</code>","text":"<p>Wrapper around Energy+.schema.epJSON providing easy access to object definitions.</p> <p>The schema contains: - Object definitions with field types, defaults, constraints - Reference lists (object-list) for cross-object validation - Legacy IDD info for IDF field ordering</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n&gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n&gt;&gt;&gt; \"Zone\" in schema\nTrue\n</code></pre> <p>Check which IDD group a type belongs to:</p> <pre><code>&gt;&gt;&gt; schema.get_group(\"Zone\")\n'Thermal Zones and Surfaces'\n</code></pre> <p>Some object types (like Timestep) are singletons with no name field:</p> <pre><code>&gt;&gt;&gt; schema.has_name(\"Zone\")\nTrue\n&gt;&gt;&gt; schema.has_name(\"Timestep\")\nFalse\n</code></pre> <p>Attributes:</p> Name Type Description <code>version</code> <code>tuple[int, int, int]</code> <p>The EnergyPlus version tuple</p> <code>_raw</code> <code>dict[str, Any]</code> <p>The raw schema dict</p> <code>_properties</code> <code>dict[str, Any]</code> <p>Object definitions</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>class EpJSONSchema:\n    \"\"\"\n    Wrapper around Energy+.schema.epJSON providing easy access to object definitions.\n\n    The schema contains:\n    - Object definitions with field types, defaults, constraints\n    - Reference lists (object-list) for cross-object validation\n    - Legacy IDD info for IDF field ordering\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n        &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n        &gt;&gt;&gt; \"Zone\" in schema\n        True\n\n        Check which IDD group a type belongs to:\n\n        &gt;&gt;&gt; schema.get_group(\"Zone\")\n        'Thermal Zones and Surfaces'\n\n        Some object types (like Timestep) are singletons with no name field:\n\n        &gt;&gt;&gt; schema.has_name(\"Zone\")\n        True\n        &gt;&gt;&gt; schema.has_name(\"Timestep\")\n        False\n\n    Attributes:\n        version: The EnergyPlus version tuple\n        _raw: The raw schema dict\n        _properties: Object definitions\n    \"\"\"\n\n    __slots__ = (\"_object_lists\", \"_parsing_cache\", \"_properties\", \"_raw\", \"_reference_lists\", \"version\")\n\n    version: tuple[int, int, int]\n    _raw: dict[str, Any]\n    _properties: dict[str, Any]\n    _reference_lists: dict[str, list[str]]\n    _object_lists: dict[str, set[str]]\n    _parsing_cache: dict[str, ParsingCache]\n\n    def __init__(self, version: tuple[int, int, int], schema_data: dict[str, Any]) -&gt; None:\n        self.version = version\n        self._raw = schema_data\n        self._properties: dict[str, Any] = schema_data.get(\"properties\", {})\n\n        # Build reference indexes\n        self._reference_lists: dict[str, list[str]] = {}\n        self._object_lists: dict[str, set[str]] = {}\n        self._parsing_cache: dict[str, ParsingCache] = {}\n        self._build_reference_indexes()\n\n    def _build_reference_indexes(self) -&gt; None:\n        \"\"\"Build indexes for reference and object lists.\"\"\"\n        for obj_type, obj_schema in self._properties.items():\n            # Check if this object provides names for any reference lists\n            name_info = obj_schema.get(\"name\", {})\n            if \"reference\" in name_info:\n                for ref_list in name_info[\"reference\"]:\n                    if ref_list not in self._reference_lists:\n                        self._reference_lists[ref_list] = []\n                    self._reference_lists[ref_list].append(obj_type)\n\n            # Find fields that reference object lists\n            pattern_props: dict[str, Any] = obj_schema.get(\"patternProperties\", {})\n            default_dict: dict[str, Any] = {}\n            inner: dict[str, Any] = next(iter(pattern_props.values()), default_dict) if pattern_props else default_dict\n            props: dict[str, Any] = inner.get(\"properties\", {})\n            for field_name, field_schema in props.items():\n                field_schema_dict: dict[str, Any] = field_schema\n                if \"object_list\" in field_schema_dict:\n                    for obj_list in field_schema_dict[\"object_list\"]:\n                        if obj_list not in self._object_lists:\n                            self._object_lists[obj_list] = set()\n                        self._object_lists[obj_list].add(f\"{obj_type}.{field_name}\")\n\n    def get_object_schema(self, obj_type: str) -&gt; dict[str, Any] | None:\n        \"\"\"Get the full schema for an object type.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n            &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n            &gt;&gt;&gt; zone_schema = schema.get_object_schema(\"Zone\")\n            &gt;&gt;&gt; zone_schema is not None\n            True\n            &gt;&gt;&gt; schema.get_object_schema(\"NonExistent\") is None\n            True\n        \"\"\"\n        return self._properties.get(obj_type)\n\n    def get_inner_schema(self, obj_type: str) -&gt; dict[str, Any] | None:\n        \"\"\"Get the inner schema (inside patternProperties) for an object type.\"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if not obj_schema:\n            return None\n        pattern_props = obj_schema.get(\"patternProperties\", {})\n        # The pattern key varies (e.g., \".*\", \"^.*\\\\S.*$\") - get the first one\n        for key in pattern_props:\n            return pattern_props[key]\n        return None\n\n    def get_field_schema(self, obj_type: str, field_name: str) -&gt; dict[str, Any] | None:\n        \"\"\"Get schema for a specific field of an object type.\"\"\"\n        inner = self.get_inner_schema(obj_type)\n        if not inner:\n            return None\n        return inner.get(\"properties\", {}).get(field_name)\n\n    def get_field_names(self, obj_type: str) -&gt; list[str]:\n        \"\"\"Get ordered list of field names for an object type (from legacy_idd).\n\n        Useful for discovering valid field names when building objects\n        programmatically.\n\n        Examples:\n            List the fields available on a Material object:\n\n            &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n            &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n            &gt;&gt;&gt; \"thickness\" in schema.get_field_names(\"Material\")\n            True\n        \"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if not obj_schema:\n            return []\n        legacy = obj_schema.get(\"legacy_idd\", {})\n        fields = legacy.get(\"fields\", [])\n        # First field is 'name', return the rest\n        return fields[1:] if fields else []\n\n    def get_all_field_names(self, obj_type: str) -&gt; list[str]:\n        \"\"\"Get all field names including 'name'.\"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if not obj_schema:\n            return []\n        legacy = obj_schema.get(\"legacy_idd\", {})\n        return list(legacy.get(\"fields\", []))\n\n    def get_required_fields(self, obj_type: str) -&gt; list[str]:\n        \"\"\"Get list of required field names for an object type.\n\n        Check which fields must be supplied before a Material is valid:\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n            &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n            &gt;&gt;&gt; schema.get_required_fields(\"Material\")\n            ['roughness', 'thickness', 'conductivity', 'density', 'specific_heat']\n        \"\"\"\n        inner = self.get_inner_schema(obj_type)\n        if not inner:\n            return []\n        return inner.get(\"required\", [])\n\n    def get_field_default(self, obj_type: str, field_name: str) -&gt; Any:\n        \"\"\"Get default value for a field.\"\"\"\n        field_schema = self.get_field_schema(obj_type, field_name)\n        if field_schema:\n            return field_schema.get(\"default\")\n        return None\n\n    def get_field_type(self, obj_type: str, field_name: str) -&gt; str | None:\n        \"\"\"Get the type of a field ('number', 'string', 'integer', 'array').\n\n        Useful for dynamic type coercion when importing data from\n        spreadsheets or CSV files.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n            &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n            &gt;&gt;&gt; schema.get_field_type(\"Material\", \"thickness\")\n            'number'\n            &gt;&gt;&gt; schema.get_field_type(\"Material\", \"roughness\")\n            'string'\n        \"\"\"\n        field_schema = self.get_field_schema(obj_type, field_name)\n        if not field_schema:\n            # Fall back to legacy_idd field_info for extensible fields\n            obj_schema = self.get_object_schema(obj_type)\n            if obj_schema:\n                field_info = obj_schema.get(\"legacy_idd\", {}).get(\"field_info\", {}).get(field_name)\n                if field_info:\n                    ft = field_info.get(\"field_type\")\n                    if ft == \"n\":\n                        return \"number\"\n                    if ft == \"a\":\n                        return \"string\"\n            return None\n\n        # Handle anyOf (e.g., number OR \"Autocalculate\")\n        if \"anyOf\" in field_schema:\n            for sub in field_schema[\"anyOf\"]:\n                if sub.get(\"type\") in (\"number\", \"integer\"):\n                    return sub[\"type\"]\n            return \"string\"\n\n        return field_schema.get(\"type\")\n\n    def get_field_object_list(self, obj_type: str, field_name: str) -&gt; list[str] | None:\n        \"\"\"Get the object_list(s) that a field references.\"\"\"\n        field_schema = self.get_field_schema(obj_type, field_name)\n        if field_schema:\n            return field_schema.get(\"object_list\")\n        return None\n\n    def is_reference_field(self, obj_type: str, field_name: str) -&gt; bool:\n        \"\"\"Check if a field is a reference to another object.\"\"\"\n        return self.get_field_object_list(obj_type, field_name) is not None\n\n    def get_parsing_cache(self, obj_type: str) -&gt; ParsingCache | None:\n        \"\"\"Get or lazily build pre-computed parsing metadata for an object type.\n\n        Returns None if *obj_type* is not in the schema.\n        \"\"\"\n        cached = self._parsing_cache.get(obj_type)\n        if cached is not None:\n            return cached\n\n        obj_schema = self._properties.get(obj_type)\n        if obj_schema is None:\n            return None\n\n        cached = self._build_parsing_cache(obj_type, obj_schema)\n        self._parsing_cache[obj_type] = cached\n        return cached\n\n    def _build_parsing_cache(self, obj_type: str, obj_schema: dict[str, Any]) -&gt; ParsingCache:\n        \"\"\"Build parsing metadata for a single object type.\"\"\"\n        has_name = \"name\" in obj_schema\n\n        legacy: dict[str, Any] = obj_schema.get(\"legacy_idd\", {})\n        all_fields_list: list[str] = legacy.get(\"fields\", [])\n        all_field_names = tuple(all_fields_list)\n        field_names = tuple(all_fields_list[1:]) if all_fields_list else ()\n\n        # Extract inner schema properties\n        pattern_props: dict[str, Any] = obj_schema.get(\"patternProperties\", {})\n        default_dict: dict[str, Any] = {}\n        inner: dict[str, Any] = next(iter(pattern_props.values()), default_dict) if pattern_props else default_dict\n        props: dict[str, Any] = inner.get(\"properties\", {})\n\n        field_info: dict[str, Any] = legacy.get(\"field_info\", {})\n\n        # Pre-compute field types and reference fields\n        field_types: dict[str, str | None] = {}\n        ref_fields_set: set[str] = set()\n        target_fields = field_names if has_name else all_field_names\n        for fname in target_fields:\n            field_types[fname] = _resolve_field_type(fname, props, field_info)\n            field_schema = props.get(fname)\n            if field_schema is not None and \"object_list\" in field_schema:\n                ref_fields_set.add(fname)\n\n        # Extensible info\n        extensible = \"extensible_size\" in obj_schema\n        ext_size = int(obj_schema.get(\"extensible_size\", 0))\n        ext_field_names_list: list[str] = legacy.get(\"extensibles\", [])\n\n        # Pre-compute field types for extensible base names\n        for ext_fname in ext_field_names_list:\n            if ext_fname not in field_types:\n                field_types[ext_fname] = _resolve_field_type(ext_fname, props, field_info)\n\n        return ParsingCache(\n            obj_schema=obj_schema,\n            has_name=has_name,\n            field_names=field_names,\n            all_field_names=all_field_names,\n            field_types=field_types,\n            ref_fields=frozenset(ref_fields_set),\n            extensible=extensible,\n            ext_size=ext_size,\n            ext_field_names=tuple(ext_field_names_list),\n        )\n\n    def get_types_providing_reference(self, ref_list: str) -&gt; list[str]:\n        \"\"\"Get object types that provide names for a reference list.\"\"\"\n        return self._reference_lists.get(ref_list, [])\n\n    def get_group(self, obj_type: str) -&gt; str | None:\n        \"\"\"Get the IDD group name for an object type.\n\n        Every object type in the EnergyPlus schema belongs to a group\n        (e.g. ``\"Thermal Zones and Surfaces\"``, ``\"HVAC Templates\"``,\n        ``\"Detailed Ground Heat Transfer\"``).  This method returns the\n        group string, which is useful for classifying objects without\n        relying on naming conventions.\n\n        Args:\n            obj_type: Case-sensitive EnergyPlus object type\n                (e.g. ``\"Zone\"``, ``\"HVACTemplate:Zone:IdealLoadsAirSystem\"``).\n\n        Returns:\n            The group name, or ``None`` if *obj_type* is not in the schema.\n\n        Examples:\n            ```python\n            schema = get_schema((24, 1, 0))\n            schema.get_group(\"Zone\")\n            # \"Thermal Zones and Surfaces\"\n            schema.get_group(\"HVACTemplate:Zone:IdealLoadsAirSystem\")\n            # \"HVAC Templates\"\n            ```\n        \"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if obj_schema:\n            return obj_schema.get(\"group\")\n        return None\n\n    def get_object_memo(self, obj_type: str) -&gt; str | None:\n        \"\"\"Get the memo/description for an object type.\"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if obj_schema:\n            return obj_schema.get(\"memo\")\n        return None\n\n    def has_name(self, obj_type: str) -&gt; bool:\n        \"\"\"Check if an object type has a name field (first IDF field is a name).\"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if not obj_schema:\n            return True  # Default: assume named (backward compat)\n        return \"name\" in obj_schema\n\n    def get_extensible_field_names(self, obj_type: str) -&gt; list[str]:\n        \"\"\"Get extensible field names from legacy_idd.extensibles.\"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if not obj_schema:\n            return []\n        legacy = obj_schema.get(\"legacy_idd\", {})\n        return legacy.get(\"extensibles\", [])\n\n    def is_extensible(self, obj_type: str) -&gt; bool:\n        \"\"\"Check if an object type has extensible fields.\n\n        Extensible types (like surfaces) can have a variable number of\n        vertices or layers.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n            &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n            &gt;&gt;&gt; schema.is_extensible(\"BuildingSurface:Detailed\")\n            True\n            &gt;&gt;&gt; schema.is_extensible(\"Zone\")\n            False\n        \"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if obj_schema:\n            return \"extensible_size\" in obj_schema\n        return False\n\n    def get_extensible_size(self, obj_type: str) -&gt; int | None:\n        \"\"\"Get the extensible group size for an object type.\"\"\"\n        obj_schema = self.get_object_schema(obj_type)\n        if obj_schema:\n            return obj_schema.get(\"extensible_size\")\n        return None\n\n    @property\n    def object_types(self) -&gt; list[str]:\n        \"\"\"Get list of all object types in the schema.\n\n        Examples:\n            &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n            &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n            &gt;&gt;&gt; \"Zone\" in schema.object_types\n            True\n            &gt;&gt;&gt; len(schema.object_types) &gt; 100\n            True\n        \"\"\"\n        return list(self._properties.keys())\n\n    def __contains__(self, obj_type: str) -&gt; bool:\n        \"\"\"Check if an object type exists in the schema.\"\"\"\n        return obj_type in self._properties\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return number of object types.\"\"\"\n        return len(self._properties)\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.object_types","title":"<code>object_types</code>  <code>property</code>","text":"<p>Get list of all object types in the schema.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n&gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n&gt;&gt;&gt; \"Zone\" in schema.object_types\nTrue\n&gt;&gt;&gt; len(schema.object_types) &gt; 100\nTrue\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.__contains__","title":"<code>__contains__(obj_type)</code>","text":"<p>Check if an object type exists in the schema.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def __contains__(self, obj_type: str) -&gt; bool:\n    \"\"\"Check if an object type exists in the schema.\"\"\"\n    return obj_type in self._properties\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.__len__","title":"<code>__len__()</code>","text":"<p>Return number of object types.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return number of object types.\"\"\"\n    return len(self._properties)\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_all_field_names","title":"<code>get_all_field_names(obj_type)</code>","text":"<p>Get all field names including 'name'.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_all_field_names(self, obj_type: str) -&gt; list[str]:\n    \"\"\"Get all field names including 'name'.\"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if not obj_schema:\n        return []\n    legacy = obj_schema.get(\"legacy_idd\", {})\n    return list(legacy.get(\"fields\", []))\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_extensible_field_names","title":"<code>get_extensible_field_names(obj_type)</code>","text":"<p>Get extensible field names from legacy_idd.extensibles.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_extensible_field_names(self, obj_type: str) -&gt; list[str]:\n    \"\"\"Get extensible field names from legacy_idd.extensibles.\"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if not obj_schema:\n        return []\n    legacy = obj_schema.get(\"legacy_idd\", {})\n    return legacy.get(\"extensibles\", [])\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_extensible_size","title":"<code>get_extensible_size(obj_type)</code>","text":"<p>Get the extensible group size for an object type.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_extensible_size(self, obj_type: str) -&gt; int | None:\n    \"\"\"Get the extensible group size for an object type.\"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if obj_schema:\n        return obj_schema.get(\"extensible_size\")\n    return None\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_field_default","title":"<code>get_field_default(obj_type, field_name)</code>","text":"<p>Get default value for a field.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_field_default(self, obj_type: str, field_name: str) -&gt; Any:\n    \"\"\"Get default value for a field.\"\"\"\n    field_schema = self.get_field_schema(obj_type, field_name)\n    if field_schema:\n        return field_schema.get(\"default\")\n    return None\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_field_names","title":"<code>get_field_names(obj_type)</code>","text":"<p>Get ordered list of field names for an object type (from legacy_idd).</p> <p>Useful for discovering valid field names when building objects programmatically.</p> <p>Examples:</p> <p>List the fields available on a Material object:</p> <pre><code>&gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n&gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n&gt;&gt;&gt; \"thickness\" in schema.get_field_names(\"Material\")\nTrue\n</code></pre> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_field_names(self, obj_type: str) -&gt; list[str]:\n    \"\"\"Get ordered list of field names for an object type (from legacy_idd).\n\n    Useful for discovering valid field names when building objects\n    programmatically.\n\n    Examples:\n        List the fields available on a Material object:\n\n        &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n        &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n        &gt;&gt;&gt; \"thickness\" in schema.get_field_names(\"Material\")\n        True\n    \"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if not obj_schema:\n        return []\n    legacy = obj_schema.get(\"legacy_idd\", {})\n    fields = legacy.get(\"fields\", [])\n    # First field is 'name', return the rest\n    return fields[1:] if fields else []\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_field_object_list","title":"<code>get_field_object_list(obj_type, field_name)</code>","text":"<p>Get the object_list(s) that a field references.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_field_object_list(self, obj_type: str, field_name: str) -&gt; list[str] | None:\n    \"\"\"Get the object_list(s) that a field references.\"\"\"\n    field_schema = self.get_field_schema(obj_type, field_name)\n    if field_schema:\n        return field_schema.get(\"object_list\")\n    return None\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_field_schema","title":"<code>get_field_schema(obj_type, field_name)</code>","text":"<p>Get schema for a specific field of an object type.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_field_schema(self, obj_type: str, field_name: str) -&gt; dict[str, Any] | None:\n    \"\"\"Get schema for a specific field of an object type.\"\"\"\n    inner = self.get_inner_schema(obj_type)\n    if not inner:\n        return None\n    return inner.get(\"properties\", {}).get(field_name)\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_field_type","title":"<code>get_field_type(obj_type, field_name)</code>","text":"<p>Get the type of a field ('number', 'string', 'integer', 'array').</p> <p>Useful for dynamic type coercion when importing data from spreadsheets or CSV files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n&gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n&gt;&gt;&gt; schema.get_field_type(\"Material\", \"thickness\")\n'number'\n&gt;&gt;&gt; schema.get_field_type(\"Material\", \"roughness\")\n'string'\n</code></pre> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_field_type(self, obj_type: str, field_name: str) -&gt; str | None:\n    \"\"\"Get the type of a field ('number', 'string', 'integer', 'array').\n\n    Useful for dynamic type coercion when importing data from\n    spreadsheets or CSV files.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n        &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n        &gt;&gt;&gt; schema.get_field_type(\"Material\", \"thickness\")\n        'number'\n        &gt;&gt;&gt; schema.get_field_type(\"Material\", \"roughness\")\n        'string'\n    \"\"\"\n    field_schema = self.get_field_schema(obj_type, field_name)\n    if not field_schema:\n        # Fall back to legacy_idd field_info for extensible fields\n        obj_schema = self.get_object_schema(obj_type)\n        if obj_schema:\n            field_info = obj_schema.get(\"legacy_idd\", {}).get(\"field_info\", {}).get(field_name)\n            if field_info:\n                ft = field_info.get(\"field_type\")\n                if ft == \"n\":\n                    return \"number\"\n                if ft == \"a\":\n                    return \"string\"\n        return None\n\n    # Handle anyOf (e.g., number OR \"Autocalculate\")\n    if \"anyOf\" in field_schema:\n        for sub in field_schema[\"anyOf\"]:\n            if sub.get(\"type\") in (\"number\", \"integer\"):\n                return sub[\"type\"]\n        return \"string\"\n\n    return field_schema.get(\"type\")\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_group","title":"<code>get_group(obj_type)</code>","text":"<p>Get the IDD group name for an object type.</p> <p>Every object type in the EnergyPlus schema belongs to a group (e.g. <code>\"Thermal Zones and Surfaces\"</code>, <code>\"HVAC Templates\"</code>, <code>\"Detailed Ground Heat Transfer\"</code>).  This method returns the group string, which is useful for classifying objects without relying on naming conventions.</p> <p>Parameters:</p> Name Type Description Default <code>obj_type</code> <code>str</code> <p>Case-sensitive EnergyPlus object type (e.g. <code>\"Zone\"</code>, <code>\"HVACTemplate:Zone:IdealLoadsAirSystem\"</code>).</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>The group name, or <code>None</code> if obj_type is not in the schema.</p> <p>Examples:</p> <pre><code>schema = get_schema((24, 1, 0))\nschema.get_group(\"Zone\")\n# \"Thermal Zones and Surfaces\"\nschema.get_group(\"HVACTemplate:Zone:IdealLoadsAirSystem\")\n# \"HVAC Templates\"\n</code></pre> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_group(self, obj_type: str) -&gt; str | None:\n    \"\"\"Get the IDD group name for an object type.\n\n    Every object type in the EnergyPlus schema belongs to a group\n    (e.g. ``\"Thermal Zones and Surfaces\"``, ``\"HVAC Templates\"``,\n    ``\"Detailed Ground Heat Transfer\"``).  This method returns the\n    group string, which is useful for classifying objects without\n    relying on naming conventions.\n\n    Args:\n        obj_type: Case-sensitive EnergyPlus object type\n            (e.g. ``\"Zone\"``, ``\"HVACTemplate:Zone:IdealLoadsAirSystem\"``).\n\n    Returns:\n        The group name, or ``None`` if *obj_type* is not in the schema.\n\n    Examples:\n        ```python\n        schema = get_schema((24, 1, 0))\n        schema.get_group(\"Zone\")\n        # \"Thermal Zones and Surfaces\"\n        schema.get_group(\"HVACTemplate:Zone:IdealLoadsAirSystem\")\n        # \"HVAC Templates\"\n        ```\n    \"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if obj_schema:\n        return obj_schema.get(\"group\")\n    return None\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_inner_schema","title":"<code>get_inner_schema(obj_type)</code>","text":"<p>Get the inner schema (inside patternProperties) for an object type.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_inner_schema(self, obj_type: str) -&gt; dict[str, Any] | None:\n    \"\"\"Get the inner schema (inside patternProperties) for an object type.\"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if not obj_schema:\n        return None\n    pattern_props = obj_schema.get(\"patternProperties\", {})\n    # The pattern key varies (e.g., \".*\", \"^.*\\\\S.*$\") - get the first one\n    for key in pattern_props:\n        return pattern_props[key]\n    return None\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_object_memo","title":"<code>get_object_memo(obj_type)</code>","text":"<p>Get the memo/description for an object type.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_object_memo(self, obj_type: str) -&gt; str | None:\n    \"\"\"Get the memo/description for an object type.\"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if obj_schema:\n        return obj_schema.get(\"memo\")\n    return None\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_object_schema","title":"<code>get_object_schema(obj_type)</code>","text":"<p>Get the full schema for an object type.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n&gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n&gt;&gt;&gt; zone_schema = schema.get_object_schema(\"Zone\")\n&gt;&gt;&gt; zone_schema is not None\nTrue\n&gt;&gt;&gt; schema.get_object_schema(\"NonExistent\") is None\nTrue\n</code></pre> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_object_schema(self, obj_type: str) -&gt; dict[str, Any] | None:\n    \"\"\"Get the full schema for an object type.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n        &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n        &gt;&gt;&gt; zone_schema = schema.get_object_schema(\"Zone\")\n        &gt;&gt;&gt; zone_schema is not None\n        True\n        &gt;&gt;&gt; schema.get_object_schema(\"NonExistent\") is None\n        True\n    \"\"\"\n    return self._properties.get(obj_type)\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_parsing_cache","title":"<code>get_parsing_cache(obj_type)</code>","text":"<p>Get or lazily build pre-computed parsing metadata for an object type.</p> <p>Returns None if obj_type is not in the schema.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_parsing_cache(self, obj_type: str) -&gt; ParsingCache | None:\n    \"\"\"Get or lazily build pre-computed parsing metadata for an object type.\n\n    Returns None if *obj_type* is not in the schema.\n    \"\"\"\n    cached = self._parsing_cache.get(obj_type)\n    if cached is not None:\n        return cached\n\n    obj_schema = self._properties.get(obj_type)\n    if obj_schema is None:\n        return None\n\n    cached = self._build_parsing_cache(obj_type, obj_schema)\n    self._parsing_cache[obj_type] = cached\n    return cached\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_required_fields","title":"<code>get_required_fields(obj_type)</code>","text":"<p>Get list of required field names for an object type.</p> <p>Check which fields must be supplied before a Material is valid:</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n&gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n&gt;&gt;&gt; schema.get_required_fields(\"Material\")\n['roughness', 'thickness', 'conductivity', 'density', 'specific_heat']\n</code></pre> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_required_fields(self, obj_type: str) -&gt; list[str]:\n    \"\"\"Get list of required field names for an object type.\n\n    Check which fields must be supplied before a Material is valid:\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n        &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n        &gt;&gt;&gt; schema.get_required_fields(\"Material\")\n        ['roughness', 'thickness', 'conductivity', 'density', 'specific_heat']\n    \"\"\"\n    inner = self.get_inner_schema(obj_type)\n    if not inner:\n        return []\n    return inner.get(\"required\", [])\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.get_types_providing_reference","title":"<code>get_types_providing_reference(ref_list)</code>","text":"<p>Get object types that provide names for a reference list.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_types_providing_reference(self, ref_list: str) -&gt; list[str]:\n    \"\"\"Get object types that provide names for a reference list.\"\"\"\n    return self._reference_lists.get(ref_list, [])\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.has_name","title":"<code>has_name(obj_type)</code>","text":"<p>Check if an object type has a name field (first IDF field is a name).</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def has_name(self, obj_type: str) -&gt; bool:\n    \"\"\"Check if an object type has a name field (first IDF field is a name).\"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if not obj_schema:\n        return True  # Default: assume named (backward compat)\n    return \"name\" in obj_schema\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.is_extensible","title":"<code>is_extensible(obj_type)</code>","text":"<p>Check if an object type has extensible fields.</p> <p>Extensible types (like surfaces) can have a variable number of vertices or layers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n&gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n&gt;&gt;&gt; schema.is_extensible(\"BuildingSurface:Detailed\")\nTrue\n&gt;&gt;&gt; schema.is_extensible(\"Zone\")\nFalse\n</code></pre> Source code in <code>src/idfkit/schema.py</code> <pre><code>def is_extensible(self, obj_type: str) -&gt; bool:\n    \"\"\"Check if an object type has extensible fields.\n\n    Extensible types (like surfaces) can have a variable number of\n    vertices or layers.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n        &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n        &gt;&gt;&gt; schema.is_extensible(\"BuildingSurface:Detailed\")\n        True\n        &gt;&gt;&gt; schema.is_extensible(\"Zone\")\n        False\n    \"\"\"\n    obj_schema = self.get_object_schema(obj_type)\n    if obj_schema:\n        return \"extensible_size\" in obj_schema\n    return False\n</code></pre>"},{"location":"api/schema/#idfkit.schema.EpJSONSchema.is_reference_field","title":"<code>is_reference_field(obj_type, field_name)</code>","text":"<p>Check if a field is a reference to another object.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def is_reference_field(self, obj_type: str, field_name: str) -&gt; bool:\n    \"\"\"Check if a field is a reference to another object.\"\"\"\n    return self.get_field_object_list(obj_type, field_name) is not None\n</code></pre>"},{"location":"api/schema/#idfkit.schema.ParsingCache","title":"<code>ParsingCache</code>  <code>dataclass</code>","text":"<p>Pre-computed parsing metadata for a single object type.</p> <p>Built lazily on first access per object type and cached for reuse. Eliminates repeated nested dict traversals during parsing.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass ParsingCache:\n    \"\"\"Pre-computed parsing metadata for a single object type.\n\n    Built lazily on first access per object type and cached for reuse.\n    Eliminates repeated nested dict traversals during parsing.\n    \"\"\"\n\n    obj_schema: dict[str, Any]\n    has_name: bool\n    field_names: tuple[str, ...]\n    all_field_names: tuple[str, ...]\n    field_types: dict[str, str | None]\n    ref_fields: frozenset[str]\n    extensible: bool\n    ext_size: int\n    ext_field_names: tuple[str, ...]\n</code></pre>"},{"location":"api/schema/#idfkit.schema.SchemaManager","title":"<code>SchemaManager</code>","text":"<p>Manages loading and caching of EpJSON schemas for different versions.</p> <p>Searches for schemas in the following order: 1. Bundled schemas directory (shipped with idfkit) - both .gz and plain 2. User cache directory (~/.idfkit/schemas/) 3. EnergyPlus installation directories</p> <p>Supports gzip-compressed schema files (.epJSON.gz) to reduce package size.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>class SchemaManager:\n    \"\"\"\n    Manages loading and caching of EpJSON schemas for different versions.\n\n    Searches for schemas in the following order:\n    1. Bundled schemas directory (shipped with idfkit) - both .gz and plain\n    2. User cache directory (~/.idfkit/schemas/)\n    3. EnergyPlus installation directories\n\n    Supports gzip-compressed schema files (.epJSON.gz) to reduce package size.\n    \"\"\"\n\n    # Common EnergyPlus installation paths by platform\n    _INSTALL_PATHS: ClassVar[dict[str, list[str]]] = {\n        \"linux\": [\"/usr/local/EnergyPlus-{v}\", \"/opt/EnergyPlus-{v}\"],\n        \"darwin\": [\"/Applications/EnergyPlus-{v}\"],\n        \"win32\": [\n            \"C:\\\\EnergyPlusV{v}\",\n            \"C:\\\\EnergyPlus-{v}\",\n            os.path.expandvars(\"$LOCALAPPDATA\\\\EnergyPlusV{v}\"),\n        ],\n    }\n\n    def __init__(\n        self,\n        bundled_schema_dir: Path | None = None,\n        cache_dir: Path | None = None,\n    ):\n        \"\"\"\n        Initialize the schema manager.\n\n        Args:\n            bundled_schema_dir: Path to directory with bundled schema files.\n                               If None, uses default location next to this file.\n            cache_dir: Path to user cache directory for downloaded schemas.\n                       If None, uses ~/.idfkit/schemas/.\n        \"\"\"\n        if bundled_schema_dir is None:\n            bundled_schema_dir = Path(__file__).parent / \"schemas\"\n\n        if cache_dir is None:\n            cache_dir = Path.home() / \".idfkit\" / \"schemas\"\n\n        self._bundled_dir = bundled_schema_dir\n        self._cache_dir = cache_dir\n        self._cache: dict[tuple[int, int, int], EpJSONSchema] = {}\n\n    @property\n    def bundled_dir(self) -&gt; Path:\n        \"\"\"Path to the bundled schemas directory.\"\"\"\n        return self._bundled_dir\n\n    @property\n    def cache_dir(self) -&gt; Path:\n        \"\"\"Path to the user cache directory for schemas.\"\"\"\n        return self._cache_dir\n\n    @lru_cache(maxsize=8)  # noqa: B019\n    def get_schema(self, version: tuple[int, int, int]) -&gt; EpJSONSchema:\n        \"\"\"\n        Load and return schema for a specific version.\n\n        If the exact version is not found, attempts to find the closest\n        supported version that is &lt;= the requested version.\n\n        Args:\n            version: EnergyPlus version tuple (major, minor, patch)\n\n        Returns:\n            EpJSONSchema for the requested version\n\n        Raises:\n            SchemaNotFoundError: If schema cannot be found\n        \"\"\"\n        if version in self._cache:\n            return self._cache[version]\n\n        # Try exact version first\n        schema_path = self._find_schema_file(version)\n        if schema_path is None:\n            # Try closest supported version\n            closest = find_closest_version(version)\n            if closest is not None and closest != version:\n                schema_path = self._find_schema_file(closest)\n\n        if schema_path is None:\n            searched = self._get_searched_paths(version)\n            raise SchemaNotFoundError(version, searched)\n\n        data = load_schema_json(schema_path)\n\n        schema = EpJSONSchema(version, data)\n        self._cache[version] = schema\n        return schema\n\n    def _find_schema_file(self, version: tuple[int, int, int]) -&gt; Path | None:\n        \"\"\"\n        Find the schema file for a version.\n\n        Searches bundled directory first (both compressed and plain),\n        then user cache, then EnergyPlus installations.\n        \"\"\"\n        # Try bundled schemas first\n        for path in self._get_bundled_paths(version):\n            if path.exists():\n                return path\n\n        # Try user cache\n        for path in self._get_cache_paths(version):\n            if path.exists():\n                return path\n\n        # Try EnergyPlus installation\n        for path in self._get_install_paths(version):\n            if path.exists():\n                return path\n\n        return None\n\n    def _get_searched_paths(self, version: tuple[int, int, int]) -&gt; list[str]:\n        \"\"\"Get all paths that would be searched for a version (for error messages).\"\"\"\n        paths: list[str] = []\n        for p in self._get_bundled_paths(version):\n            paths.append(str(p))\n        for p in self._get_cache_paths(version):\n            paths.append(str(p))\n        for p in self._get_install_paths(version):\n            paths.append(str(p))\n        return paths\n\n    def _get_bundled_paths(self, version: tuple[int, int, int]) -&gt; list[Path]:\n        \"\"\"Get potential bundled schema paths for a version.\"\"\"\n        paths: list[Path] = []\n        dirname = version_dirname(version)\n\n        # Compressed first (preferred for bundled), then plain\n        paths.append(self._bundled_dir / dirname / _SCHEMA_FILENAME_GZ)\n        paths.append(self._bundled_dir / dirname / _SCHEMA_FILENAME)\n\n        return paths\n\n    def _get_cache_paths(self, version: tuple[int, int, int]) -&gt; list[Path]:\n        \"\"\"Get potential user cache schema paths for a version.\"\"\"\n        paths: list[Path] = []\n        dirname = version_dirname(version)\n\n        paths.append(self._cache_dir / dirname / _SCHEMA_FILENAME_GZ)\n        paths.append(self._cache_dir / dirname / _SCHEMA_FILENAME)\n\n        return paths\n\n    def _get_install_paths(self, version: tuple[int, int, int]) -&gt; list[Path]:\n        \"\"\"Get potential EnergyPlus installation schema paths.\"\"\"\n        import sys\n\n        platform = sys.platform\n        paths: list[Path] = []\n        v = version\n\n        # Get base paths for this platform\n        base_patterns: list[str] = self._INSTALL_PATHS.get(platform, self._INSTALL_PATHS.get(\"linux\", []))\n\n        version_formats = [\n            f\"{v[0]}-{v[1]}-{v[2]}\",\n            f\"{v[0]}.{v[1]}.{v[2]}\",\n            f\"{v[0]}-{v[1]}\",\n        ]\n\n        for base_pattern in base_patterns:\n            for v_fmt in version_formats:\n                base_path = Path(base_pattern.format(v=v_fmt))\n                paths.append(base_path / _SCHEMA_FILENAME)\n\n        return paths\n\n    def get_available_versions(self) -&gt; list[tuple[int, int, int]]:  # noqa: C901\n        \"\"\"\n        Get list of versions with available schemas.\n\n        Checks bundled schemas, user cache, and installed EnergyPlus versions.\n        \"\"\"\n        versions: set[tuple[int, int, int]] = set()\n\n        # Check bundled\n        if self._bundled_dir.exists():\n            for item in self._bundled_dir.iterdir():\n                if item.is_dir():\n                    version = self._parse_version_from_dirname(item.name)\n                    if version and self._dir_has_schema(item):\n                        versions.add(version)\n\n        # Check user cache\n        if self._cache_dir.exists():\n            for item in self._cache_dir.iterdir():\n                if item.is_dir():\n                    version = self._parse_version_from_dirname(item.name)\n                    if version and self._dir_has_schema(item):\n                        versions.add(version)\n\n        # Check installed EnergyPlus versions\n        import sys\n\n        platform = sys.platform\n        base_patterns: list[str] = self._INSTALL_PATHS.get(platform, self._INSTALL_PATHS.get(\"linux\", []))\n\n        for pattern in base_patterns:\n            # Look for existing directories matching the pattern\n            parent = Path(pattern.split(\"{v}\")[0])\n            if parent.exists():\n                for item in parent.iterdir():\n                    if item.is_dir() and \"EnergyPlus\" in item.name:\n                        version = self._parse_version_from_dirname(item.name)\n                        if version:\n                            schema_path = item / _SCHEMA_FILENAME\n                            if schema_path.exists():\n                                versions.add(version)\n\n        return sorted(versions)\n\n    @staticmethod\n    def _dir_has_schema(directory: Path) -&gt; bool:\n        \"\"\"Check if a directory contains a schema file (plain or compressed).\"\"\"\n        return (directory / _SCHEMA_FILENAME).exists() or (directory / _SCHEMA_FILENAME_GZ).exists()\n\n    @staticmethod\n    def _parse_version_from_dirname(dirname: str) -&gt; tuple[int, int, int] | None:\n        \"\"\"Parse version tuple from directory name.\"\"\"\n        import re\n\n        # Match patterns like \"9-2-0\", \"9.2.0\", \"V9-2-0\", \"EnergyPlus-9-2-0\"\n        match = re.search(r\"(\\d+)[-._](\\d+)[-._]?(\\d+)?\", dirname)\n        if match:\n            major = int(match.group(1))\n            minor = int(match.group(2))\n            patch = int(match.group(3)) if match.group(3) else 0\n            return (major, minor, patch)\n        return None\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear the schema cache.\"\"\"\n        self._cache.clear()\n        self.get_schema.cache_clear()\n\n    def get_supported_versions(self) -&gt; list[tuple[int, int, int]]:\n        \"\"\"Get list of all EnergyPlus versions that idfkit supports.\n\n        This returns all versions in the registry, regardless of whether\n        schema files are currently available locally.\n        \"\"\"\n        return list(ENERGYPLUS_VERSIONS)\n</code></pre>"},{"location":"api/schema/#idfkit.schema.SchemaManager.bundled_dir","title":"<code>bundled_dir</code>  <code>property</code>","text":"<p>Path to the bundled schemas directory.</p>"},{"location":"api/schema/#idfkit.schema.SchemaManager.cache_dir","title":"<code>cache_dir</code>  <code>property</code>","text":"<p>Path to the user cache directory for schemas.</p>"},{"location":"api/schema/#idfkit.schema.SchemaManager.__init__","title":"<code>__init__(bundled_schema_dir=None, cache_dir=None)</code>","text":"<p>Initialize the schema manager.</p> <p>Parameters:</p> Name Type Description Default <code>bundled_schema_dir</code> <code>Path | None</code> <p>Path to directory with bundled schema files.                If None, uses default location next to this file.</p> <code>None</code> <code>cache_dir</code> <code>Path | None</code> <p>Path to user cache directory for downloaded schemas.        If None, uses ~/.idfkit/schemas/.</p> <code>None</code> Source code in <code>src/idfkit/schema.py</code> <pre><code>def __init__(\n    self,\n    bundled_schema_dir: Path | None = None,\n    cache_dir: Path | None = None,\n):\n    \"\"\"\n    Initialize the schema manager.\n\n    Args:\n        bundled_schema_dir: Path to directory with bundled schema files.\n                           If None, uses default location next to this file.\n        cache_dir: Path to user cache directory for downloaded schemas.\n                   If None, uses ~/.idfkit/schemas/.\n    \"\"\"\n    if bundled_schema_dir is None:\n        bundled_schema_dir = Path(__file__).parent / \"schemas\"\n\n    if cache_dir is None:\n        cache_dir = Path.home() / \".idfkit\" / \"schemas\"\n\n    self._bundled_dir = bundled_schema_dir\n    self._cache_dir = cache_dir\n    self._cache: dict[tuple[int, int, int], EpJSONSchema] = {}\n</code></pre>"},{"location":"api/schema/#idfkit.schema.SchemaManager.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the schema cache.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear the schema cache.\"\"\"\n    self._cache.clear()\n    self.get_schema.cache_clear()\n</code></pre>"},{"location":"api/schema/#idfkit.schema.SchemaManager.get_available_versions","title":"<code>get_available_versions()</code>","text":"<p>Get list of versions with available schemas.</p> <p>Checks bundled schemas, user cache, and installed EnergyPlus versions.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_available_versions(self) -&gt; list[tuple[int, int, int]]:  # noqa: C901\n    \"\"\"\n    Get list of versions with available schemas.\n\n    Checks bundled schemas, user cache, and installed EnergyPlus versions.\n    \"\"\"\n    versions: set[tuple[int, int, int]] = set()\n\n    # Check bundled\n    if self._bundled_dir.exists():\n        for item in self._bundled_dir.iterdir():\n            if item.is_dir():\n                version = self._parse_version_from_dirname(item.name)\n                if version and self._dir_has_schema(item):\n                    versions.add(version)\n\n    # Check user cache\n    if self._cache_dir.exists():\n        for item in self._cache_dir.iterdir():\n            if item.is_dir():\n                version = self._parse_version_from_dirname(item.name)\n                if version and self._dir_has_schema(item):\n                    versions.add(version)\n\n    # Check installed EnergyPlus versions\n    import sys\n\n    platform = sys.platform\n    base_patterns: list[str] = self._INSTALL_PATHS.get(platform, self._INSTALL_PATHS.get(\"linux\", []))\n\n    for pattern in base_patterns:\n        # Look for existing directories matching the pattern\n        parent = Path(pattern.split(\"{v}\")[0])\n        if parent.exists():\n            for item in parent.iterdir():\n                if item.is_dir() and \"EnergyPlus\" in item.name:\n                    version = self._parse_version_from_dirname(item.name)\n                    if version:\n                        schema_path = item / _SCHEMA_FILENAME\n                        if schema_path.exists():\n                            versions.add(version)\n\n    return sorted(versions)\n</code></pre>"},{"location":"api/schema/#idfkit.schema.SchemaManager.get_schema","title":"<code>get_schema(version)</code>  <code>cached</code>","text":"<p>Load and return schema for a specific version.</p> <p>If the exact version is not found, attempts to find the closest supported version that is &lt;= the requested version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>tuple[int, int, int]</code> <p>EnergyPlus version tuple (major, minor, patch)</p> required <p>Returns:</p> Type Description <code>EpJSONSchema</code> <p>EpJSONSchema for the requested version</p> <p>Raises:</p> Type Description <code>SchemaNotFoundError</code> <p>If schema cannot be found</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>@lru_cache(maxsize=8)  # noqa: B019\ndef get_schema(self, version: tuple[int, int, int]) -&gt; EpJSONSchema:\n    \"\"\"\n    Load and return schema for a specific version.\n\n    If the exact version is not found, attempts to find the closest\n    supported version that is &lt;= the requested version.\n\n    Args:\n        version: EnergyPlus version tuple (major, minor, patch)\n\n    Returns:\n        EpJSONSchema for the requested version\n\n    Raises:\n        SchemaNotFoundError: If schema cannot be found\n    \"\"\"\n    if version in self._cache:\n        return self._cache[version]\n\n    # Try exact version first\n    schema_path = self._find_schema_file(version)\n    if schema_path is None:\n        # Try closest supported version\n        closest = find_closest_version(version)\n        if closest is not None and closest != version:\n            schema_path = self._find_schema_file(closest)\n\n    if schema_path is None:\n        searched = self._get_searched_paths(version)\n        raise SchemaNotFoundError(version, searched)\n\n    data = load_schema_json(schema_path)\n\n    schema = EpJSONSchema(version, data)\n    self._cache[version] = schema\n    return schema\n</code></pre>"},{"location":"api/schema/#idfkit.schema.SchemaManager.get_supported_versions","title":"<code>get_supported_versions()</code>","text":"<p>Get list of all EnergyPlus versions that idfkit supports.</p> <p>This returns all versions in the registry, regardless of whether schema files are currently available locally.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_supported_versions(self) -&gt; list[tuple[int, int, int]]:\n    \"\"\"Get list of all EnergyPlus versions that idfkit supports.\n\n    This returns all versions in the registry, regardless of whether\n    schema files are currently available locally.\n    \"\"\"\n    return list(ENERGYPLUS_VERSIONS)\n</code></pre>"},{"location":"api/schema/#idfkit.schema.get_schema","title":"<code>get_schema(version)</code>","text":"<p>Convenience function to get schema for a version.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n&gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n&gt;&gt;&gt; \"Zone\" in schema\nTrue\n&gt;&gt;&gt; schema = get_schema((24, 1, 0))\n&gt;&gt;&gt; schema.version\n(24, 1, 0)\n</code></pre> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_schema(version: tuple[int, int, int]) -&gt; EpJSONSchema:\n    \"\"\"Convenience function to get schema for a version.\n\n    Examples:\n        &gt;&gt;&gt; from idfkit import get_schema, LATEST_VERSION\n        &gt;&gt;&gt; schema = get_schema(LATEST_VERSION)\n        &gt;&gt;&gt; \"Zone\" in schema\n        True\n        &gt;&gt;&gt; schema = get_schema((24, 1, 0))\n        &gt;&gt;&gt; schema.version\n        (24, 1, 0)\n    \"\"\"\n    return get_schema_manager().get_schema(version)\n</code></pre>"},{"location":"api/schema/#idfkit.schema.get_schema_manager","title":"<code>get_schema_manager()</code>","text":"<p>Get the global schema manager instance.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def get_schema_manager() -&gt; SchemaManager:\n    \"\"\"Get the global schema manager instance.\"\"\"\n    global _schema_manager\n    if _schema_manager is None:\n        _schema_manager = SchemaManager()\n    return _schema_manager\n</code></pre>"},{"location":"api/schema/#idfkit.schema.load_schema_json","title":"<code>load_schema_json(path)</code>","text":"<p>Load a schema JSON file, handling both plain and gzip-compressed files.</p> Source code in <code>src/idfkit/schema.py</code> <pre><code>def load_schema_json(path: Path) -&gt; dict[str, Any]:\n    \"\"\"Load a schema JSON file, handling both plain and gzip-compressed files.\"\"\"\n    if path.suffix == \".gz\" or path.name.endswith(\".epJSON.gz\"):\n        with gzip.open(path, \"rt\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    with open(path, encoding=\"utf-8\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/thermal/","title":"Thermal Properties","text":"<p>The <code>idfkit.thermal</code> module provides functions to calculate thermal properties for EnergyPlus construction assemblies, including R-value, U-value, and SHGC.</p>"},{"location":"api/thermal/#quick-start","title":"Quick Start","text":"<pre><code>from idfkit import load_idf\nfrom idfkit.thermal import calculate_r_value, calculate_u_value, get_thermal_properties\n\nmodel = load_idf(\"building.idf\")\nwall = model[\"Construction\"][\"ExteriorWall\"]\n\n# Calculate individual properties\nr_value = calculate_r_value(wall)\nu_value = calculate_u_value(wall)\n\nprint(f\"R-value: {r_value:.2f} m\u00b2\u00b7K/W\")\nprint(f\"U-value: {u_value:.2f} W/m\u00b2\u00b7K\")\n\n# Or get all properties at once\nprops = get_thermal_properties(wall)\nprint(f\"R-value (with films): {props.r_value_with_films:.2f} m\u00b2\u00b7K/W\")\nprint(f\"U-value: {props.u_value:.2f} W/m\u00b2\u00b7K\")\n</code></pre>"},{"location":"api/thermal/#opaque-constructions","title":"Opaque Constructions","text":"<p>For opaque constructions (walls, roofs, floors), the module calculates:</p> <ul> <li>R-value: Thermal resistance in m\u00b2\u00b7K/W</li> <li>U-value: Overall heat transfer coefficient in W/m\u00b2\u00b7K</li> </ul>"},{"location":"api/thermal/#supported-material-types","title":"Supported Material Types","text":"Material Type How R-value is Calculated <code>Material</code> R = thickness / conductivity <code>Material:NoMass</code> R = thermal_resistance (provided directly) <code>Material:AirGap</code> R = thermal_resistance (provided directly)"},{"location":"api/thermal/#example-multi-layer-wall","title":"Example: Multi-Layer Wall","text":"<pre><code>from idfkit import new_document\nfrom idfkit.thermal import calculate_r_value, calculate_u_value\n\ndoc = new_document()\n\n# Add materials\ndoc.add(\"Material\", \"Brick\", {\n    \"roughness\": \"Rough\",\n    \"thickness\": 0.1,\n    \"conductivity\": 0.6,\n    \"density\": 1800,\n    \"specific_heat\": 900,\n})\n\ndoc.add(\"Material\", \"Insulation\", {\n    \"roughness\": \"MediumSmooth\",\n    \"thickness\": 0.05,\n    \"conductivity\": 0.04,\n    \"density\": 30,\n    \"specific_heat\": 1000,\n})\n\ndoc.add(\"Material\", \"Gypsum\", {\n    \"roughness\": \"Smooth\",\n    \"thickness\": 0.013,\n    \"conductivity\": 0.16,\n    \"density\": 800,\n    \"specific_heat\": 1000,\n})\n\n# Create construction\ndoc.add(\"Construction\", \"InsulatedWall\", {\n    \"outside_layer\": \"Brick\",\n    \"layer_2\": \"Insulation\",\n    \"layer_3\": \"Gypsum\",\n})\n\nwall = doc[\"Construction\"][\"InsulatedWall\"]\n\n# Calculate R-value without surface films\nr_no_films = calculate_r_value(wall, include_films=False)\nprint(f\"R-value (assembly only): {r_no_films:.2f} m\u00b2\u00b7K/W\")\n\n# Calculate R-value with surface films (default)\nr_with_films = calculate_r_value(wall, include_films=True)\nprint(f\"R-value (with films): {r_with_films:.2f} m\u00b2\u00b7K/W\")\n\n# Calculate U-value (always includes films)\nu_value = calculate_u_value(wall)\nprint(f\"U-value: {u_value:.2f} W/m\u00b2\u00b7K\")\n</code></pre>"},{"location":"api/thermal/#surface-film-resistances","title":"Surface Film Resistances","text":"<p>The module uses ASHRAE standard film resistances:</p> Surface Resistance (m\u00b2\u00b7K/W) Exterior (15 mph wind) 0.030 Interior (still air) 0.120 Interior ceiling (heat up) 0.107 Interior floor (heat down) 0.160"},{"location":"api/thermal/#glazing-constructions","title":"Glazing Constructions","text":"<p>For glazing constructions (windows), the module calculates:</p> <ul> <li>U-value: Center-of-glass heat transfer coefficient</li> <li>SHGC: Solar Heat Gain Coefficient</li> <li>VT: Visible Transmittance</li> </ul>"},{"location":"api/thermal/#supported-glazing-material-types","title":"Supported Glazing Material Types","text":"Material Type Properties Used <code>WindowMaterial:SimpleGlazingSystem</code> U-factor, SHGC, VT provided directly <code>WindowMaterial:Glazing</code> Thickness, conductivity, optical properties <code>WindowMaterial:Gas</code> Gas type, thickness"},{"location":"api/thermal/#example-double-glazing","title":"Example: Double Glazing","text":"<pre><code>from idfkit import new_document\nfrom idfkit.thermal import calculate_u_value, calculate_shgc, get_thermal_properties\n\ndoc = new_document()\n\n# Add glazing layers\ndoc.add(\"WindowMaterial:Glazing\", \"ClearGlass\", {\n    \"thickness\": 0.006,\n    \"solar_transmittance_at_normal_incidence\": 0.775,\n    \"front_side_solar_reflectance_at_normal_incidence\": 0.071,\n    \"visible_transmittance_at_normal_incidence\": 0.881,\n    \"front_side_infrared_hemispherical_emissivity\": 0.84,\n    \"back_side_infrared_hemispherical_emissivity\": 0.84,\n})\n\ndoc.add(\"WindowMaterial:Gas\", \"ArgonGap\", {\n    \"gas_type\": \"Argon\",\n    \"thickness\": 0.012,\n})\n\ndoc.add(\"WindowMaterial:Glazing\", \"LowEGlass\", {\n    \"thickness\": 0.006,\n    \"solar_transmittance_at_normal_incidence\": 0.6,\n    \"front_side_solar_reflectance_at_normal_incidence\": 0.17,\n    \"visible_transmittance_at_normal_incidence\": 0.78,\n    \"front_side_infrared_hemispherical_emissivity\": 0.84,\n    \"back_side_infrared_hemispherical_emissivity\": 0.10,  # Low-E coating\n})\n\n# Create construction\ndoc.add(\"Construction\", \"DoubleGlazing\", {\n    \"outside_layer\": \"ClearGlass\",\n    \"layer_2\": \"ArgonGap\",\n    \"layer_3\": \"LowEGlass\",\n})\n\nwindow = doc[\"Construction\"][\"DoubleGlazing\"]\n\n# Get all thermal properties\nprops = get_thermal_properties(window)\n\nprint(f\"U-value: {props.u_value:.2f} W/m\u00b2\u00b7K\")\nprint(f\"SHGC: {props.shgc:.2f}\")\nif props.visible_transmittance:\n    print(f\"VT: {props.visible_transmittance:.2f}\")\n</code></pre>"},{"location":"api/thermal/#gas-fill-properties","title":"Gas Fill Properties","text":"<p>The module includes temperature-dependent property correlations for common fill gases used in insulated glazing units:</p> Gas Molecular Weight Thermal Performance Air 28.97 g/mol Baseline Argon 39.95 g/mol ~5% better than air Krypton 83.80 g/mol ~10% better than air Xenon 131.30 g/mol ~15% better than air <pre><code>from idfkit.thermal import typical_gap_r_value\n\n# R-value for a 12mm argon gap\nr_argon = typical_gap_r_value(\"Argon\", 12)  # 12mm\nprint(f\"Argon gap R-value: {r_argon:.3f} m\u00b2\u00b7K/W\")\n</code></pre>"},{"location":"api/thermal/#layer-by-layer-analysis","title":"Layer-by-Layer Analysis","text":"<p>Use <code>get_construction_layers()</code> to analyze individual layers:</p> <pre><code>from idfkit.thermal import get_construction_layers\n\nlayers = get_construction_layers(wall)\n\nfor layer in layers:\n    print(f\"{layer.name}:\")\n    print(f\"  Type: {layer.obj_type}\")\n    if layer.thickness:\n        print(f\"  Thickness: {layer.thickness * 1000:.1f} mm\")\n    print(f\"  R-value: {layer.r_value:.3f} m\u00b2\u00b7K/W\")\n</code></pre>"},{"location":"api/thermal/#api-reference","title":"API Reference","text":""},{"location":"api/thermal/#functions","title":"Functions","text":""},{"location":"api/thermal/#calculate_r_valueconstruction-include_filmstrue","title":"<code>calculate_r_value(construction, include_films=True)</code>","text":"<p>Calculate thermal resistance for a construction.</p> <p>Parameters: - <code>construction</code>: Construction IDFObject - <code>include_films</code>: Include surface film resistances (default: True)</p> <p>Returns: R-value in m\u00b2\u00b7K/W</p>"},{"location":"api/thermal/#calculate_u_valueconstruction","title":"<code>calculate_u_value(construction)</code>","text":"<p>Calculate overall heat transfer coefficient.</p> <p>Parameters: - <code>construction</code>: Construction IDFObject</p> <p>Returns: U-value in W/m\u00b2\u00b7K</p>"},{"location":"api/thermal/#calculate_shgcconstruction","title":"<code>calculate_shgc(construction)</code>","text":"<p>Calculate Solar Heat Gain Coefficient for glazing.</p> <p>Parameters: - <code>construction</code>: Construction IDFObject</p> <p>Returns: SHGC (0-1) or None if not a glazing construction</p>"},{"location":"api/thermal/#calculate_visible_transmittanceconstruction","title":"<code>calculate_visible_transmittance(construction)</code>","text":"<p>Calculate visible light transmittance for glazing.</p> <p>Parameters: - <code>construction</code>: Construction IDFObject</p> <p>Returns: VT (0-1) or None if not a glazing construction</p>"},{"location":"api/thermal/#get_thermal_propertiesconstruction","title":"<code>get_thermal_properties(construction)</code>","text":"<p>Get complete thermal properties for a construction.</p> <p>Parameters: - <code>construction</code>: Construction IDFObject</p> <p>Returns: <code>ConstructionThermalProperties</code> dataclass with: - <code>name</code>: Construction name - <code>layers</code>: List of <code>LayerThermalProperties</code> - <code>r_value</code>: R-value without films - <code>r_value_with_films</code>: R-value with films - <code>u_value</code>: U-value - <code>is_glazing</code>: True if glazing construction - <code>shgc</code>: SHGC (glazing only) - <code>visible_transmittance</code>: VT (glazing only)</p>"},{"location":"api/thermal/#get_construction_layersconstruction","title":"<code>get_construction_layers(construction)</code>","text":"<p>Get thermal properties for each layer.</p> <p>Parameters: - <code>construction</code>: Construction IDFObject</p> <p>Returns: List of <code>LayerThermalProperties</code></p>"},{"location":"api/thermal/#limitations","title":"Limitations","text":"<p>The thermal calculations in this module are simplified approximations:</p> <ol> <li>Center-of-glass only: Frame and edge effects are not included</li> <li>Normal incidence: Optical properties are for perpendicular solar radiation</li> <li>Simplified gas gaps: Uses pre-computed R-values rather than full TARCOG iteration</li> <li>Standard conditions: Film coefficients assume standard ASHRAE/NFRC conditions</li> </ol> <p>For precise calculations, use the full EnergyPlus simulation or specialized tools like WINDOW/THERM.</p>"},{"location":"api/thermal/#see-also","title":"See Also","text":"<ul> <li>Construction Visualization \u2014 SVG diagrams for constructions</li> <li>Objects \u2014 IDFObject reference</li> </ul>"},{"location":"api/validation/","title":"Validation","text":"<p><code>validate_document()</code> checks a document against the epJSON schema: required fields, value types, numeric ranges, enum choices, and reference integrity.</p> <p>On-demand validation system for IDF documents.</p> <p>Provides validation against EpJSON schema without requiring eager validation during parsing.</p>"},{"location":"api/validation/#idfkit.validation.Severity","title":"<code>Severity</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Validation issue severity levels.</p> Source code in <code>src/idfkit/validation.py</code> <pre><code>class Severity(Enum):\n    \"\"\"Validation issue severity levels.\"\"\"\n\n    ERROR = \"error\"\n    WARNING = \"warning\"\n    INFO = \"info\"\n</code></pre>"},{"location":"api/validation/#idfkit.validation.ValidationError","title":"<code>ValidationError</code>  <code>dataclass</code>","text":"<p>Represents a validation issue.</p> <p>Attributes:</p> Name Type Description <code>severity</code> <code>Severity</code> <p>Issue severity (ERROR, WARNING, INFO)</p> <code>obj_type</code> <code>str</code> <p>Object type where issue was found</p> <code>obj_name</code> <code>str</code> <p>Object name where issue was found</p> <code>field</code> <code>str | None</code> <p>Field name where issue was found (if applicable)</p> <code>message</code> <code>str</code> <p>Human-readable description</p> <code>code</code> <code>str</code> <p>Machine-readable error code</p> Source code in <code>src/idfkit/validation.py</code> <pre><code>@dataclass\nclass ValidationError:\n    \"\"\"\n    Represents a validation issue.\n\n    Attributes:\n        severity: Issue severity (ERROR, WARNING, INFO)\n        obj_type: Object type where issue was found\n        obj_name: Object name where issue was found\n        field: Field name where issue was found (if applicable)\n        message: Human-readable description\n        code: Machine-readable error code\n    \"\"\"\n\n    severity: Severity\n    obj_type: str\n    obj_name: str\n    field: str | None\n    message: str\n    code: str\n\n    def __str__(self) -&gt; str:\n        location = f\"{self.obj_type}:'{self.obj_name}'\"\n        if self.field:\n            location += f\".{self.field}\"\n        return f\"[{self.severity.value.upper()}] {location}: {self.message}\"\n</code></pre>"},{"location":"api/validation/#idfkit.validation.ValidationResult","title":"<code>ValidationResult</code>  <code>dataclass</code>","text":"<p>Result of document validation.</p> <p>Attributes:</p> Name Type Description <code>errors</code> <code>list[ValidationError]</code> <p>List of validation errors</p> <code>warnings</code> <code>list[ValidationError]</code> <p>List of validation warnings</p> <code>info</code> <code>list[ValidationError]</code> <p>List of informational messages</p> Source code in <code>src/idfkit/validation.py</code> <pre><code>@dataclass\nclass ValidationResult:\n    \"\"\"\n    Result of document validation.\n\n    Attributes:\n        errors: List of validation errors\n        warnings: List of validation warnings\n        info: List of informational messages\n    \"\"\"\n\n    errors: list[ValidationError]\n    warnings: list[ValidationError]\n    info: list[ValidationError]\n\n    @property\n    def is_valid(self) -&gt; bool:\n        \"\"\"True if there are no errors.\"\"\"\n        return len(self.errors) == 0\n\n    @property\n    def total_issues(self) -&gt; int:\n        \"\"\"Total number of issues found.\"\"\"\n        return len(self.errors) + len(self.warnings) + len(self.info)\n\n    def __str__(self) -&gt; str:\n        lines = [f\"Validation: {len(self.errors)} errors, {len(self.warnings)} warnings\"]\n        for err in self.errors[:10]:\n            lines.append(f\"  {err}\")\n        if len(self.errors) &gt; 10:\n            lines.append(f\"  ... and {len(self.errors) - 10} more errors\")\n        return \"\\n\".join(lines)\n\n    def __bool__(self) -&gt; bool:\n        return self.is_valid\n</code></pre>"},{"location":"api/validation/#idfkit.validation.ValidationResult.is_valid","title":"<code>is_valid</code>  <code>property</code>","text":"<p>True if there are no errors.</p>"},{"location":"api/validation/#idfkit.validation.ValidationResult.total_issues","title":"<code>total_issues</code>  <code>property</code>","text":"<p>Total number of issues found.</p>"},{"location":"api/validation/#idfkit.validation.validate_document","title":"<code>validate_document(doc, schema=None, check_references=True, check_required=True, check_types=True, check_ranges=True, object_types=None)</code>","text":"<p>Validate an IDF document against schema.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to validate</p> required <code>schema</code> <code>EpJSONSchema | None</code> <p>Schema to validate against (uses doc's schema if not provided)</p> <code>None</code> <code>check_references</code> <code>bool</code> <p>Check reference integrity</p> <code>True</code> <code>check_required</code> <code>bool</code> <p>Check required fields</p> <code>True</code> <code>check_types</code> <code>bool</code> <p>Check field types</p> <code>True</code> <code>check_ranges</code> <code>bool</code> <p>Check numeric ranges</p> <code>True</code> <code>object_types</code> <code>list[str] | None</code> <p>Only validate these types (None = all)</p> <code>None</code> <p>Returns:</p> Type Description <code>ValidationResult</code> <p>ValidationResult with all issues found</p> <p>Examples:</p> <p>Validate a model before running a simulation:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document, validate_document\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")\nZone('Perimeter_ZN_1')\n&gt;&gt;&gt; result = validate_document(model)\n&gt;&gt;&gt; result.is_valid\nTrue\n&gt;&gt;&gt; result.total_issues\n0\n</code></pre> <p>Validate only material and construction definitions:</p> <pre><code>&gt;&gt;&gt; result = validate_document(model, object_types=[\"Material\", \"Construction\"])\n&gt;&gt;&gt; result.is_valid\nTrue\n</code></pre> Source code in <code>src/idfkit/validation.py</code> <pre><code>def validate_document(  # noqa: C901\n    doc: IDFDocument,\n    schema: EpJSONSchema | None = None,\n    check_references: bool = True,\n    check_required: bool = True,\n    check_types: bool = True,\n    check_ranges: bool = True,\n    object_types: list[str] | None = None,\n) -&gt; ValidationResult:\n    \"\"\"\n    Validate an IDF document against schema.\n\n    Args:\n        doc: The document to validate\n        schema: Schema to validate against (uses doc's schema if not provided)\n        check_references: Check reference integrity\n        check_required: Check required fields\n        check_types: Check field types\n        check_ranges: Check numeric ranges\n        object_types: Only validate these types (None = all)\n\n    Returns:\n        ValidationResult with all issues found\n\n    Examples:\n        Validate a model before running a simulation:\n\n        &gt;&gt;&gt; from idfkit import new_document, validate_document\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; model.add(\"Zone\", \"Perimeter_ZN_1\")  # doctest: +ELLIPSIS\n        Zone('Perimeter_ZN_1')\n        &gt;&gt;&gt; result = validate_document(model)\n        &gt;&gt;&gt; result.is_valid\n        True\n        &gt;&gt;&gt; result.total_issues\n        0\n\n        Validate only material and construction definitions:\n\n        &gt;&gt;&gt; result = validate_document(model, object_types=[\"Material\", \"Construction\"])\n        &gt;&gt;&gt; result.is_valid\n        True\n    \"\"\"\n    schema = schema or doc.schema\n\n    errors: list[ValidationError] = []\n    warnings: list[ValidationError] = []\n    info: list[ValidationError] = []\n\n    if schema is None:\n        warnings.append(\n            ValidationError(\n                severity=Severity.WARNING,\n                obj_type=\"Document\",\n                obj_name=\"\",\n                field=None,\n                message=\"No schema available - skipping schema validation\",\n                code=\"W001\",\n            )\n        )\n        return ValidationResult(errors, warnings, info)\n\n    # Determine which object types to validate\n    types_to_check = object_types or list(doc.collections.keys())\n\n    for obj_type in types_to_check:\n        if obj_type not in doc.collections:\n            continue\n\n        for obj in doc[obj_type]:\n            obj_errors = _validate_object(\n                obj,\n                schema,\n                check_required=check_required,\n                check_types=check_types,\n                check_ranges=check_ranges,\n            )\n\n            for err in obj_errors:\n                if err.severity == Severity.ERROR:\n                    errors.append(err)\n                elif err.severity == Severity.WARNING:\n                    warnings.append(err)\n                else:\n                    info.append(err)\n\n    # Check reference integrity\n    if check_references:\n        ref_errors = _validate_references(doc, schema)\n        for err in ref_errors:\n            if err.severity == Severity.ERROR:\n                errors.append(err)\n            elif err.severity == Severity.WARNING:\n                warnings.append(err)\n\n    return ValidationResult(errors, warnings, info)\n</code></pre>"},{"location":"api/validation/#idfkit.validation.validate_object","title":"<code>validate_object(obj, schema, *, check_required=True, check_types=True, check_ranges=True, check_unknown=True)</code>","text":"<p>Validate a single object against schema.</p> <p>This is a public API for validating individual objects, useful for checking objects at creation time with the validate=True option.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>IDFObject</code> <p>The IDFObject to validate</p> required <code>schema</code> <code>EpJSONSchema</code> <p>The EpJSON schema to validate against</p> required <code>check_required</code> <code>bool</code> <p>Check that required fields are present</p> <code>True</code> <code>check_types</code> <code>bool</code> <p>Check that field values match expected types</p> <code>True</code> <code>check_ranges</code> <code>bool</code> <p>Check that numeric values are within bounds</p> <code>True</code> <code>check_unknown</code> <code>bool</code> <p>Check for unknown fields (not in schema)</p> <code>True</code> <p>Returns:</p> Type Description <code>list[ValidationError]</code> <p>List of ValidationError objects describing any issues found</p> <p>Examples:</p> <p>Check a newly created zone for schema violations:</p> <pre><code>&gt;&gt;&gt; from idfkit import new_document, validate_object, get_schema, LATEST_VERSION\n&gt;&gt;&gt; model = new_document()\n&gt;&gt;&gt; zone = model.add(\"Zone\", \"Perimeter_ZN_1\")\n&gt;&gt;&gt; errors = validate_object(zone, get_schema(LATEST_VERSION))\n&gt;&gt;&gt; len(errors)\n0\n</code></pre> Source code in <code>src/idfkit/validation.py</code> <pre><code>def validate_object(\n    obj: IDFObject,\n    schema: EpJSONSchema,\n    *,\n    check_required: bool = True,\n    check_types: bool = True,\n    check_ranges: bool = True,\n    check_unknown: bool = True,\n) -&gt; list[ValidationError]:\n    \"\"\"\n    Validate a single object against schema.\n\n    This is a public API for validating individual objects, useful for\n    checking objects at creation time with the validate=True option.\n\n    Args:\n        obj: The IDFObject to validate\n        schema: The EpJSON schema to validate against\n        check_required: Check that required fields are present\n        check_types: Check that field values match expected types\n        check_ranges: Check that numeric values are within bounds\n        check_unknown: Check for unknown fields (not in schema)\n\n    Returns:\n        List of ValidationError objects describing any issues found\n\n    Examples:\n        Check a newly created zone for schema violations:\n\n        &gt;&gt;&gt; from idfkit import new_document, validate_object, get_schema, LATEST_VERSION\n        &gt;&gt;&gt; model = new_document()\n        &gt;&gt;&gt; zone = model.add(\"Zone\", \"Perimeter_ZN_1\")\n        &gt;&gt;&gt; errors = validate_object(zone, get_schema(LATEST_VERSION))\n        &gt;&gt;&gt; len(errors)\n        0\n    \"\"\"\n    return _validate_object(\n        obj,\n        schema,\n        check_required=check_required,\n        check_types=check_types,\n        check_ranges=check_ranges,\n        check_unknown=check_unknown,\n    )\n</code></pre>"},{"location":"api/versions/","title":"Versions","text":"<p>Constants and utilities for working with EnergyPlus version numbers. idfkit bundles schemas for every release from v8.9 through v25.2.</p> <p>EnergyPlus version registry.</p> <p>Defines all supported EnergyPlus versions since v8.9 (the first to publish epJSON schema) and provides utilities for version manipulation.</p>"},{"location":"api/versions/#idfkit.versions.find_closest_version","title":"<code>find_closest_version(version)</code>","text":"<p>Find the closest supported version that is &lt;= the given version.</p> <p>This is useful when a file specifies a patch version that doesn't exactly match a supported version (e.g. 9.0.0 -&gt; 9.0.1).</p> <p>Returns:</p> Type Description <code>tuple[int, int, int] | None</code> <p>The closest supported version, or None if no suitable version exists.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_closest_version((24, 1, 5))\n(24, 1, 0)\n&gt;&gt;&gt; find_closest_version((9, 0, 0))\n(8, 9, 0)\n&gt;&gt;&gt; find_closest_version((1, 0, 0)) is None\nTrue\n</code></pre> Source code in <code>src/idfkit/versions.py</code> <pre><code>def find_closest_version(version: tuple[int, int, int]) -&gt; tuple[int, int, int] | None:\n    \"\"\"\n    Find the closest supported version that is &lt;= the given version.\n\n    This is useful when a file specifies a patch version that doesn't\n    exactly match a supported version (e.g. 9.0.0 -&gt; 9.0.1).\n\n    Returns:\n        The closest supported version, or None if no suitable version exists.\n\n    Examples:\n        &gt;&gt;&gt; find_closest_version((24, 1, 5))\n        (24, 1, 0)\n        &gt;&gt;&gt; find_closest_version((9, 0, 0))\n        (8, 9, 0)\n        &gt;&gt;&gt; find_closest_version((1, 0, 0)) is None\n        True\n    \"\"\"\n    best: tuple[int, int, int] | None = None\n    for v in ENERGYPLUS_VERSIONS:\n        if v &lt;= version:\n            best = v\n    return best\n</code></pre>"},{"location":"api/versions/#idfkit.versions.github_release_tag","title":"<code>github_release_tag(version)</code>","text":"<p>Return the GitHub release tag for a version.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; github_release_tag((24, 1, 0))\n'v24.1.0'\n&gt;&gt;&gt; github_release_tag((9, 2, 0))\n'v9.2.0'\n</code></pre> Source code in <code>src/idfkit/versions.py</code> <pre><code>def github_release_tag(version: tuple[int, int, int]) -&gt; str:\n    \"\"\"Return the GitHub release tag for a version.\n\n    Examples:\n        &gt;&gt;&gt; github_release_tag((24, 1, 0))\n        'v24.1.0'\n        &gt;&gt;&gt; github_release_tag((9, 2, 0))\n        'v9.2.0'\n    \"\"\"\n    return f\"v{version[0]}.{version[1]}.{version[2]}\"\n</code></pre>"},{"location":"api/versions/#idfkit.versions.is_supported_version","title":"<code>is_supported_version(version)</code>","text":"<p>Check if a version is in the supported set.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_supported_version((24, 1, 0))\nTrue\n&gt;&gt;&gt; is_supported_version((99, 0, 0))\nFalse\n&gt;&gt;&gt; is_supported_version(MINIMUM_VERSION)\nTrue\n</code></pre> Source code in <code>src/idfkit/versions.py</code> <pre><code>def is_supported_version(version: tuple[int, int, int]) -&gt; bool:\n    \"\"\"Check if a version is in the supported set.\n\n    Examples:\n        &gt;&gt;&gt; is_supported_version((24, 1, 0))\n        True\n        &gt;&gt;&gt; is_supported_version((99, 0, 0))\n        False\n        &gt;&gt;&gt; is_supported_version(MINIMUM_VERSION)\n        True\n    \"\"\"\n    return version in _VERSION_SET\n</code></pre>"},{"location":"api/versions/#idfkit.versions.version_dirname","title":"<code>version_dirname(version)</code>","text":"<p>Return the schema directory name for a version.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; version_dirname((24, 1, 0))\n'V24-1-0'\n&gt;&gt;&gt; version_dirname((9, 6, 0))\n'V9-6-0'\n</code></pre> Source code in <code>src/idfkit/versions.py</code> <pre><code>def version_dirname(version: tuple[int, int, int]) -&gt; str:\n    \"\"\"Return the schema directory name for a version.\n\n    Examples:\n        &gt;&gt;&gt; version_dirname((24, 1, 0))\n        'V24-1-0'\n        &gt;&gt;&gt; version_dirname((9, 6, 0))\n        'V9-6-0'\n    \"\"\"\n    return f\"V{version[0]}-{version[1]}-{version[2]}\"\n</code></pre>"},{"location":"api/versions/#idfkit.versions.version_string","title":"<code>version_string(version)</code>","text":"<p>Format a version tuple as a human-readable string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; version_string((24, 1, 0))\n'24.1.0'\n&gt;&gt;&gt; version_string((9, 6, 0))\n'9.6.0'\n</code></pre> Source code in <code>src/idfkit/versions.py</code> <pre><code>def version_string(version: tuple[int, int, int]) -&gt; str:\n    \"\"\"Format a version tuple as a human-readable string.\n\n    Examples:\n        &gt;&gt;&gt; version_string((24, 1, 0))\n        '24.1.0'\n        &gt;&gt;&gt; version_string((9, 6, 0))\n        '9.6.0'\n    \"\"\"\n    return f\"{version[0]}.{version[1]}.{version[2]}\"\n</code></pre>"},{"location":"api/visualization/","title":"Construction Visualization","text":"<p>The <code>idfkit.visualization</code> module provides SVG diagram generation for construction assemblies, showing layer sequence, thicknesses, and thermal properties.</p>"},{"location":"api/visualization/#quick-start","title":"Quick Start","text":""},{"location":"api/visualization/#in-jupyteripython","title":"In Jupyter/IPython","text":"<p>Construction objects display automatically as SVG diagrams in Jupyter notebooks:</p> <pre><code>from idfkit import load_idf\n\nmodel = load_idf(\"building.idf\")\nwall = model[\"Construction\"][\"ExteriorWall\"]\n\n# Just display the construction - SVG renders automatically\nwall\n</code></pre> <p></p>"},{"location":"api/visualization/#manual-svg-generation","title":"Manual SVG Generation","text":"<pre><code>from idfkit.visualization import construction_to_svg\n\nsvg = construction_to_svg(wall)\n\n# Save to file\nwith open(\"wall_section.svg\", \"w\") as f:\n    f.write(svg)\n</code></pre>"},{"location":"api/visualization/#diagram-features","title":"Diagram Features","text":"<p>The SVG diagram includes:</p> <ul> <li>Layer rectangles proportional to thickness</li> <li>Material names below each layer</li> <li>Thickness labels for each layer</li> <li>Thermal properties (U-value, R-value, SHGC)</li> <li>Outside/Inside indicators</li> <li>Color coding by material type</li> </ul>"},{"location":"api/visualization/#opaque-constructions","title":"Opaque Constructions","text":"<p>For walls, roofs, and floors:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  ExteriorWall                    U = 0.54 W/m\u00b2\u00b7K       \u2502\n\u2502                                  R = 1.87 m\u00b2\u00b7K/W       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 OUT                                               IN   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2510           \u2502\n\u2502  \u2502\u2588\u2588\u2588\u2588\u2502    \u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502  \u2502           \u2502\n\u2502  \u2502\u2588\u2588\u2588\u2588\u2502air \u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502  \u2502           \u2502\n\u2502  \u2502\u2588\u2588\u2588\u2588\u2502gap \u2502\u2591\u2591insulation\u2591\u2591\u2502\u2588\u2588concrete\u2588\u2588\u2502pl\u2502           \u2502\n\u2502  \u2502\u2588\u2588\u2588\u2588\u2502    \u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502  \u2502           \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2518           \u2502\n\u2502  brick  R=0.15  R=1.25        R=0.12    R=0.06        \u2502\n\u2502  0.1m          0.05m          0.2m      0.01m         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"api/visualization/#window-constructions","title":"Window Constructions","text":"<p>For glazing systems, SHGC is also displayed:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TripleGlazing                   U = 0.78 W/m\u00b2\u00b7K       \u2502\n\u2502                                  SHGC = 0.47           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 OUT                                               IN   \u2502\n\u2502  \u250c\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2510                       \u2502\n\u2502  \u2502\u2592\u2592\u2502  Argon  \u2502\u2592\u2592\u2502  Argon  \u2502\u2592\u2592\u2502                       \u2502\n\u2502  \u2502\u2592\u2592\u2502         \u2502\u2592\u2592\u2502         \u2502\u2592\u2592\u2502                       \u2502\n\u2502  \u2502\u2592\u2592\u2502   12mm  \u2502\u2592\u2592\u2502   12mm  \u2502\u2592\u2592\u2502   \u2190 Low-E indicator  \u2502\n\u2502  \u2502\u2592\u2592\u2502         \u2502\u2592\u2592\u2502         \u2502\u2592\u2592\u2502                       \u2502\n\u2502  \u2514\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2518                       \u2502\n\u2502  6mm           6mm          6mm                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Low-E coatings (emissivity &lt; 0.2) are indicated with an orange line on the coated surface.</p>"},{"location":"api/visualization/#color-coding","title":"Color Coding","text":"<p>Materials are color-coded based on type and name:</p>"},{"location":"api/visualization/#opaque-materials","title":"Opaque Materials","text":"Material Type Color Concrete Gray (#808080) Brick/Masonry Firebrick (#B22222) Insulation Gold (#FFD700) Wood Burlywood (#DEB887) Gypsum/Drywall Beige (#F5F5DC) Plaster/Stucco Floral White (#FFFAF0) Metal Silver (#C0C0C0) Other Sienna (#A0522D)"},{"location":"api/visualization/#special-layer-types","title":"Special Layer Types","text":"Layer Type Visual <code>Material:NoMass</code> Lavender with dot pattern <code>Material:AirGap</code> Light blue with diagonal lines <code>WindowMaterial:Glazing</code> Sky blue with vertical lines <code>WindowMaterial:Gas</code> Alice blue (very light)"},{"location":"api/visualization/#customization","title":"Customization","text":"<p>Use <code>SVGConfig</code> to customize the diagram appearance:</p> <pre><code>from idfkit.thermal import get_thermal_properties\nfrom idfkit.visualization import SVGConfig, generate_construction_svg\n\nprops = get_thermal_properties(wall)\n\nconfig = SVGConfig(\n    width=800,           # SVG width in pixels\n    height=300,          # SVG height in pixels\n    padding=30,          # Padding around diagram\n    min_layer_width=40,  # Minimum width for thin layers\n    font_size=14,        # Base font size\n)\n\nsvg = generate_construction_svg(props, config)\n</code></pre>"},{"location":"api/visualization/#svgconfig-options","title":"SVGConfig Options","text":"Option Default Description <code>width</code> 600 Total SVG width in pixels <code>height</code> 200 Total SVG height in pixels <code>padding</code> 20 Padding around the diagram <code>header_height</code> 40 Height of header section <code>footer_height</code> 50 Height of footer/labels <code>min_layer_width</code> 30 Minimum layer width in pixels <code>font_family</code> system-ui Font family for text <code>font_size</code> 12 Base font size <code>font_size_small</code> 10 Small label font size <code>theme</code> <code>\"light\"</code> Color theme: <code>\"light\"</code>, <code>\"dark\"</code>, or <code>\"auto\"</code>"},{"location":"api/visualization/#theming","title":"Theming","text":"<p>SVG diagrams support light and dark color themes via CSS custom properties.</p>"},{"location":"api/visualization/#dark-mode","title":"Dark Mode","text":"<pre><code>from idfkit.visualization import SVGConfig, construction_to_svg\n\nsvg = construction_to_svg(wall, config=SVGConfig(theme=\"dark\"))\n</code></pre>"},{"location":"api/visualization/#auto-os-preference","title":"Auto (OS Preference)","text":"<p>The <code>\"auto\"</code> theme uses <code>@media (prefers-color-scheme: dark)</code> to automatically match the user's OS or browser setting:</p> <pre><code>svg = construction_to_svg(wall, config=SVGConfig(theme=\"auto\"))\n</code></pre> <p>When embedded in HTML or displayed in Jupyter, the SVG will switch between light and dark palettes based on the host environment's color scheme.</p>"},{"location":"api/visualization/#embedding-in-html","title":"Embedding in HTML","text":"<p>The generated SVG can be embedded directly in HTML:</p> <pre><code>from idfkit.visualization import construction_to_svg\n\nsvg = construction_to_svg(wall)\n\nhtml = f\"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;Wall Construction&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Exterior Wall Assembly&lt;/h1&gt;\n    {svg}\n&lt;/body&gt;\n&lt;/html&gt;\n\"\"\"\n\nwith open(\"wall.html\", \"w\") as f:\n    f.write(html)\n</code></pre>"},{"location":"api/visualization/#api-reference","title":"API Reference","text":""},{"location":"api/visualization/#functions","title":"Functions","text":""},{"location":"api/visualization/#construction_to_svgconstruction-confignone","title":"<code>construction_to_svg(construction, config=None)</code>","text":"<p>Generate SVG for a Construction IDFObject.</p> <p>Parameters: - <code>construction</code>: Construction IDFObject (must have <code>obj_type == \"Construction\"</code>) - <code>config</code>: Optional <code>SVGConfig</code> for customization (including theme)</p> <p>Returns: SVG string</p> <p>Raises: - <code>TypeError</code>: If not an IDFObject or not a Construction</p>"},{"location":"api/visualization/#generate_construction_svgprops-confignone","title":"<code>generate_construction_svg(props, config=None)</code>","text":"<p>Generate SVG from thermal properties.</p> <p>Parameters: - <code>props</code>: <code>ConstructionThermalProperties</code> from <code>get_thermal_properties()</code> - <code>config</code>: Optional <code>SVGConfig</code> for customization</p> <p>Returns: SVG string</p>"},{"location":"api/visualization/#classes","title":"Classes","text":""},{"location":"api/visualization/#svgconfig","title":"<code>SVGConfig</code>","text":"<p>Configuration dataclass for SVG diagram customization.</p> <p>See SVGConfig Options for available settings.</p>"},{"location":"api/visualization/#ipythonjupyter-integration","title":"IPython/Jupyter Integration","text":"<p>Construction objects implement <code>_repr_svg_()</code> for automatic rich display:</p> <pre><code># In Jupyter, this displays as SVG:\nwall\n\n# Check if SVG is available:\nsvg = wall._repr_svg_()\nif svg:\n    print(\"SVG available\")\nelse:\n    print(\"SVG not available (not a Construction or no document)\")\n</code></pre> <p>The <code>_repr_svg_()</code> method returns: - SVG string for Construction objects with a parent document - <code>None</code> for non-Construction objects or objects without a document reference</p>"},{"location":"api/visualization/#3d-model-visualization","title":"3D Model Visualization","text":"<p>The <code>idfkit.visualization</code> module also provides interactive 3D building model viewers using plotly. These are useful for model QA and geometry exploration in Jupyter notebooks.</p> <p>Requires the <code>plotly</code> extra: <code>pip install idfkit[plotly]</code></p>"},{"location":"api/visualization/#quick-start_1","title":"Quick Start","text":"<pre><code>from idfkit import load_idf\nfrom idfkit.visualization import view_model\n\nmodel = load_idf(\"building.idf\")\nfig = view_model(model)\nfig.show()\n</code></pre>"},{"location":"api/visualization/#available-views","title":"Available Views","text":""},{"location":"api/visualization/#view_modeldoc-config-title-zones","title":"<code>view_model(doc, *, config, title, zones)</code>","text":"<p>Interactive 3D building viewer with orbit, pan, and zoom controls.</p> <pre><code>from idfkit.visualization import view_model, ModelViewConfig, ColorBy\n\nfig = view_model(model, config=ModelViewConfig(color_by=ColorBy.SURFACE_TYPE))\n</code></pre>"},{"location":"api/visualization/#view_floor_plandoc-config-title-z_cut-zones","title":"<code>view_floor_plan(doc, *, config, title, z_cut, zones)</code>","text":"<p>2D top-down floor plan projection. Shows floor polygons; optionally slices walls at a given Z height.</p> <pre><code>from idfkit.visualization import view_floor_plan\n\nfig = view_floor_plan(model, z_cut=1.0)\n</code></pre>"},{"location":"api/visualization/#view_explodeddoc-config-title-separation-zones","title":"<code>view_exploded(doc, *, config, title, separation, zones)</code>","text":"<p>Pulls zones apart to reveal internal partitions and inter-zone surfaces.</p> <pre><code>from idfkit.visualization import view_exploded\n\nfig = view_exploded(model, separation=5.0)\n</code></pre>"},{"location":"api/visualization/#view_normalsdoc-config-title-arrow_length-zones","title":"<code>view_normals(doc, *, config, title, arrow_length, zones)</code>","text":"<p>Displays surface normal arrows for orientation QA. Useful for checking that surfaces face the correct direction.</p> <pre><code>from idfkit.visualization import view_normals\n\nfig = view_normals(model, arrow_length=1.5)\n</code></pre>"},{"location":"api/visualization/#configuration","title":"Configuration","text":"<p>Use <code>ModelViewConfig</code> to customize the 3D view appearance:</p> <pre><code>from idfkit.visualization import ModelViewConfig, ColorBy\n\nconfig = ModelViewConfig(\n    width=1200,\n    height=800,\n    color_by=ColorBy.BOUNDARY_CONDITION,\n    show_fenestration=True,\n    show_edges=True,\n    show_labels=True,\n    opacity=0.9,\n)\n</code></pre>"},{"location":"api/visualization/#modelviewconfig-options","title":"ModelViewConfig Options","text":"Option Default Description <code>width</code> 1000 Figure width in pixels <code>height</code> 700 Figure height in pixels <code>color_by</code> <code>ColorBy.ZONE</code> Coloring strategy <code>show_fenestration</code> <code>True</code> Show windows and doors <code>show_edges</code> <code>True</code> Show wireframe edges <code>show_labels</code> <code>True</code> Show zone name labels <code>opacity</code> 0.85 Surface opacity (0-1) <code>fenestration_opacity</code> 0.4 Window/door opacity (0-1) <code>background_color</code> <code>#f8f9fa</code> Plot background color <code>edge_color</code> <code>rgba(40,40,40,0.6)</code> Wireframe edge color <code>edge_width</code> 1.5 Wireframe edge width"},{"location":"api/visualization/#colorby-options","title":"ColorBy Options","text":"Value Description <code>ColorBy.ZONE</code> Color surfaces by thermal zone <code>ColorBy.SURFACE_TYPE</code> Wall / Floor / Roof / Ceiling <code>ColorBy.BOUNDARY_CONDITION</code> Outdoors / Ground / Surface / Adiabatic <code>ColorBy.CONSTRUCTION</code> By construction name"},{"location":"api/visualization/#filtering-by-zone","title":"Filtering by Zone","text":"<p>All view functions accept a <code>zones</code> parameter to display only specific zones:</p> <pre><code>fig = view_model(model, zones=[\"Zone1\", \"Zone2\"])\n</code></pre>"},{"location":"api/visualization/#see-also","title":"See Also","text":"<ul> <li>Thermal Properties \u2014 R-value, U-value, SHGC calculations</li> <li>Objects \u2014 IDFObject reference</li> </ul>"},{"location":"api/zoning/","title":"Zoning","text":"<p>Automatic thermal zoning for EnergyPlus models.  Splits a 2-D building footprint into thermal zones and creates all <code>Zone</code>, <code>BuildingSurface:Detailed</code>, and (optionally) <code>Construction:AirBoundary</code> objects needed for simulation.</p>"},{"location":"api/zoning/#quick-start","title":"Quick Start","text":"<pre><code>from idfkit import new_document, create_building, ZoningScheme\nfrom idfkit.zoning import footprint_rectangle\n\ndoc = new_document()\ncreate_building(\n    doc,\n    name=\"Office\",\n    footprint=footprint_rectangle(50, 30),\n    floor_to_floor=3.5,\n    num_stories=3,\n    zoning=ZoningScheme.CORE_PERIMETER,\n)\n\n# 3 stories \u00d7 5 zones (4 perimeter + 1 core) = 15 zones\nprint(len(doc[\"Zone\"]))  # 15\n</code></pre>"},{"location":"api/zoning/#zoning-schemes","title":"Zoning Schemes","text":"<p><code>create_building</code> supports three zoning strategies via the <code>zoning</code> parameter:</p> Scheme Zones per floor Description <code>BY_STOREY</code> 1 One zone per floor (default). <code>CORE_PERIMETER</code> 5 Four orientation-based perimeter zones plus an interior core zone. <code>CUSTOM</code> User-defined Caller supplies named zone polygons via <code>custom_zones</code>."},{"location":"api/zoning/#by-storey-default","title":"By Storey (default)","text":"<p>The simplest scheme \u2014 one thermal zone per floor:</p> <pre><code>from idfkit import new_document, create_building\n\ndoc = new_document()\ncreate_building(\n    doc,\n    name=\"Warehouse\",\n    footprint=[(0, 0), (40, 0), (40, 20), (0, 20)],\n    floor_to_floor=4.0,\n    num_stories=2,\n)\n\nprint(len(doc[\"Zone\"]))  # 2\n</code></pre>"},{"location":"api/zoning/#core-perimeter","title":"Core-Perimeter","text":"<p>Splits each floor into four perimeter zones (North, East, South, West) and one interior core zone.  The perimeter depth defaults to 4.57 m (15 ft) per ASHRAE 90.1 Appendix G and the DOE prototype buildings.</p> <pre><code>from idfkit import new_document, create_building, ZoningScheme\nfrom idfkit.zoning import footprint_rectangle\n\ndoc = new_document()\ncreate_building(\n    doc,\n    name=\"Office\",\n    footprint=footprint_rectangle(50, 30),\n    floor_to_floor=3.5,\n    num_stories=3,\n    zoning=ZoningScheme.CORE_PERIMETER,\n)\n\n# 3 stories \u00d7 5 zones = 15 zones\nprint(len(doc[\"Zone\"]))  # 15\n</code></pre> <p>You can override the perimeter depth:</p> <pre><code>from idfkit import new_document, create_building, ZoningScheme\nfrom idfkit.zoning import footprint_rectangle\n\ndoc = new_document()\ncreate_building(\n    doc,\n    name=\"Office\",\n    footprint=footprint_rectangle(50, 30),\n    floor_to_floor=3.5,\n    num_stories=1,\n    zoning=ZoningScheme.CORE_PERIMETER,\n    perimeter_depth=3.0,  # 3 m instead of the default 4.57 m\n)\n</code></pre> <p>Note</p> <p>When the footprint is too small for the requested perimeter depth (i.e. the inradius is less than 0.5 m after insetting), zoning automatically falls back to a single zone per floor.</p>"},{"location":"api/zoning/#custom-zoning","title":"Custom Zoning","text":"<p>Supply your own named zone polygons per floor using <code>custom_zones</code>. Each entry is a <code>(name, polygon)</code> tuple:</p> <pre><code>from idfkit import ZoneFootprint, ZoningScheme, create_building, new_document\n\ndoc = new_document()\ncreate_building(\n    doc,\n    name=\"Lab\",\n    footprint=[(0, 0), (30, 0), (30, 20), (0, 20)],\n    floor_to_floor=3.5,\n    num_stories=1,\n    zoning=ZoningScheme.CUSTOM,\n    custom_zones=[\n        ZoneFootprint(\"Wet Lab\", [(0, 0), (15, 0), (15, 20), (0, 20)]),\n        ZoneFootprint(\"Dry Lab\", [(15, 0), (30, 0), (30, 20), (15, 20)]),\n    ],\n)\n\nprint(len(doc[\"Zone\"]))  # 2\n</code></pre>"},{"location":"api/zoning/#air-boundaries","title":"Air Boundaries","text":"<p>Set <code>air_boundary=True</code> to apply <code>Construction:AirBoundary</code> to all inter-zone walls.  This is useful for open-plan spaces where zone boundaries are notional rather than physical:</p> <pre><code>from idfkit import new_document, create_building, ZoningScheme\nfrom idfkit.zoning import footprint_rectangle\n\ndoc = new_document()\ncreate_building(\n    doc,\n    name=\"Open Office\",\n    footprint=footprint_rectangle(50, 30),\n    floor_to_floor=3.5,\n    num_stories=1,\n    zoning=ZoningScheme.CORE_PERIMETER,\n    air_boundary=True,\n)\n\n# A Construction:AirBoundary object is created automatically\nprint(len(doc[\"Construction:AirBoundary\"]))  # 1\n</code></pre>"},{"location":"api/zoning/#multi-story-boundary-conditions","title":"Multi-Story Boundary Conditions","text":"<p>For multi-story buildings, inter-story floors and ceilings are automatically linked with <code>Surface</code> boundary conditions:</p> Story Floor BC Ceiling BC Ground floor <code>Ground</code> <code>Surface</code> (story above) Mid floors <code>Surface</code> (story below) <code>Surface</code> (story above) Top floor <code>Surface</code> (story below) <code>Outdoors</code> (Roof)"},{"location":"api/zoning/#footprint-helpers","title":"Footprint Helpers","text":"<p>Pre-built footprint generators for common commercial building shapes. All return a list of <code>(x, y)</code> tuples in counter-clockwise order.</p>"},{"location":"api/zoning/#footprint_rectangle","title":"<code>footprint_rectangle</code>","text":"<pre><code>from idfkit.zoning import footprint_rectangle\n\nfp = footprint_rectangle(50, 30)\n# [(0, 0), (50, 0), (50, 30), (0, 30)]\n</code></pre>"},{"location":"api/zoning/#footprint_l_shape","title":"<code>footprint_l_shape</code>","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  wing  \u2502\n\u2502        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      base         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>from idfkit.zoning import footprint_l_shape\n\nfp = footprint_l_shape(width=40, depth=10, wing_width=15, wing_depth=20)\n</code></pre>"},{"location":"api/zoning/#footprint_u_shape","title":"<code>footprint_u_shape</code>","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      \u2502    \u2502      \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>from idfkit.zoning import footprint_u_shape\n\nfp = footprint_u_shape(width=40, depth=30, courtyard_width=20, courtyard_depth=15)\n</code></pre>"},{"location":"api/zoning/#footprint_t_shape","title":"<code>footprint_t_shape</code>","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       top bar        \u2502\n\u2514\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2518\n    \u2502    base      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>from idfkit.zoning import footprint_t_shape\n\nfp = footprint_t_shape(base_width=20, base_depth=15, top_width=40, top_depth=10)\n</code></pre>"},{"location":"api/zoning/#footprint_h_shape","title":"<code>footprint_h_shape</code>","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502     connector    \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>from idfkit.zoning import footprint_h_shape\n\nfp = footprint_h_shape(width=40, depth=30, courtyard_width=20, courtyard_depth=10)\n</code></pre>"},{"location":"api/zoning/#footprint_courtyard","title":"<code>footprint_courtyard</code>","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  courtyard \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>from idfkit.zoning import footprint_courtyard\n\nfp = footprint_courtyard(outer_width=50, outer_depth=40, inner_width=30, inner_depth=20)\n</code></pre>"},{"location":"api/zoning/#using-footprint-helpers-with-create_building","title":"Using Footprint Helpers with <code>create_building</code>","text":"<p>All footprint helpers plug directly into <code>create_building</code>:</p> <pre><code>from idfkit import new_document, create_building, ZoningScheme\nfrom idfkit.zoning import footprint_l_shape\n\ndoc = new_document()\ncreate_building(\n    doc,\n    name=\"L-Wing\",\n    footprint=footprint_l_shape(40, 10, 15, 20),\n    floor_to_floor=3.5,\n    num_stories=2,\n    zoning=ZoningScheme.CORE_PERIMETER,\n)\n</code></pre>"},{"location":"api/zoning/#zonedblock-describe-then-apply","title":"ZonedBlock (Describe-then-Apply)","text":"<p><code>ZonedBlock</code> is a frozen dataclass alternative that validates all parameters up front.  Call <code>build()</code> to realise the geometry. This describe-then-apply pattern lets you inspect computed properties before committing to a document.</p> <pre><code>from idfkit import new_document, ZonedBlock, ZoningScheme\nfrom idfkit.zoning import footprint_rectangle\n\nblock = ZonedBlock(\n    name=\"Office\",\n    footprint=footprint_rectangle(50, 30),\n    floor_to_floor=3.5,\n    num_stories=3,\n    zoning=ZoningScheme.CORE_PERIMETER,\n)\n\nprint(f\"Building height: {block.height} m\")              # 10.5\nprint(f\"Floor area: {block.floor_area} m\u00b2\")               # 1500.0\nprint(f\"Total floor area: {block.total_floor_area} m\u00b2\")  # 4500.0\n\ndoc = new_document()\nobjects = block.build(doc)\nprint(len(objects))  # all created Zone + BuildingSurface:Detailed objects\n</code></pre>"},{"location":"api/zoning/#api-reference","title":"API Reference","text":"<p>Automatic thermal zoning for EnergyPlus models.</p> <p>Splits a 2-D building footprint into thermal zones using one of several standard schemes and creates all <code>Zone</code>, <code>BuildingSurface:Detailed</code>, and (optionally) <code>Construction:AirBoundary</code> objects needed for simulation.</p> <p>Three zoning schemes are provided:</p> <ul> <li>by_storey - one zone per floor.</li> <li>core_perimeter - four orientation-based perimeter zones plus an   interior core zone per floor.  Perimeter depth defaults to   4.57 m (15 ft) per ASHRAE 90.1 Appendix G and the DOE prototype   buildings.</li> <li>custom - the caller supplies named polygons for each floor.</li> </ul> <p>Footprint helpers for common commercial shapes (rectangle, L, U, T, H, courtyard) are included so users never have to compute vertices by hand.</p> <p>Examples:</p> <pre><code>from idfkit import new_document\nfrom idfkit.zoning import (\n    ZoningScheme,\n    create_building,\n    footprint_rectangle,\n)\n\ndoc = new_document()\nzones = create_building(\n    doc,\n    name=\"Office\",\n    footprint=footprint_rectangle(50, 30),\n    floor_to_floor=3.5,\n    num_stories=3,\n    zoning=ZoningScheme.CORE_PERIMETER,\n    perimeter_depth=4.57,\n)\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.ZoneFootprint","title":"<code>ZoneFootprint</code>  <code>dataclass</code>","text":"<p>Named 2-D polygon for one thermal zone on one floor.</p> Source code in <code>src/idfkit/zoning.py</code> <pre><code>@dataclass(frozen=True)\nclass ZoneFootprint:\n    \"\"\"Named 2-D polygon for one thermal zone on one floor.\"\"\"\n\n    name_suffix: str  # e.g. \"Core\", \"Perimeter_South\"\n    polygon: list[tuple[float, float]]\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.ZonedBlock","title":"<code>ZonedBlock</code>  <code>dataclass</code>","text":"<p>Describes a building block with a zoning strategy.</p> <p>This is a pure data object.  Call [build][] to realise the geometry in an IDFDocument.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Base name for zones and surfaces.</p> <code>footprint</code> <code>Sequence[tuple[float, float]]</code> <p>2-D footprint as <code>(x, y)</code> tuples (CCW).</p> <code>floor_to_floor</code> <code>float</code> <p>Floor-to-floor height in metres.</p> <code>num_stories</code> <code>int</code> <p>Number of above-ground stories.</p> <code>zoning</code> <code>ZoningScheme</code> <p>Zoning strategy.</p> <code>perimeter_depth</code> <code>float</code> <p>Perimeter zone depth (metres), used only when <code>zoning</code> is <code>CORE_PERIMETER</code>.</p> <code>custom_zones</code> <code>list[ZoneFootprint] | None</code> <p>Per-floor named zone polygons, used only when <code>zoning</code> is <code>CUSTOM</code>.</p> <code>air_boundary</code> <code>bool</code> <p>Whether to use <code>Construction:AirBoundary</code> between inter-zone walls.</p> Source code in <code>src/idfkit/zoning.py</code> <pre><code>@dataclass(frozen=True)\nclass ZonedBlock:\n    \"\"\"Describes a building block with a zoning strategy.\n\n    This is a pure data object.  Call [build][] to realise the\n    geometry in an [IDFDocument][idfkit.document.IDFDocument].\n\n    Attributes:\n        name: Base name for zones and surfaces.\n        footprint: 2-D footprint as ``(x, y)`` tuples (CCW).\n        floor_to_floor: Floor-to-floor height in metres.\n        num_stories: Number of above-ground stories.\n        zoning: Zoning strategy.\n        perimeter_depth: Perimeter zone depth (metres), used only\n            when ``zoning`` is ``CORE_PERIMETER``.\n        custom_zones: Per-floor named zone polygons, used only when\n            ``zoning`` is ``CUSTOM``.\n        air_boundary: Whether to use ``Construction:AirBoundary``\n            between inter-zone walls.\n    \"\"\"\n\n    name: str\n    footprint: Sequence[tuple[float, float]]\n    floor_to_floor: float\n    num_stories: int = 1\n    zoning: ZoningScheme = ZoningScheme.BY_STOREY\n    perimeter_depth: float = ASHRAE_PERIMETER_DEPTH\n    custom_zones: list[ZoneFootprint] | None = None\n    air_boundary: bool = False\n\n    def __post_init__(self) -&gt; None:\n        if len(self.footprint) &lt; 3:\n            msg = f\"Footprint must have at least 3 vertices, got {len(self.footprint)}\"\n            raise ValueError(msg)\n        if self.floor_to_floor &lt;= 0:\n            msg = f\"floor_to_floor must be positive, got {self.floor_to_floor}\"\n            raise ValueError(msg)\n        if self.num_stories &lt; 1:\n            msg = f\"num_stories must be &gt;= 1, got {self.num_stories}\"\n            raise ValueError(msg)\n        if self.perimeter_depth &lt;= 0:\n            msg = f\"perimeter_depth must be positive, got {self.perimeter_depth}\"\n            raise ValueError(msg)\n        if self.zoning == ZoningScheme.CUSTOM and not self.custom_zones:\n            msg = \"custom_zones is required when zoning is CUSTOM\"\n            raise ValueError(msg)\n\n    @property\n    def height(self) -&gt; float:\n        \"\"\"Total building height in metres.\"\"\"\n        return self.floor_to_floor * self.num_stories\n\n    @property\n    def floor_area(self) -&gt; float:\n        \"\"\"Single-floor footprint area in square metres.\"\"\"\n        return abs(_polygon_area_signed(list(self.footprint)))\n\n    @property\n    def total_floor_area(self) -&gt; float:\n        \"\"\"Total floor area across all stories in square metres.\"\"\"\n        return self.floor_area * self.num_stories\n\n    def build(self, doc: IDFDocument) -&gt; list[IDFObject]:\n        \"\"\"Realise the zoned geometry in the document.\n\n        Returns:\n            All created [IDFObject][idfkit.objects.IDFObject] instances.\n        \"\"\"\n        fp = list(self.footprint)\n\n        # Determine zone layout per floor\n        if self.zoning == ZoningScheme.CORE_PERIMETER:\n            zone_footprints = _split_core_perimeter(fp, self.perimeter_depth)\n        elif self.zoning == ZoningScheme.CUSTOM:\n            zone_footprints = self.custom_zones or [ZoneFootprint(\"Whole\", fp)]\n        else:  # BY_STOREY\n            zone_footprints = [ZoneFootprint(\"Whole\", fp)]\n\n        # Build story specs\n        story_specs: list[_StorySpec] = []\n        for i in range(self.num_stories):\n            z_bot = i * self.floor_to_floor\n            z_top = (i + 1) * self.floor_to_floor\n            story_specs.append(_StorySpec(i + 1, z_bot, z_top, zone_footprints))\n\n        # Create surfaces for each story\n        created: list[IDFObject] = []\n        for spec in story_specs:\n            created.extend(\n                _build_story_surfaces(\n                    doc,\n                    self.name,\n                    spec,\n                    self.num_stories,\n                    all_story_specs=story_specs,\n                    air_boundary=self.air_boundary,\n                )\n            )\n        return created\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.ZonedBlock.floor_area","title":"<code>floor_area</code>  <code>property</code>","text":"<p>Single-floor footprint area in square metres.</p>"},{"location":"api/zoning/#idfkit.zoning.ZonedBlock.height","title":"<code>height</code>  <code>property</code>","text":"<p>Total building height in metres.</p>"},{"location":"api/zoning/#idfkit.zoning.ZonedBlock.total_floor_area","title":"<code>total_floor_area</code>  <code>property</code>","text":"<p>Total floor area across all stories in square metres.</p>"},{"location":"api/zoning/#idfkit.zoning.ZonedBlock.build","title":"<code>build(doc)</code>","text":"<p>Realise the zoned geometry in the document.</p> <p>Returns:</p> Type Description <code>list[IDFObject]</code> <p>All created IDFObject instances.</p> Source code in <code>src/idfkit/zoning.py</code> <pre><code>def build(self, doc: IDFDocument) -&gt; list[IDFObject]:\n    \"\"\"Realise the zoned geometry in the document.\n\n    Returns:\n        All created [IDFObject][idfkit.objects.IDFObject] instances.\n    \"\"\"\n    fp = list(self.footprint)\n\n    # Determine zone layout per floor\n    if self.zoning == ZoningScheme.CORE_PERIMETER:\n        zone_footprints = _split_core_perimeter(fp, self.perimeter_depth)\n    elif self.zoning == ZoningScheme.CUSTOM:\n        zone_footprints = self.custom_zones or [ZoneFootprint(\"Whole\", fp)]\n    else:  # BY_STOREY\n        zone_footprints = [ZoneFootprint(\"Whole\", fp)]\n\n    # Build story specs\n    story_specs: list[_StorySpec] = []\n    for i in range(self.num_stories):\n        z_bot = i * self.floor_to_floor\n        z_top = (i + 1) * self.floor_to_floor\n        story_specs.append(_StorySpec(i + 1, z_bot, z_top, zone_footprints))\n\n    # Create surfaces for each story\n    created: list[IDFObject] = []\n    for spec in story_specs:\n        created.extend(\n            _build_story_surfaces(\n                doc,\n                self.name,\n                spec,\n                self.num_stories,\n                all_story_specs=story_specs,\n                air_boundary=self.air_boundary,\n            )\n        )\n    return created\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.ZoningScheme","title":"<code>ZoningScheme</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Thermal zoning strategy.</p> <p>Attributes:</p> Name Type Description <code>BY_STOREY</code> <p>One zone per floor.</p> <code>CORE_PERIMETER</code> <p>Core + 4 perimeter zones per floor.</p> <code>CUSTOM</code> <p>User-supplied zone polygons per floor.</p> Source code in <code>src/idfkit/zoning.py</code> <pre><code>class ZoningScheme(enum.Enum):\n    \"\"\"Thermal zoning strategy.\n\n    Attributes:\n        BY_STOREY: One zone per floor.\n        CORE_PERIMETER: Core + 4 perimeter zones per floor.\n        CUSTOM: User-supplied zone polygons per floor.\n    \"\"\"\n\n    BY_STOREY = \"by_storey\"\n    CORE_PERIMETER = \"core_perimeter\"\n    CUSTOM = \"custom\"\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.create_building","title":"<code>create_building(doc, name, footprint, floor_to_floor, num_stories=1, *, zoning=ZoningScheme.BY_STOREY, perimeter_depth=ASHRAE_PERIMETER_DEPTH, custom_zones=None, air_boundary=False)</code>","text":"<p>Create a fully-zoned building in one call.</p> <p>This is the primary entry point for the zoning module.  It combines footprint definition, zoning strategy, and multi-story extrusion into a single function call.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The document to add objects to.</p> required <code>name</code> <code>str</code> <p>Base name for zones and surfaces (e.g. <code>\"Office\"</code>).</p> required <code>footprint</code> <code>Sequence[tuple[float, float]]</code> <p>2-D footprint as <code>(x, y)</code> tuples (CCW order).</p> required <code>floor_to_floor</code> <code>float</code> <p>Floor-to-floor height in metres.</p> required <code>num_stories</code> <code>int</code> <p>Number of above-ground stories.</p> <code>1</code> <code>zoning</code> <code>ZoningScheme</code> <p>Zoning strategy (default: one zone per floor).</p> <code>BY_STOREY</code> <code>perimeter_depth</code> <code>float</code> <p>Depth of perimeter zones in metres. Only used when <code>zoning</code> is <code>CORE_PERIMETER</code>. Defaults to 4.57 m (ASHRAE 90.1 / DOE prototypes).</p> <code>ASHRAE_PERIMETER_DEPTH</code> <code>custom_zones</code> <code>list[ZoneFootprint] | None</code> <p>Named zone polygons, required when <code>zoning</code> is <code>CUSTOM</code>.</p> <code>None</code> <code>air_boundary</code> <code>bool</code> <p>If <code>True</code>, apply <code>Construction:AirBoundary</code> to all inter-zone walls (for open-plan spaces).</p> <code>False</code> <p>Returns:</p> Type Description <code>list[IDFObject]</code> <p>All created IDFObject instances.</p> <p>Examples:</p> <p>Core-perimeter zoning for a 3-story office:</p> <pre><code>```python\nfrom idfkit import new_document\nfrom idfkit.zoning import (\n    ZoningScheme,\n    create_building,\n    footprint_rectangle,\n)\n\ndoc = new_document()\ncreate_building(\n    doc,\n    name=\"Office\",\n    footprint=footprint_rectangle(50, 30),\n    floor_to_floor=3.5,\n    num_stories=3,\n    zoning=ZoningScheme.CORE_PERIMETER,\n)\n```\n</code></pre> Source code in <code>src/idfkit/zoning.py</code> <pre><code>def create_building(\n    doc: IDFDocument,\n    name: str,\n    footprint: Sequence[tuple[float, float]],\n    floor_to_floor: float,\n    num_stories: int = 1,\n    *,\n    zoning: ZoningScheme = ZoningScheme.BY_STOREY,\n    perimeter_depth: float = ASHRAE_PERIMETER_DEPTH,\n    custom_zones: list[ZoneFootprint] | None = None,\n    air_boundary: bool = False,\n) -&gt; list[IDFObject]:\n    \"\"\"Create a fully-zoned building in one call.\n\n    This is the primary entry point for the zoning module.  It combines\n    footprint definition, zoning strategy, and multi-story extrusion into\n    a single function call.\n\n    Args:\n        doc: The document to add objects to.\n        name: Base name for zones and surfaces (e.g. ``\"Office\"``).\n        footprint: 2-D footprint as ``(x, y)`` tuples (CCW order).\n        floor_to_floor: Floor-to-floor height in metres.\n        num_stories: Number of above-ground stories.\n        zoning: Zoning strategy (default: one zone per floor).\n        perimeter_depth: Depth of perimeter zones in metres.\n            Only used when ``zoning`` is ``CORE_PERIMETER``.\n            Defaults to 4.57 m (ASHRAE 90.1 / DOE prototypes).\n        custom_zones: Named zone polygons, required when ``zoning``\n            is ``CUSTOM``.\n        air_boundary: If ``True``, apply ``Construction:AirBoundary``\n            to all inter-zone walls (for open-plan spaces).\n\n    Returns:\n        All created [IDFObject][idfkit.objects.IDFObject] instances.\n\n    Examples:\n        Core-perimeter zoning for a 3-story office:\n\n            ```python\n            from idfkit import new_document\n            from idfkit.zoning import (\n                ZoningScheme,\n                create_building,\n                footprint_rectangle,\n            )\n\n            doc = new_document()\n            create_building(\n                doc,\n                name=\"Office\",\n                footprint=footprint_rectangle(50, 30),\n                floor_to_floor=3.5,\n                num_stories=3,\n                zoning=ZoningScheme.CORE_PERIMETER,\n            )\n            ```\n    \"\"\"\n    block = ZonedBlock(\n        name=name,\n        footprint=footprint,\n        floor_to_floor=floor_to_floor,\n        num_stories=num_stories,\n        zoning=zoning,\n        perimeter_depth=perimeter_depth,\n        custom_zones=custom_zones,\n        air_boundary=air_boundary,\n    )\n    return block.build(doc)\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.footprint_courtyard","title":"<code>footprint_courtyard(outer_width, outer_depth, inner_width, inner_depth, origin=(0.0, 0.0))</code>","text":"<p>Return a courtyard (donut) footprint as a single slit polygon.</p> <p>The polygon traces the outer boundary counter-clockwise, steps into the inner courtyard through a slit at the bottom-right corner, traces the courtyard clockwise, and returns.  This is a valid simple polygon that EnergyPlus can handle.</p> <pre><code>```text\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  courtyard \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>outer_width</code> <code>float</code> <p>Outer bounding box width (X).</p> required <code>outer_depth</code> <code>float</code> <p>Outer bounding box depth (Y).</p> required <code>inner_width</code> <code>float</code> <p>Courtyard width (X), must be &lt; outer_width.</p> required <code>inner_depth</code> <code>float</code> <p>Courtyard depth (Y), must be &lt; outer_depth.</p> required <code>origin</code> <code>tuple[float, float]</code> <p><code>(x, y)</code> of the lower-left corner.</p> <code>(0.0, 0.0)</code> Source code in <code>src/idfkit/zoning.py</code> <pre><code>def footprint_courtyard(\n    outer_width: float,\n    outer_depth: float,\n    inner_width: float,\n    inner_depth: float,\n    origin: tuple[float, float] = (0.0, 0.0),\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Return a courtyard (donut) footprint as a single slit polygon.\n\n    The polygon traces the outer boundary counter-clockwise, steps into\n    the inner courtyard through a slit at the bottom-right corner, traces\n    the courtyard clockwise, and returns.  This is a valid simple polygon\n    that EnergyPlus can handle.\n\n        ```text\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n        \u2502  \u2502  courtyard \u2502  \u2502\n        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n    Args:\n        outer_width: Outer bounding box width (X).\n        outer_depth: Outer bounding box depth (Y).\n        inner_width: Courtyard width (X), must be &lt; *outer_width*.\n        inner_depth: Courtyard depth (Y), must be &lt; *outer_depth*.\n        origin: ``(x, y)`` of the lower-left corner.\n    \"\"\"\n    if inner_width &gt;= outer_width:\n        msg = f\"inner_width ({inner_width}) must be &lt; outer_width ({outer_width})\"\n        raise ValueError(msg)\n    if inner_depth &gt;= outer_depth:\n        msg = f\"inner_depth ({inner_depth}) must be &lt; outer_depth ({outer_depth})\"\n        raise ValueError(msg)\n    x, y = origin\n    margin_x = (outer_width - inner_width) / 2\n    margin_y = (outer_depth - inner_depth) / 2\n    # Outer CCW\n    ix0 = x + margin_x\n    iy0 = y + margin_y\n    ix1 = ix0 + inner_width\n    iy1 = iy0 + inner_depth\n    return [\n        # Outer rectangle (CCW)\n        (x, y),\n        (x + outer_width, y),\n        (x + outer_width, y + outer_depth),\n        (x, y + outer_depth),\n        # Slit down to inner (from outer top-left back down to inner)\n        (x, y + margin_y),  # slit entry\n        # Inner rectangle (CW to cut a hole)\n        (ix0, iy0),\n        (ix0, iy1),\n        (ix1, iy1),\n        (ix1, iy0),\n        # Slit back out\n        (x, y + margin_y),  # slit exit (same point, degenerate edge)\n    ]\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.footprint_h_shape","title":"<code>footprint_h_shape(width, depth, courtyard_width, courtyard_depth, origin=(0.0, 0.0))</code>","text":"<p>Return an H-shaped footprint (counter-clockwise).</p> <p>Two symmetrical courtyards are cut from the left and right sides of the bounding rectangle.</p> <pre><code>```text\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502     connector    \u2502\n\u2502      \u250c\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>float</code> <p>Overall width (X).</p> required <code>depth</code> <code>float</code> <p>Overall depth (Y).</p> required <code>courtyard_width</code> <code>float</code> <p>Width of each courtyard notch (X).</p> required <code>courtyard_depth</code> <code>float</code> <p>Depth of each courtyard notch (Y).</p> required <code>origin</code> <code>tuple[float, float]</code> <p><code>(x, y)</code> of the lower-left corner.</p> <code>(0.0, 0.0)</code> Source code in <code>src/idfkit/zoning.py</code> <pre><code>def footprint_h_shape(\n    width: float,\n    depth: float,\n    courtyard_width: float,\n    courtyard_depth: float,\n    origin: tuple[float, float] = (0.0, 0.0),\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Return an H-shaped footprint (counter-clockwise).\n\n    Two symmetrical courtyards are cut from the left and right sides of\n    the bounding rectangle.\n\n        ```text\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502      \u2514\u2500\u2500\u2500\u2500\u2518      \u2502\n        \u2502     connector    \u2502\n        \u2502      \u250c\u2500\u2500\u2500\u2500\u2510      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n    Args:\n        width: Overall width (X).\n        depth: Overall depth (Y).\n        courtyard_width: Width of each courtyard notch (X).\n        courtyard_depth: Depth of each courtyard notch (Y).\n        origin: ``(x, y)`` of the lower-left corner.\n    \"\"\"\n    if courtyard_width &gt;= width:\n        msg = f\"courtyard_width ({courtyard_width}) must be &lt; width ({width})\"\n        raise ValueError(msg)\n    if 2 * courtyard_depth &gt;= depth:\n        msg = f\"2 * courtyard_depth ({2 * courtyard_depth}) must be &lt; depth ({depth})\"\n        raise ValueError(msg)\n    x, y = origin\n    w2 = (width - courtyard_width) / 2\n    cy_bot_top = y + courtyard_depth\n    cy_top_bot = y + depth - courtyard_depth\n    cx_left = x + w2\n    cx_right = x + w2 + courtyard_width\n    return [\n        (x, y),\n        (cx_left, y),\n        (cx_left, cy_bot_top),\n        (cx_right, cy_bot_top),\n        (cx_right, y),\n        (x + width, y),\n        (x + width, y + depth),\n        (cx_right, y + depth),\n        (cx_right, cy_top_bot),\n        (cx_left, cy_top_bot),\n        (cx_left, y + depth),\n        (x, y + depth),\n    ]\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.footprint_l_shape","title":"<code>footprint_l_shape(width, depth, wing_width, wing_depth, origin=(0.0, 0.0))</code>","text":"<p>Return an L-shaped footprint (counter-clockwise).</p> <p>The base rectangle runs from the origin along <code>width</code> (X) and <code>depth</code> (Y).  A shorter wing extends upward from the left side with dimensions <code>wing_width</code> x <code>wing_depth</code>.</p> <pre><code>```text\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  wing  \u2502\n\u2502        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      base         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>float</code> <p>Base width (X).</p> required <code>depth</code> <code>float</code> <p>Base depth (Y).</p> required <code>wing_width</code> <code>float</code> <p>Wing width (X), must be &lt;= width.</p> required <code>wing_depth</code> <code>float</code> <p>Wing depth (Y), added above the base.</p> required <code>origin</code> <code>tuple[float, float]</code> <p><code>(x, y)</code> of the lower-left corner.</p> <code>(0.0, 0.0)</code> Source code in <code>src/idfkit/zoning.py</code> <pre><code>def footprint_l_shape(\n    width: float,\n    depth: float,\n    wing_width: float,\n    wing_depth: float,\n    origin: tuple[float, float] = (0.0, 0.0),\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Return an L-shaped footprint (counter-clockwise).\n\n    The *base* rectangle runs from the origin along ``width`` (X) and\n    ``depth`` (Y).  A shorter wing extends upward from the left side\n    with dimensions ``wing_width`` x ``wing_depth``.\n\n        ```text\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502  wing  \u2502\n        \u2502        \u2502\n        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502      base         \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n    Args:\n        width: Base width (X).\n        depth: Base depth (Y).\n        wing_width: Wing width (X), must be &lt;= *width*.\n        wing_depth: Wing depth (Y), added above the base.\n        origin: ``(x, y)`` of the lower-left corner.\n    \"\"\"\n    if wing_width &gt; width:\n        msg = f\"wing_width ({wing_width}) must be &lt;= width ({width})\"\n        raise ValueError(msg)\n    x, y = origin\n    return [\n        (x, y),\n        (x + width, y),\n        (x + width, y + depth),\n        (x + wing_width, y + depth),\n        (x + wing_width, y + depth + wing_depth),\n        (x, y + depth + wing_depth),\n    ]\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.footprint_rectangle","title":"<code>footprint_rectangle(width, depth, origin=(0.0, 0.0))</code>","text":"<p>Return a rectangular footprint (counter-clockwise).</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>float</code> <p>Dimension along the X axis (metres).</p> required <code>depth</code> <code>float</code> <p>Dimension along the Y axis (metres).</p> required <code>origin</code> <code>tuple[float, float]</code> <p><code>(x, y)</code> of the lower-left corner.</p> <code>(0.0, 0.0)</code> Source code in <code>src/idfkit/zoning.py</code> <pre><code>def footprint_rectangle(\n    width: float,\n    depth: float,\n    origin: tuple[float, float] = (0.0, 0.0),\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Return a rectangular footprint (counter-clockwise).\n\n    Args:\n        width: Dimension along the X axis (metres).\n        depth: Dimension along the Y axis (metres).\n        origin: ``(x, y)`` of the lower-left corner.\n    \"\"\"\n    x, y = origin\n    return [(x, y), (x + width, y), (x + width, y + depth), (x, y + depth)]\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.footprint_t_shape","title":"<code>footprint_t_shape(base_width, base_depth, top_width, top_depth, origin=(0.0, 0.0))</code>","text":"<p>Return a T-shaped footprint (counter-clockwise).</p> <p>A narrower base rectangle is centred below a wider top bar.</p> <pre><code>```text\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       top bar        \u2502\n\u2514\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2518\n    \u2502    base      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>base_width</code> <code>float</code> <p>Width of the stem (X).</p> required <code>base_depth</code> <code>float</code> <p>Depth of the stem (Y).</p> required <code>top_width</code> <code>float</code> <p>Width of the top bar (X), must be &gt;= base_width.</p> required <code>top_depth</code> <code>float</code> <p>Depth of the top bar (Y).</p> required <code>origin</code> <code>tuple[float, float]</code> <p><code>(x, y)</code> of the lower-left corner of the stem.</p> <code>(0.0, 0.0)</code> Source code in <code>src/idfkit/zoning.py</code> <pre><code>def footprint_t_shape(\n    base_width: float,\n    base_depth: float,\n    top_width: float,\n    top_depth: float,\n    origin: tuple[float, float] = (0.0, 0.0),\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Return a T-shaped footprint (counter-clockwise).\n\n    A narrower base rectangle is centred below a wider top bar.\n\n        ```text\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502       top bar        \u2502\n        \u2514\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2518\n            \u2502    base      \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n    Args:\n        base_width: Width of the stem (X).\n        base_depth: Depth of the stem (Y).\n        top_width: Width of the top bar (X), must be &gt;= *base_width*.\n        top_depth: Depth of the top bar (Y).\n        origin: ``(x, y)`` of the lower-left corner of the stem.\n    \"\"\"\n    if top_width &lt; base_width:\n        msg = f\"top_width ({top_width}) must be &gt;= base_width ({base_width})\"\n        raise ValueError(msg)\n    x, y = origin\n    overhang = (top_width - base_width) / 2\n    return [\n        (x, y),\n        (x + base_width, y),\n        (x + base_width, y + base_depth),\n        (x + base_width + overhang, y + base_depth),\n        (x + base_width + overhang, y + base_depth + top_depth),\n        (x - overhang, y + base_depth + top_depth),\n        (x - overhang, y + base_depth),\n        (x, y + base_depth),\n    ]\n</code></pre>"},{"location":"api/zoning/#idfkit.zoning.footprint_u_shape","title":"<code>footprint_u_shape(width, depth, courtyard_width, courtyard_depth, origin=(0.0, 0.0))</code>","text":"<p>Return a U-shaped footprint (counter-clockwise).</p> <p>The overall bounding box is <code>width</code> x <code>depth</code>.  A rectangular courtyard is cut from the top centre of the footprint.</p> <pre><code>```text\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      \u2502    \u2502      \u2502\n\u2502      \u2514\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>float</code> <p>Overall width (X).</p> required <code>depth</code> <code>float</code> <p>Overall depth (Y).</p> required <code>courtyard_width</code> <code>float</code> <p>Width of the courtyard opening (X).</p> required <code>courtyard_depth</code> <code>float</code> <p>Depth of the courtyard from the top edge (Y).</p> required <code>origin</code> <code>tuple[float, float]</code> <p><code>(x, y)</code> of the lower-left corner.</p> <code>(0.0, 0.0)</code> Source code in <code>src/idfkit/zoning.py</code> <pre><code>def footprint_u_shape(\n    width: float,\n    depth: float,\n    courtyard_width: float,\n    courtyard_depth: float,\n    origin: tuple[float, float] = (0.0, 0.0),\n) -&gt; list[tuple[float, float]]:\n    \"\"\"Return a U-shaped footprint (counter-clockwise).\n\n    The overall bounding box is ``width`` x ``depth``.  A rectangular\n    courtyard is cut from the top centre of the footprint.\n\n        ```text\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502      \u2502    \u2502      \u2502\n        \u2502      \u2514\u2500\u2500\u2500\u2500\u2518      \u2502\n        \u2502                  \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n    Args:\n        width: Overall width (X).\n        depth: Overall depth (Y).\n        courtyard_width: Width of the courtyard opening (X).\n        courtyard_depth: Depth of the courtyard from the top edge (Y).\n        origin: ``(x, y)`` of the lower-left corner.\n    \"\"\"\n    if courtyard_width &gt;= width:\n        msg = f\"courtyard_width ({courtyard_width}) must be &lt; width ({width})\"\n        raise ValueError(msg)\n    if courtyard_depth &gt;= depth:\n        msg = f\"courtyard_depth ({courtyard_depth}) must be &lt; depth ({depth})\"\n        raise ValueError(msg)\n    x, y = origin\n    cx_left = x + (width - courtyard_width) / 2\n    cx_right = cx_left + courtyard_width\n    cy_bottom = y + depth - courtyard_depth\n    return [\n        (x, y),\n        (x + width, y),\n        (x + width, y + depth),\n        (cx_right, y + depth),\n        (cx_right, cy_bottom),\n        (cx_left, cy_bottom),\n        (cx_left, y + depth),\n        (x, y + depth),\n    ]\n</code></pre>"},{"location":"api/zoning/#see-also","title":"See Also","text":"<ul> <li>Geometry Builders -- Shading blocks and utility functions   (<code>bounding_box</code>, <code>scale_building</code>, <code>set_default_constructions</code>)</li> <li>Geometry -- Lower-level 3D primitives, coordinate transforms,   and surface intersection</li> <li>Visualization -- 3D rendering of building geometry</li> </ul>"},{"location":"api/schedules/","title":"Schedules API Reference","text":"<p>The schedules module provides functions to evaluate EnergyPlus schedules without running a simulation.</p>"},{"location":"api/schedules/#quick-reference","title":"Quick Reference","text":"Function/Class Description <code>evaluate()</code> Evaluate a schedule at a specific datetime <code>values()</code> Generate values for a date range <code>to_series()</code> Convert to pandas Series <code>plot_schedule()</code> Quick visualization <code>get_holidays()</code> Extract holidays from model"},{"location":"api/schedules/#string-parameters","title":"String Parameters","text":"<p>The <code>day_type</code> and <code>interpolation</code> parameters accept strings for convenience:</p> <p>day_type: <code>\"normal\"</code>, <code>\"summer\"</code>, <code>\"winter\"</code>, <code>\"holiday\"</code>, <code>\"customday1\"</code>, <code>\"customday2\"</code></p> <p>interpolation: <code>\"no\"</code>, <code>\"step\"</code>, <code>\"average\"</code>, <code>\"linear\"</code></p>"},{"location":"api/schedules/#core-functions","title":"Core Functions","text":""},{"location":"api/schedules/#idfkit.schedules.evaluate","title":"<code>idfkit.schedules.evaluate</code>","text":"<p>Core schedule evaluation logic.</p> <p>Provides the main evaluate() and values() functions that dispatch to type-specific evaluators.</p>"},{"location":"api/schedules/#idfkit.schedules.evaluate.MalformedScheduleError","title":"<code>MalformedScheduleError</code>","text":"<p>               Bases: <code>ScheduleEvaluationError</code></p> <p>Raised when a schedule has invalid syntax.</p>"},{"location":"api/schedules/#idfkit.schedules.evaluate.ScheduleEvaluationError","title":"<code>ScheduleEvaluationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for schedule evaluation errors.</p>"},{"location":"api/schedules/#idfkit.schedules.evaluate.ScheduleReferenceError","title":"<code>ScheduleReferenceError</code>","text":"<p>               Bases: <code>ScheduleEvaluationError</code></p> <p>Raised when a referenced schedule cannot be found.</p>"},{"location":"api/schedules/#idfkit.schedules.evaluate.UnsupportedScheduleType","title":"<code>UnsupportedScheduleType</code>","text":"<p>               Bases: <code>ScheduleEvaluationError</code></p> <p>Raised when a schedule type is not supported.</p>"},{"location":"api/schedules/#idfkit.schedules.evaluate.evaluate","title":"<code>evaluate(schedule, dt, document=None, day_type=None, fs=None, base_path=None)</code>","text":"<p>Evaluate a schedule at a specific datetime.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>IDFObject</code> <p>An IDF schedule object (any supported type).</p> required <code>dt</code> <code>datetime</code> <p>The datetime to evaluate.</p> required <code>document</code> <code>IDFDocument | None</code> <p>Required for schedules that reference others (Year, Week).       If None, extracted from schedule._document if available.</p> <code>None</code> <code>day_type</code> <code>DayTypeInput | None</code> <p>Override with design day schedule (for sizing calculations).       Accepts \"normal\", \"summer\", \"winter\", \"holiday\", \"customday1\", \"customday2\".</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>FileSystem for Schedule:File (default: LocalFileSystem).</p> <code>None</code> <code>base_path</code> <code>Path | str | None</code> <p>Base directory for Schedule:File relative paths.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>The schedule value as a float.</p> <p>Raises:</p> Type Description <code>ScheduleEvaluationError</code> <p>If schedule type is unsupported or malformed.</p> <code>ScheduleReferenceError</code> <p>If a referenced schedule cannot be found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; evaluate(schedule, datetime(2024, 7, 15, 14, 0))  # Normal evaluation\n&gt;&gt;&gt; evaluate(schedule, dt, day_type=\"summer\")  # Summer design day\n&gt;&gt;&gt; evaluate(schedule, dt, day_type=\"winter\")  # Winter design day\n</code></pre>"},{"location":"api/schedules/#idfkit.schedules.evaluate.values","title":"<code>values(schedule, year=2024, timestep=1, start_date=(1, 1), end_date=(12, 31), document=None, day_type=None, interpolation=None, fs=None, base_path=None)</code>","text":"<p>Generate schedule values for a date range.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>IDFObject</code> <p>An IDF schedule object.</p> required <code>year</code> <code>int</code> <p>Year for the date range.</p> <code>2024</code> <code>timestep</code> <code>int</code> <p>Values per hour (1, 2, 4, 6, 12, or 60).</p> <code>1</code> <code>start_date</code> <code>tuple[int, int]</code> <p>Start date as (month, day).</p> <code>(1, 1)</code> <code>end_date</code> <code>tuple[int, int]</code> <p>End date as (month, day).</p> <code>(12, 31)</code> <code>document</code> <code>IDFDocument | None</code> <p>Required for schedules that reference others.</p> <code>None</code> <code>day_type</code> <code>DayTypeInput | None</code> <p>Override day type for all days.       Accepts \"normal\", \"summer\", \"winter\", \"holiday\", \"customday1\", \"customday2\".</p> <code>None</code> <code>interpolation</code> <code>InterpolationInput | None</code> <p>Interpolation mode for sub-hourly values.            Accepts \"no\", \"step\", \"average\", \"linear\".</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>FileSystem for Schedule:File.</p> <code>None</code> <code>base_path</code> <code>Path | str | None</code> <p>Base directory for Schedule:File relative paths.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of values, one per timestep for the entire period.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; values(schedule, year=2024)  # Hourly values for full year\n&gt;&gt;&gt; values(schedule, year=2024, timestep=4)  # 15-minute intervals\n&gt;&gt;&gt; values(schedule, year=2024, day_type=\"summer\")  # Summer design day\n</code></pre>"},{"location":"api/schedules/#idfkit.schedules.values","title":"<code>idfkit.schedules.values(schedule, year=2024, timestep=1, start_date=(1, 1), end_date=(12, 31), document=None, day_type=None, interpolation=None, fs=None, base_path=None)</code>","text":"<p>Generate schedule values for a date range.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>IDFObject</code> <p>An IDF schedule object.</p> required <code>year</code> <code>int</code> <p>Year for the date range.</p> <code>2024</code> <code>timestep</code> <code>int</code> <p>Values per hour (1, 2, 4, 6, 12, or 60).</p> <code>1</code> <code>start_date</code> <code>tuple[int, int]</code> <p>Start date as (month, day).</p> <code>(1, 1)</code> <code>end_date</code> <code>tuple[int, int]</code> <p>End date as (month, day).</p> <code>(12, 31)</code> <code>document</code> <code>IDFDocument | None</code> <p>Required for schedules that reference others.</p> <code>None</code> <code>day_type</code> <code>DayTypeInput | None</code> <p>Override day type for all days.       Accepts \"normal\", \"summer\", \"winter\", \"holiday\", \"customday1\", \"customday2\".</p> <code>None</code> <code>interpolation</code> <code>InterpolationInput | None</code> <p>Interpolation mode for sub-hourly values.            Accepts \"no\", \"step\", \"average\", \"linear\".</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>FileSystem for Schedule:File.</p> <code>None</code> <code>base_path</code> <code>Path | str | None</code> <p>Base directory for Schedule:File relative paths.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of values, one per timestep for the entire period.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; values(schedule, year=2024)  # Hourly values for full year\n&gt;&gt;&gt; values(schedule, year=2024, timestep=4)  # 15-minute intervals\n&gt;&gt;&gt; values(schedule, year=2024, day_type=\"summer\")  # Summer design day\n</code></pre>"},{"location":"api/schedules/#pandas-integration","title":"Pandas Integration","text":""},{"location":"api/schedules/#idfkit.schedules.to_series","title":"<code>idfkit.schedules.to_series(schedule, year=2024, freq='h', start_date=(1, 1), end_date=(12, 31), document=None, day_type=None, interpolation=None, fs=None, base_path=None)</code>","text":"<p>Convert a schedule to a pandas Series with DatetimeIndex.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>IDFObject</code> <p>An IDF schedule object.</p> required <code>year</code> <code>int</code> <p>Year for the date range.</p> <code>2024</code> <code>freq</code> <code>str</code> <p>Pandas frequency string (\"h\" for hourly, \"30min\", \"15min\", etc.).</p> <code>'h'</code> <code>start_date</code> <code>tuple[int, int]</code> <p>Start date as (month, day).</p> <code>(1, 1)</code> <code>end_date</code> <code>tuple[int, int]</code> <p>End date as (month, day).</p> <code>(12, 31)</code> <code>document</code> <code>IDFDocument | None</code> <p>Required for schedules that reference others.</p> <code>None</code> <code>day_type</code> <code>DayTypeInput | None</code> <p>Override day type (\"normal\", \"summer\", \"winter\", etc.).</p> <code>None</code> <code>interpolation</code> <code>InterpolationInput | None</code> <p>Interpolation mode (\"no\", \"average\", \"linear\").</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>FileSystem for Schedule:File.</p> <code>None</code> <code>base_path</code> <code>Path | str | None</code> <p>Base directory for Schedule:File relative paths.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>pandas Series with DatetimeIndex.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pandas is not installed.</p>"},{"location":"api/schedules/#idfkit.schedules.plot_schedule","title":"<code>idfkit.schedules.plot_schedule(schedule, year=2024, start_date=(1, 1), end_date=(12, 31), document=None, day_type=None, fs=None, base_path=None, **plot_kwargs)</code>","text":"<p>Plot a schedule as a time series.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>IDFObject</code> <p>An IDF schedule object.</p> required <code>year</code> <code>int</code> <p>Year for the date range.</p> <code>2024</code> <code>start_date</code> <code>tuple[int, int]</code> <p>Start date as (month, day).</p> <code>(1, 1)</code> <code>end_date</code> <code>tuple[int, int]</code> <p>End date as (month, day).</p> <code>(12, 31)</code> <code>document</code> <code>IDFDocument | None</code> <p>Required for schedules that reference others.</p> <code>None</code> <code>day_type</code> <code>DayTypeInput | None</code> <p>Override day type (\"normal\", \"summer\", \"winter\", etc.).</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>FileSystem for Schedule:File.</p> <code>None</code> <code>base_path</code> <code>Path | str | None</code> <p>Base directory for Schedule:File relative paths.</p> <code>None</code> <code>**plot_kwargs</code> <code>Any</code> <p>Additional arguments passed to pandas Series.plot().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>matplotlib Axes object.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pandas or matplotlib is not installed.</p>"},{"location":"api/schedules/#idfkit.schedules.plot_week","title":"<code>idfkit.schedules.plot_week(schedule, year=2024, week=1, document=None, day_type=None, fs=None, base_path=None, **plot_kwargs)</code>","text":"<p>Plot a schedule for a single week.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>IDFObject</code> <p>An IDF schedule object.</p> required <code>year</code> <code>int</code> <p>Year.</p> <code>2024</code> <code>week</code> <code>int</code> <p>ISO week number (1-53).</p> <code>1</code> <code>document</code> <code>IDFDocument | None</code> <p>Required for schedules that reference others.</p> <code>None</code> <code>day_type</code> <code>DayTypeInput | None</code> <p>Override day type (\"normal\", \"summer\", \"winter\", etc.).</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>FileSystem for Schedule:File.</p> <code>None</code> <code>base_path</code> <code>Path | str | None</code> <p>Base directory for Schedule:File relative paths.</p> <code>None</code> <code>**plot_kwargs</code> <code>Any</code> <p>Additional arguments passed to pandas Series.plot().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>matplotlib Axes object.</p>"},{"location":"api/schedules/#idfkit.schedules.plot_day","title":"<code>idfkit.schedules.plot_day(schedule, year=2024, month=1, day=1, document=None, day_type=None, fs=None, base_path=None, **plot_kwargs)</code>","text":"<p>Plot a schedule for a single day.</p> <p>Parameters:</p> Name Type Description Default <code>schedule</code> <code>IDFObject</code> <p>An IDF schedule object.</p> required <code>year</code> <code>int</code> <p>Year.</p> <code>2024</code> <code>month</code> <code>int</code> <p>Month (1-12).</p> <code>1</code> <code>day</code> <code>int</code> <p>Day (1-31).</p> <code>1</code> <code>document</code> <code>IDFDocument | None</code> <p>Required for schedules that reference others.</p> <code>None</code> <code>day_type</code> <code>DayTypeInput | None</code> <p>Override day type (\"normal\", \"summer\", \"winter\", etc.).</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>FileSystem for Schedule:File.</p> <code>None</code> <code>base_path</code> <code>Path | str | None</code> <p>Base directory for Schedule:File relative paths.</p> <code>None</code> <code>**plot_kwargs</code> <code>Any</code> <p>Additional arguments passed to pandas Series.plot().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Axes</code> <p>matplotlib Axes object.</p>"},{"location":"api/schedules/#enumerations","title":"Enumerations","text":""},{"location":"api/schedules/#idfkit.schedules.DayType","title":"<code>idfkit.schedules.DayType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Special day type for schedule evaluation.</p> <p>Used to override the calendar day with a design day schedule, which is useful for sizing calculations.</p>"},{"location":"api/schedules/#idfkit.schedules.DayType.NORMAL","title":"<code>NORMAL = 'normal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use calendar day (weekday, weekend, or holiday based on date).</p>"},{"location":"api/schedules/#idfkit.schedules.DayType.SUMMER_DESIGN","title":"<code>SUMMER_DESIGN = 'summer'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use SummerDesignDay schedule regardless of actual date.</p>"},{"location":"api/schedules/#idfkit.schedules.DayType.WINTER_DESIGN","title":"<code>WINTER_DESIGN = 'winter'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use WinterDesignDay schedule regardless of actual date.</p>"},{"location":"api/schedules/#idfkit.schedules.DayType.HOLIDAY","title":"<code>HOLIDAY = 'holiday'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Treat as holiday regardless of actual date.</p>"},{"location":"api/schedules/#idfkit.schedules.DayType.CUSTOM_DAY_1","title":"<code>CUSTOM_DAY_1 = 'customday1'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use CustomDay1 schedule.</p>"},{"location":"api/schedules/#idfkit.schedules.DayType.CUSTOM_DAY_2","title":"<code>CUSTOM_DAY_2 = 'customday2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use CustomDay2 schedule.</p>"},{"location":"api/schedules/#idfkit.schedules.Interpolation","title":"<code>idfkit.schedules.Interpolation</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Interpolation mode for schedule values.</p> <p>Controls how values are computed when the evaluation timestep doesn't align with the schedule's native intervals.</p>"},{"location":"api/schedules/#idfkit.schedules.Interpolation.NO","title":"<code>NO = 'no'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Step function - value at each interval applies until next interval.</p>"},{"location":"api/schedules/#idfkit.schedules.Interpolation.AVERAGE","title":"<code>AVERAGE = 'average'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Linear interpolation when timestep doesn't align with intervals.</p>"},{"location":"api/schedules/#idfkit.schedules.Interpolation.LINEAR","title":"<code>LINEAR = 'linear'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Alias for AVERAGE.</p>"},{"location":"api/schedules/#holiday-functions","title":"Holiday Functions","text":""},{"location":"api/schedules/#idfkit.schedules.get_holidays","title":"<code>idfkit.schedules.get_holidays(doc, year)</code>","text":"<p>Get all dates marked as Holiday for a given year.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The IDF document.</p> required <code>year</code> <code>int</code> <p>Year to get holidays for.</p> required <p>Returns:</p> Type Description <code>set[date]</code> <p>Set of dates that are holidays.</p>"},{"location":"api/schedules/#idfkit.schedules.get_special_days_by_type","title":"<code>idfkit.schedules.get_special_days_by_type(doc, year, day_type)</code>","text":"<p>Get all dates of a specific day type.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The IDF document.</p> required <code>year</code> <code>int</code> <p>Year to get dates for.</p> required <code>day_type</code> <code>str</code> <p>The day type to filter by (\"Holiday\", \"CustomDay1\", \"CustomDay2\").</p> required <p>Returns:</p> Type Description <code>set[date]</code> <p>Set of dates matching the day type.</p>"},{"location":"api/schedules/#idfkit.schedules.extract_special_days","title":"<code>idfkit.schedules.extract_special_days(doc, year)</code>","text":"<p>Parse all RunPeriodControl:SpecialDays objects from a document.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>IDFDocument</code> <p>The IDF document to extract special days from.</p> required <code>year</code> <code>int</code> <p>Year to use for date calculations.</p> required <p>Returns:</p> Type Description <code>list[SpecialDay]</code> <p>List of SpecialDay objects representing all special days.</p>"},{"location":"api/schedules/#data-classes","title":"Data Classes","text":""},{"location":"api/schedules/#idfkit.schedules.SpecialDay","title":"<code>idfkit.schedules.SpecialDay</code>  <code>dataclass</code>","text":"<p>A special day period from RunPeriodControl:SpecialDays.</p> <p>Represents a holiday or custom day type that spans one or more days.</p>"},{"location":"api/schedules/#idfkit.schedules.SpecialDay.day_type","title":"<code>day_type</code>  <code>instance-attribute</code>","text":"<p>Day type: \"Holiday\", \"CustomDay1\", or \"CustomDay2\".</p>"},{"location":"api/schedules/#idfkit.schedules.SpecialDay.duration","title":"<code>duration</code>  <code>instance-attribute</code>","text":"<p>Number of days this special day spans.</p>"},{"location":"api/schedules/#idfkit.schedules.SpecialDay.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>Name of the special day (e.g., \"Christmas\").</p>"},{"location":"api/schedules/#idfkit.schedules.SpecialDay.start_date","title":"<code>start_date</code>  <code>instance-attribute</code>","text":"<p>Start date of the special day period.</p>"},{"location":"api/schedules/#idfkit.schedules.SpecialDay.contains","title":"<code>contains(d)</code>","text":"<p>Check if a date falls within this special day period.</p>"},{"location":"api/schedules/#idfkit.schedules.TimeValue","title":"<code>idfkit.schedules.TimeValue</code>  <code>dataclass</code>","text":"<p>A time-value pair for interval schedules.</p> <p>The value applies from the previous time until this time.</p>"},{"location":"api/schedules/#idfkit.schedules.TimeValue.until_time","title":"<code>until_time</code>  <code>instance-attribute</code>","text":"<p>Time until which the value applies.</p>"},{"location":"api/schedules/#idfkit.schedules.TimeValue.value","title":"<code>value</code>  <code>instance-attribute</code>","text":"<p>The schedule value.</p>"},{"location":"api/schedules/#idfkit.schedules.CompactDayRule","title":"<code>idfkit.schedules.CompactDayRule</code>  <code>dataclass</code>","text":"<p>A 'For:' block in Schedule:Compact with day types and time-value pairs.</p>"},{"location":"api/schedules/#idfkit.schedules.CompactDayRule.day_types","title":"<code>day_types</code>  <code>instance-attribute</code>","text":"<p>Day types this rule applies to (e.g., {\"Weekdays\", \"Weekends\"}).</p>"},{"location":"api/schedules/#idfkit.schedules.CompactDayRule.time_values","title":"<code>time_values</code>  <code>instance-attribute</code>","text":"<p>Time-value pairs defining the schedule for these days.</p>"},{"location":"api/schedules/#idfkit.schedules.CompactPeriod","title":"<code>idfkit.schedules.CompactPeriod</code>  <code>dataclass</code>","text":"<p>A 'Through:' block in Schedule:Compact covering a date range.</p>"},{"location":"api/schedules/#idfkit.schedules.CompactPeriod.day_rules","title":"<code>day_rules</code>  <code>instance-attribute</code>","text":"<p>Day rules within this period.</p>"},{"location":"api/schedules/#idfkit.schedules.CompactPeriod.end_day","title":"<code>end_day</code>  <code>instance-attribute</code>","text":"<p>End day of the period (1-31).</p>"},{"location":"api/schedules/#idfkit.schedules.CompactPeriod.end_month","title":"<code>end_month</code>  <code>instance-attribute</code>","text":"<p>End month of the period (1-12).</p>"},{"location":"api/schedules/#idfkit.schedules.CompactPeriod.contains","title":"<code>contains(d)</code>","text":"<p>Check if a date falls within this period.</p> <p>Periods implicitly start from January 1 or the end of the previous period. This method only checks the end boundary.</p>"},{"location":"api/schedules/#file-support","title":"File Support","text":""},{"location":"api/schedules/#idfkit.schedules.ScheduleFileCache","title":"<code>idfkit.schedules.ScheduleFileCache</code>","text":"<p>Cache for Schedule:File CSV data.</p> <p>Caches parsed CSV data to avoid repeated file I/O when evaluating the same schedule multiple times.</p>"},{"location":"api/schedules/#idfkit.schedules.ScheduleFileCache.__init__","title":"<code>__init__()</code>","text":"<p>Initialize an empty cache.</p>"},{"location":"api/schedules/#idfkit.schedules.ScheduleFileCache.clear","title":"<code>clear()</code>","text":"<p>Clear the cache.</p>"},{"location":"api/schedules/#idfkit.schedules.ScheduleFileCache.get_values","title":"<code>get_values(obj, fs, base_path=None)</code>","text":"<p>Get cached values or read from file.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>IDFObject</code> <p>The Schedule:File object.</p> required <code>fs</code> <code>FileSystem</code> <p>FileSystem for reading the file.</p> required <code>base_path</code> <code>Path | str | None</code> <p>Base directory for resolving relative paths.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of schedule values from the file.</p>"},{"location":"api/schedules/#idfkit.schedules.ScheduleFileCache.invalidate","title":"<code>invalidate(path)</code>","text":"<p>Invalidate all cache entries whose key starts with path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>File path to invalidate.  All entries for this file (regardless of column / separator settings) are removed.</p> required"},{"location":"api/schedules/#exceptions","title":"Exceptions","text":""},{"location":"api/schedules/#idfkit.schedules.ScheduleEvaluationError","title":"<code>idfkit.schedules.ScheduleEvaluationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for schedule evaluation errors.</p>"},{"location":"api/schedules/#idfkit.schedules.UnsupportedScheduleType","title":"<code>idfkit.schedules.UnsupportedScheduleType</code>","text":"<p>               Bases: <code>ScheduleEvaluationError</code></p> <p>Raised when a schedule type is not supported.</p>"},{"location":"api/schedules/#idfkit.schedules.ScheduleReferenceError","title":"<code>idfkit.schedules.ScheduleReferenceError</code>","text":"<p>               Bases: <code>ScheduleEvaluationError</code></p> <p>Raised when a referenced schedule cannot be found.</p>"},{"location":"api/schedules/#idfkit.schedules.MalformedScheduleError","title":"<code>idfkit.schedules.MalformedScheduleError</code>","text":"<p>               Bases: <code>ScheduleEvaluationError</code></p> <p>Raised when a schedule has invalid syntax.</p>"},{"location":"api/simulation/","title":"Simulation API Overview","text":"<p>The simulation module provides EnergyPlus execution and result parsing.</p>"},{"location":"api/simulation/#quick-reference","title":"Quick Reference","text":"Function/Class Description <code>simulate()</code> Run a single simulation <code>simulate_batch()</code> Run multiple simulations in parallel <code>async_simulate()</code> Non-blocking single simulation <code>async_simulate_batch()</code> Non-blocking parallel simulations <code>async_simulate_batch_stream()</code> Streaming progress via async generator <code>SimulationEvent</code> Progress event from streaming batch <code>find_energyplus()</code> Discover EnergyPlus installation <code>expand_objects()</code> Expand <code>HVACTemplate:*</code> objects <code>run_slab_preprocessor()</code> Run the Slab ground heat-transfer preprocessor <code>run_basement_preprocessor()</code> Run the Basement ground heat-transfer preprocessor <code>run_preprocessing()</code> Run all needed preprocessors (combined pipeline) <code>needs_ground_heat_preprocessing()</code> Check if model needs GHT preprocessing <code>SimulationResult</code> Simulation result container <code>SimulationJob</code> Job specification for batch runs <code>BatchResult</code> Aggregated batch results <code>SQLResult</code> SQL database query interface <code>SimulationCache</code> Content-addressed result cache <code>FileSystem</code> Pluggable storage protocol <code>S3FileSystem</code> Amazon S3 storage backend"},{"location":"api/simulation/#module-contents","title":"Module Contents","text":"<p>EnergyPlus simulation execution and result handling.</p> <p>Provides subprocess-based simulation execution, EnergyPlus installation discovery, structured result containers, output parsing, and variable discovery.</p> Example <p>from idfkit import load_idf from idfkit.simulation import simulate, find_energyplus</p> <p>model = load_idf(\"building.idf\") result = simulate(model, \"weather.epw\") print(result.errors.summary())</p>"},{"location":"api/simulation/#idfkit.simulation--parser-coverage","title":"Parser Coverage","text":"<p>The module provides parsers for the most commonly used EnergyPlus output formats:</p> <ul> <li>SQLite (SQLResult): Time-series data, tabular reports, and metadata.   This is the recommended output format as it contains all simulation data in a   single queryable file.</li> <li>CSV (CSVResult): Time-series data in comma-separated format.</li> <li>HTML (HTMLResult): Tabular reports in HTML format.</li> <li>RDD/MDD (OutputVariableIndex): Available output variables and meters.</li> <li>ERR (ErrorReport): Errors, warnings, and simulation status.</li> </ul> <p>The following formats are intentionally not implemented as the SQLite output covers the same data more reliably and completely:</p> <ul> <li>ESO/MTR: Binary-text time-series format (use SQLite instead).</li> <li>EIO: Simulation metadata and invariant outputs (use SQLite instead).</li> </ul> <p>If you have a specific need for these formats, please open an issue describing your use case.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationEvent","title":"<code>SimulationEvent</code>  <code>dataclass</code>","text":"<p>Progress event emitted by async_simulate_batch_stream.</p> <p>Each event represents a single simulation that has finished (successfully or not).  Events are yielded in completion order, not submission order.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>Zero-based position of this job in the original jobs sequence.</p> <code>label</code> <code>str</code> <p>Human-readable label from the SimulationJob.</p> <code>result</code> <code>SimulationResult</code> <p>The simulation result.</p> <code>completed</code> <code>int</code> <p>Number of jobs completed so far (including this one).</p> <code>total</code> <code>int</code> <p>Total number of jobs in the batch.</p>"},{"location":"api/simulation/#idfkit.simulation.BatchResult","title":"<code>BatchResult</code>  <code>dataclass</code>","text":"<p>Aggregated results from a batch simulation run.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>tuple[SimulationResult, ...]</code> <p>Simulation results in the same order as the input jobs.</p> <code>total_runtime_seconds</code> <code>float</code> <p>Wall-clock time for the entire batch.</p>"},{"location":"api/simulation/#idfkit.simulation.BatchResult.succeeded","title":"<code>succeeded</code>  <code>property</code>","text":"<p>Results that completed successfully.</p>"},{"location":"api/simulation/#idfkit.simulation.BatchResult.failed","title":"<code>failed</code>  <code>property</code>","text":"<p>Results that failed.</p>"},{"location":"api/simulation/#idfkit.simulation.BatchResult.all_succeeded","title":"<code>all_succeeded</code>  <code>property</code>","text":"<p>Whether every job in the batch succeeded.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationJob","title":"<code>SimulationJob</code>  <code>dataclass</code>","text":"<p>Specification for a single simulation within a batch.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>object</code> <p>The EnergyPlus model to simulate.</p> <code>weather</code> <code>str | Path</code> <p>Path to the weather file.</p> <code>label</code> <code>str</code> <p>Human-readable label for progress reporting.</p> <code>output_dir</code> <code>str | Path | None</code> <p>Directory for output files (default: auto temp dir).</p> <code>expand_objects</code> <code>bool</code> <p>Run ExpandObjects before simulation.</p> <code>annual</code> <code>bool</code> <p>Run annual simulation.</p> <code>design_day</code> <code>bool</code> <p>Run design-day-only simulation.</p> <code>output_prefix</code> <code>str</code> <p>Prefix for output files.</p> <code>output_suffix</code> <code>Literal['C', 'L', 'D']</code> <p>Output file naming suffix (<code>\"C\"</code>, <code>\"L\"</code>, or <code>\"D\"</code>).</p> <code>readvars</code> <code>bool</code> <p>Run ReadVarsESO after simulation.</p> <code>timeout</code> <code>float</code> <p>Maximum runtime in seconds.</p> <code>extra_args</code> <code>tuple[str, ...] | None</code> <p>Additional command-line arguments.</p>"},{"location":"api/simulation/#idfkit.simulation.CacheKey","title":"<code>CacheKey</code>  <code>dataclass</code>","text":"<p>Opaque cache key wrapping a hex digest string.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationCache","title":"<code>SimulationCache</code>","text":"<p>Content-addressed simulation result cache.</p> <p>Each entry is a directory named by the cache key containing a full copy of the simulation run directory plus a <code>_cache_meta.json</code> manifest.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationCache.cache_dir","title":"<code>cache_dir</code>  <code>property</code>","text":"<p>Root directory for cached simulation entries.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationCache.compute_key","title":"<code>compute_key(model, weather, *, expand_objects=True, annual=False, design_day=False, output_suffix='C', readvars=False, extra_args=None)</code>","text":"<p>Compute a deterministic cache key for a simulation invocation.</p> <p>The model is copied and normalised (<code>Output:SQLite</code> is ensured) so that models differing only in the presence of that object produce the same key.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model.</p> required <code>weather</code> <code>str | Path</code> <p>Path to the weather file.</p> required <code>expand_objects</code> <code>bool</code> <p>Whether ExpandObjects will run.</p> <code>True</code> <code>annual</code> <code>bool</code> <p>Whether annual simulation is used.</p> <code>False</code> <code>design_day</code> <code>bool</code> <p>Whether design-day-only simulation is used.</p> <code>False</code> <code>output_suffix</code> <code>Literal['C', 'L', 'D']</code> <p>Output file naming suffix (<code>\"C\"</code>, <code>\"L\"</code>, or <code>\"D\"</code>).</p> <code>'C'</code> <code>readvars</code> <code>bool</code> <p>Whether ReadVarsESO post-processing will run.</p> <code>False</code> <code>extra_args</code> <code>list[str] | tuple[str, ...] | None</code> <p>Additional command-line arguments.</p> <code>None</code> <p>Returns:</p> Type Description <code>CacheKey</code> <p>A CacheKey for use with get / put.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationCache.get","title":"<code>get(key)</code>","text":"<p>Retrieve a cached simulation result.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>CacheKey</code> <p>Cache key from compute_key.</p> required <p>Returns:</p> Type Description <code>SimulationResult | None</code> <p>A SimulationResult if a cache hit exists, otherwise</p> <code>SimulationResult | None</code> <p><code>None</code>.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationCache.put","title":"<code>put(key, result)</code>","text":"<p>Store a successful simulation result in the cache.</p> <p>Only results with <code>success=True</code> are cached.  The entire run directory is copied into the cache atomically.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>CacheKey</code> <p>Cache key from compute_key.</p> required <code>result</code> <code>SimulationResult</code> <p>Successful simulation result to cache.</p> required"},{"location":"api/simulation/#idfkit.simulation.SimulationCache.contains","title":"<code>contains(key)</code>","text":"<p>Check whether a cache entry exists for key.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationCache.clear","title":"<code>clear()</code>","text":"<p>Remove all cached entries.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig","title":"<code>EnergyPlusConfig</code>  <code>dataclass</code>","text":"<p>Validated EnergyPlus installation configuration.</p> <p>Attributes:</p> Name Type Description <code>executable</code> <code>Path</code> <p>Path to the energyplus executable.</p> <code>version</code> <code>tuple[int, int, int]</code> <p>Parsed version as (major, minor, patch).</p> <code>install_dir</code> <code>Path</code> <p>Root installation directory.</p> <code>idd_path</code> <code>Path</code> <p>Path to the Energy+.idd file.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig.weather_dir","title":"<code>weather_dir</code>  <code>property</code>","text":"<p>Path to the bundled WeatherData directory, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig.schema_path","title":"<code>schema_path</code>  <code>property</code>","text":"<p>Path to Energy+.schema.epJSON, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig.expand_objects_exe","title":"<code>expand_objects_exe</code>  <code>property</code>","text":"<p>Path to ExpandObjects executable, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig.slab_exe","title":"<code>slab_exe</code>  <code>property</code>","text":"<p>Path to the Slab ground heat-transfer preprocessor, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig.slab_idd","title":"<code>slab_idd</code>  <code>property</code>","text":"<p>Path to SlabGHT.idd, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig.basement_exe","title":"<code>basement_exe</code>  <code>property</code>","text":"<p>Path to the Basement ground heat-transfer preprocessor, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig.basement_idd","title":"<code>basement_idd</code>  <code>property</code>","text":"<p>Path to BasementGHT.idd, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.EnergyPlusConfig.from_path","title":"<code>from_path(path)</code>  <code>classmethod</code>","text":"<p>Create config from an explicit installation path.</p> <p>The path can point to either the installation directory or the energyplus executable directly.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to EnergyPlus install directory or executable.</p> required <p>Returns:</p> Type Description <code>EnergyPlusConfig</code> <p>Validated EnergyPlusConfig.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If the path is not a valid installation.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem","title":"<code>AsyncFileSystem</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for async file system operations used by the async simulation module.</p> <p>This is the async counterpart to FileSystem.  Use this with async_simulate and the async batch functions to avoid blocking the event loop during file I/O \u2014 especially important for network-backed storage like S3.</p> <p>All methods accept <code>str | Path</code> for path arguments.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.read_bytes","title":"<code>read_bytes(path)</code>  <code>async</code>","text":"<p>Read a file as raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The file contents as bytes.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>  <code>async</code>","text":"<p>Write raw bytes to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>data</code> <code>bytes</code> <p>Bytes to write.</p> required"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>  <code>async</code>","text":"<p>Read a file as text.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>encoding</code> <code>str</code> <p>Text encoding (default <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>str</code> <p>The file contents as a string.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>  <code>async</code>","text":"<p>Write text to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>text</code> <code>str</code> <p>Text to write.</p> required <code>encoding</code> <code>str</code> <p>Text encoding (default <code>\"utf-8\"</code>).</p> <code>'utf-8'</code>"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.exists","title":"<code>exists(path)</code>  <code>async</code>","text":"<p>Check whether a file exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>  <code>async</code>","text":"<p>Create directories recursively.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Directory path to create.</p> required <code>exist_ok</code> <code>bool</code> <p>If True, do not raise if the directory already exists.</p> <code>False</code>"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.copy","title":"<code>copy(src, dst)</code>  <code>async</code>","text":"<p>Copy a file from src to dst.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str | Path</code> <p>Source file path.</p> required <code>dst</code> <code>str | Path</code> <p>Destination file path.</p> required"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.glob","title":"<code>glob(path, pattern)</code>  <code>async</code>","text":"<p>List files matching a glob pattern under path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Base directory.</p> required <code>pattern</code> <code>str</code> <p>Glob pattern (e.g. <code>\"*.sql\"</code>).</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of matching file paths as strings.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncFileSystem.remove","title":"<code>remove(path)</code>  <code>async</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file to remove.</p> required"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem","title":"<code>AsyncLocalFileSystem</code>","text":"<p>Non-blocking local file system using asyncio.to_thread.</p> <p>Wraps LocalFileSystem so that each blocking I/O call runs in the default executor, keeping the event loop free.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.read_bytes","title":"<code>read_bytes(path)</code>  <code>async</code>","text":"<p>Read a file as raw bytes without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>  <code>async</code>","text":"<p>Write raw bytes to a file without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>  <code>async</code>","text":"<p>Read a file as text without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>  <code>async</code>","text":"<p>Write text to a file without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.exists","title":"<code>exists(path)</code>  <code>async</code>","text":"<p>Check whether a file exists without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>  <code>async</code>","text":"<p>Create directories recursively without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.copy","title":"<code>copy(src, dst)</code>  <code>async</code>","text":"<p>Copy a file without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.glob","title":"<code>glob(path, pattern)</code>  <code>async</code>","text":"<p>List files matching a glob pattern without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncLocalFileSystem.remove","title":"<code>remove(path)</code>  <code>async</code>","text":"<p>Remove a file without blocking the event loop.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem","title":"<code>AsyncS3FileSystem</code>","text":"<p>Async file system implementation backed by Amazon S3 via <code>aiobotocore</code>.</p> <p>Requires the <code>aiobotocore</code> package (install via <code>pip install idfkit[async-s3]</code>).</p> <p>This is the non-blocking counterpart to S3FileSystem.  Use it with async_simulate and the async batch functions to avoid blocking the event loop during S3 I/O.</p> <p>The client must be initialised via the async context manager protocol:</p> <pre><code>```python\nasync with AsyncS3FileSystem(bucket=\"my-bucket\") as fs:\n    result = await async_simulate(model, weather, output_dir=\"run-001\", fs=fs)\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>prefix</code> <code>str</code> <p>Optional key prefix prepended to all paths. Use this to namespace simulations (e.g., <code>\"project-x/batch-42/\"</code>).</p> <code>''</code> <code>**boto_kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>session.create_client(\"s3\", ...)</code>. Common options include:</p> <ul> <li><code>region_name</code>: AWS region (e.g., <code>\"us-east-1\"</code>)</li> <li><code>endpoint_url</code>: Custom endpoint for S3-compatible services   (MinIO, LocalStack, etc.)</li> <li><code>aws_access_key_id</code>, <code>aws_secret_access_key</code>: Explicit   credentials (normally use IAM roles or environment variables)</li> </ul> <code>{}</code> <p>Examples:</p> <pre><code>from idfkit.simulation import AsyncS3FileSystem, async_simulate\n\nasync with AsyncS3FileSystem(bucket=\"my-bucket\", prefix=\"sims/\") as fs:\n    result = await async_simulate(model, weather, output_dir=\"run-001\", fs=fs)\n    errors = await result.async_errors()\n</code></pre>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.read_bytes","title":"<code>read_bytes(path)</code>  <code>async</code>","text":"<p>Read a file as raw bytes from S3.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>  <code>async</code>","text":"<p>Write raw bytes to S3.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>  <code>async</code>","text":"<p>Read a file as text from S3.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>  <code>async</code>","text":"<p>Write text to S3.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.exists","title":"<code>exists(path)</code>  <code>async</code>","text":"<p>Check whether an object exists in S3.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>  <code>async</code>","text":"<p>No-op \u2014 S3 has no directory concept.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.copy","title":"<code>copy(src, dst)</code>  <code>async</code>","text":"<p>Copy an object within the same bucket.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.glob","title":"<code>glob(path, pattern)</code>  <code>async</code>","text":"<p>List objects matching a glob pattern under path.</p> <p>Returns logical paths (without the configured S3 prefix) so that they can be passed back to other <code>AsyncS3FileSystem</code> methods which prepend the prefix automatically via <code>_key()</code>.</p>"},{"location":"api/simulation/#idfkit.simulation.AsyncS3FileSystem.remove","title":"<code>remove(path)</code>  <code>async</code>","text":"<p>Delete an object from S3.</p>"},{"location":"api/simulation/#idfkit.simulation.FileSystem","title":"<code>FileSystem</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for file system operations used by the simulation module.</p> <p>All methods accept <code>str | Path</code> for path arguments.</p>"},{"location":"api/simulation/#idfkit.simulation.FileSystem.read_bytes","title":"<code>read_bytes(path)</code>","text":"<p>Read a file as raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The file contents as bytes.</p>"},{"location":"api/simulation/#idfkit.simulation.FileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>","text":"<p>Write raw bytes to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>data</code> <code>bytes</code> <p>Bytes to write.</p> required"},{"location":"api/simulation/#idfkit.simulation.FileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>","text":"<p>Read a file as text.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>encoding</code> <code>str</code> <p>Text encoding (default <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>str</code> <p>The file contents as a string.</p>"},{"location":"api/simulation/#idfkit.simulation.FileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>","text":"<p>Write text to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>text</code> <code>str</code> <p>Text to write.</p> required <code>encoding</code> <code>str</code> <p>Text encoding (default <code>\"utf-8\"</code>).</p> <code>'utf-8'</code>"},{"location":"api/simulation/#idfkit.simulation.FileSystem.exists","title":"<code>exists(path)</code>","text":"<p>Check whether a file exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists.</p>"},{"location":"api/simulation/#idfkit.simulation.FileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>","text":"<p>Create directories recursively.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Directory path to create.</p> required <code>exist_ok</code> <code>bool</code> <p>If True, do not raise if the directory already exists.</p> <code>False</code>"},{"location":"api/simulation/#idfkit.simulation.FileSystem.copy","title":"<code>copy(src, dst)</code>","text":"<p>Copy a file from src to dst.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str | Path</code> <p>Source file path.</p> required <code>dst</code> <code>str | Path</code> <p>Destination file path.</p> required"},{"location":"api/simulation/#idfkit.simulation.FileSystem.glob","title":"<code>glob(path, pattern)</code>","text":"<p>List files matching a glob pattern under path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Base directory.</p> required <code>pattern</code> <code>str</code> <p>Glob pattern (e.g. <code>\"*.sql\"</code>).</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of matching file paths as strings.</p>"},{"location":"api/simulation/#idfkit.simulation.FileSystem.remove","title":"<code>remove(path)</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file to remove.</p> required"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem","title":"<code>LocalFileSystem</code>","text":"<p>File system implementation backed by pathlib and shutil.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.read_bytes","title":"<code>read_bytes(path)</code>","text":"<p>Read a file as raw bytes.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>","text":"<p>Write raw bytes to a file.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>","text":"<p>Read a file as text.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>","text":"<p>Write text to a file.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.exists","title":"<code>exists(path)</code>","text":"<p>Check whether a file exists.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>","text":"<p>Create directories recursively.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.copy","title":"<code>copy(src, dst)</code>","text":"<p>Copy a file from src to dst.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.glob","title":"<code>glob(path, pattern)</code>","text":"<p>List files matching a glob pattern under path.</p>"},{"location":"api/simulation/#idfkit.simulation.LocalFileSystem.remove","title":"<code>remove(path)</code>","text":"<p>Remove a file.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem","title":"<code>S3FileSystem</code>","text":"<p>File system implementation backed by Amazon S3.</p> <p>Requires the <code>boto3</code> package (install via <code>pip install idfkit[s3]</code>).</p> <p>This backend enables cloud-native simulation workflows where results are stored directly in S3 for later retrieval. EnergyPlus runs locally in a temporary directory, then results are uploaded to S3 after completion.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>prefix</code> <code>str</code> <p>Optional key prefix prepended to all paths. Use this to namespace simulations (e.g., <code>\"project-x/batch-42/\"</code>).</p> <code>''</code> <code>**boto_kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>boto3.client(\"s3\", ...)</code>. Common options include:</p> <ul> <li><code>region_name</code>: AWS region (e.g., <code>\"us-east-1\"</code>)</li> <li><code>endpoint_url</code>: Custom endpoint for S3-compatible services   (MinIO, LocalStack, etc.)</li> <li><code>aws_access_key_id</code>, <code>aws_secret_access_key</code>: Explicit   credentials (normally use IAM roles or environment variables)</li> </ul> <code>{}</code> <p>Examples:</p> <pre><code># Basic usage\nfs = S3FileSystem(bucket=\"my-bucket\", prefix=\"simulations/\")\n\n# With MinIO (S3-compatible)\nfs = S3FileSystem(\n    bucket=\"local-bucket\",\n    endpoint_url=\"http://localhost:9000\",\n    aws_access_key_id=\"minioadmin\",\n    aws_secret_access_key=\"minioadmin\",\n)\n\n# Use with simulate()\nresult = simulate(model, weather, output_dir=\"run-001\", fs=fs)\n</code></pre>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.read_bytes","title":"<code>read_bytes(path)</code>","text":"<p>Read a file as raw bytes from S3.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>","text":"<p>Write raw bytes to S3.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>","text":"<p>Read a file as text from S3.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>","text":"<p>Write text to S3.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.exists","title":"<code>exists(path)</code>","text":"<p>Check whether an object exists in S3.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>","text":"<p>No-op \u2014 S3 has no directory concept.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.copy","title":"<code>copy(src, dst)</code>","text":"<p>Copy an object within the same bucket.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.glob","title":"<code>glob(path, pattern)</code>","text":"<p>List objects matching a glob pattern under path.</p> <p>Returns logical paths (without the configured S3 prefix) so that they can be passed back to other <code>S3FileSystem</code> methods which prepend the prefix automatically via <code>_key()</code>.</p>"},{"location":"api/simulation/#idfkit.simulation.S3FileSystem.remove","title":"<code>remove(path)</code>","text":"<p>Delete an object from S3.</p>"},{"location":"api/simulation/#idfkit.simulation.OutputVariableIndex","title":"<code>OutputVariableIndex</code>  <code>dataclass</code>","text":"<p>Index of available output variables and meters for a model.</p> <p>Constructed from .rdd and .mdd files produced by EnergyPlus during a simulation run. Provides search, filtering, and model injection methods.</p> <p>Attributes:</p> Name Type Description <code>variables</code> <code>tuple[OutputVariable, ...]</code> <p>Available output variables from the .rdd file.</p> <code>meters</code> <code>tuple[OutputMeter, ...]</code> <p>Available output meters from the .mdd file.</p>"},{"location":"api/simulation/#idfkit.simulation.OutputVariableIndex.from_simulation","title":"<code>from_simulation(result)</code>  <code>classmethod</code>","text":"<p>Create an index from a completed simulation result.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>SimulationResult</code> <p>A SimulationResult with .rdd (and optionally .mdd) files.</p> required <p>Returns:</p> Type Description <code>OutputVariableIndex</code> <p>An OutputVariableIndex populated from the simulation output.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the .rdd file is not found.</p>"},{"location":"api/simulation/#idfkit.simulation.OutputVariableIndex.from_files","title":"<code>from_files(rdd_path, mdd_path=None)</code>  <code>classmethod</code>","text":"<p>Create an index from .rdd and .mdd file paths.</p> <p>Parameters:</p> Name Type Description Default <code>rdd_path</code> <code>str | Path</code> <p>Path to the .rdd file.</p> required <code>mdd_path</code> <code>str | Path | None</code> <p>Optional path to the .mdd file.</p> <code>None</code> <p>Returns:</p> Type Description <code>OutputVariableIndex</code> <p>An OutputVariableIndex populated from the files.</p>"},{"location":"api/simulation/#idfkit.simulation.OutputVariableIndex.search","title":"<code>search(pattern)</code>","text":"<p>Search variables and meters by name pattern.</p> <p>Uses case-insensitive regex matching against variable/meter names.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>A regex pattern to match against names.</p> required <p>Returns:</p> Type Description <code>list[OutputVariable | OutputMeter]</code> <p>List of matching OutputVariable and OutputMeter entries.</p>"},{"location":"api/simulation/#idfkit.simulation.OutputVariableIndex.filter_by_units","title":"<code>filter_by_units(units)</code>","text":"<p>Filter variables and meters by unit type.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>The unit string to filter by (case-insensitive).</p> required <p>Returns:</p> Type Description <code>list[OutputVariable | OutputMeter]</code> <p>List of matching OutputVariable and OutputMeter entries.</p>"},{"location":"api/simulation/#idfkit.simulation.OutputVariableIndex.add_all_to_model","title":"<code>add_all_to_model(model, *, frequency='Timestep', filter_pattern=None)</code>","text":"<p>Add output variables and meters to a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The IDFDocument to add outputs to.</p> required <code>frequency</code> <code>str</code> <p>The reporting frequency for all added outputs.</p> <code>'Timestep'</code> <code>filter_pattern</code> <code>str | None</code> <p>Optional regex pattern to filter which variables and meters are added (case-insensitive match on name).</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of output objects added.</p>"},{"location":"api/simulation/#idfkit.simulation.CSVColumn","title":"<code>CSVColumn</code>  <code>dataclass</code>","text":"<p>A single data column from a CSV output file.</p> <p>Attributes:</p> Name Type Description <code>header</code> <code>str</code> <p>The raw column header string.</p> <code>variable_name</code> <code>str</code> <p>Parsed variable name.</p> <code>key_value</code> <code>str</code> <p>Parsed key value (e.g. <code>\"Environment\"</code>).</p> <code>units</code> <code>str</code> <p>Parsed units string.</p> <code>values</code> <code>tuple[float, ...]</code> <p>The numeric values for this column.</p>"},{"location":"api/simulation/#idfkit.simulation.CSVResult","title":"<code>CSVResult</code>  <code>dataclass</code>","text":"<p>Parsed EnergyPlus CSV output file.</p> <p>Attributes:</p> Name Type Description <code>timestamps</code> <code>tuple[str, ...]</code> <p>The timestamp strings from the Date/Time column.</p> <code>columns</code> <code>tuple[CSVColumn, ...]</code> <p>Parsed data columns with extracted metadata.</p>"},{"location":"api/simulation/#idfkit.simulation.CSVResult.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Parse a CSV output file from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the CSV file.</p> required <p>Returns:</p> Type Description <code>CSVResult</code> <p>Parsed CSVResult.</p>"},{"location":"api/simulation/#idfkit.simulation.CSVResult.from_string","title":"<code>from_string(text)</code>  <code>classmethod</code>","text":"<p>Parse CSV output from a string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw CSV file contents.</p> required <p>Returns:</p> Type Description <code>CSVResult</code> <p>Parsed CSVResult.</p>"},{"location":"api/simulation/#idfkit.simulation.CSVResult.get_column","title":"<code>get_column(variable_name, key_value=None)</code>","text":"<p>Find a column by variable name and optional key value.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>The variable name to search for (case-insensitive).</p> required <code>key_value</code> <code>str | None</code> <p>Optional key value filter (case-insensitive).</p> <code>None</code> <p>Returns:</p> Type Description <code>CSVColumn | None</code> <p>The matching CSVColumn, or None if not found.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorMessage","title":"<code>ErrorMessage</code>  <code>dataclass</code>","text":"<p>A single error/warning message from EnergyPlus.</p> <p>Attributes:</p> Name Type Description <code>severity</code> <code>str</code> <p>One of \"Fatal\", \"Severe\", \"Warning\", \"Info\".</p> <code>message</code> <code>str</code> <p>The primary message text.</p> <code>details</code> <code>tuple[str, ...]</code> <p>Additional continuation lines (<code>** ~~~   **</code> lines).</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport","title":"<code>ErrorReport</code>  <code>dataclass</code>","text":"<p>Parsed contents of an EnergyPlus .err file.</p> <p>Attributes:</p> Name Type Description <code>fatal</code> <code>tuple[ErrorMessage, ...]</code> <p>Fatal error messages.</p> <code>severe</code> <code>tuple[ErrorMessage, ...]</code> <p>Severe error messages.</p> <code>warnings</code> <code>tuple[ErrorMessage, ...]</code> <p>Warning messages.</p> <code>info</code> <code>tuple[ErrorMessage, ...]</code> <p>Informational messages.</p> <code>warmup_converged</code> <code>bool</code> <p>Whether warmup convergence was reported.</p> <code>simulation_complete</code> <code>bool</code> <p>Whether the simulation completed successfully.</p> <code>raw_text</code> <code>str</code> <p>The original unparsed file text.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.has_fatal","title":"<code>has_fatal</code>  <code>property</code>","text":"<p>Whether any fatal errors were found.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.has_severe","title":"<code>has_severe</code>  <code>property</code>","text":"<p>Whether any severe errors were found.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.fatal_count","title":"<code>fatal_count</code>  <code>property</code>","text":"<p>Number of fatal errors.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.severe_count","title":"<code>severe_count</code>  <code>property</code>","text":"<p>Number of severe errors.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.error_count","title":"<code>error_count</code>  <code>property</code>","text":"<p>Total number of fatal + severe errors.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.warning_count","title":"<code>warning_count</code>  <code>property</code>","text":"<p>Total number of warnings.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.summary","title":"<code>summary()</code>","text":"<p>Return a human-readable summary of the error report.</p> <p>Returns:</p> Type Description <code>str</code> <p>A multi-line summary string.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Parse an .err file from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the .err file.</p> required <p>Returns:</p> Type Description <code>ErrorReport</code> <p>Parsed ErrorReport.</p>"},{"location":"api/simulation/#idfkit.simulation.ErrorReport.from_string","title":"<code>from_string(text)</code>  <code>classmethod</code>","text":"<p>Parse .err content from a string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw .err file contents.</p> required <p>Returns:</p> Type Description <code>ErrorReport</code> <p>Parsed ErrorReport.</p>"},{"location":"api/simulation/#idfkit.simulation.OutputMeter","title":"<code>OutputMeter</code>  <code>dataclass</code>","text":"<p>An available meter from a <code>.mdd</code> file.</p> <p>Meters aggregate energy or resource consumption and have no key value, unlike OutputVariable.  For post-simulation SQL results where variables and meters are stored together, see VariableInfo.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The meter name (e.g. <code>\"Electricity:Facility\"</code>).</p> <code>frequency</code> <code>str</code> <p>The default reporting frequency (e.g. <code>\"hourly\"</code>).</p> <code>units</code> <code>str</code> <p>The meter units (e.g. <code>\"J\"</code>).</p>"},{"location":"api/simulation/#idfkit.simulation.OutputVariable","title":"<code>OutputVariable</code>  <code>dataclass</code>","text":"<p>An available output variable from a <code>.rdd</code> file.</p> <p>Unlike meters, variables are associated with a specific key (zone, surface, etc.).  For post-simulation SQL results where variables and meters are stored together, see VariableInfo.</p> <p>Attributes:</p> Name Type Description <code>key</code> <code>str</code> <p>The key value (e.g. <code>\"*\"</code> or <code>\"ZONE 1\"</code>).</p> <code>name</code> <code>str</code> <p>The variable name (e.g. <code>\"Zone Mean Air Temperature\"</code>).</p> <code>frequency</code> <code>str</code> <p>The default reporting frequency (e.g. <code>\"hourly\"</code>).</p> <code>units</code> <code>str</code> <p>The variable units (e.g. <code>\"C\"</code>, <code>\"W\"</code>).</p>"},{"location":"api/simulation/#idfkit.simulation.EnvironmentInfo","title":"<code>EnvironmentInfo</code>  <code>dataclass</code>","text":"<p>Metadata about a simulation environment period.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>The environment period index in the database.</p> <code>name</code> <code>str</code> <p>The environment name (e.g. <code>\"RUN PERIOD 1\"</code>).</p> <code>environment_type</code> <code>int</code> <p>The type integer (1 = DesignDay, 2 = DesignRunPeriod, 3 = WeatherFileRunPeriod).</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult","title":"<code>SQLResult</code>","text":"<p>Query interface for an EnergyPlus SQLite output database.</p> <p>Opens the database in read-only mode and provides methods for retrieving time-series data, tabular reports, and variable metadata.</p> <p>Can be used as a context manager:</p> <pre><code>```python\nwith SQLResult(\"eplusout.sql\") as sql:\n    ts = sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\n```\n</code></pre>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.close","title":"<code>close()</code>","text":"<p>Close the database connection.</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.get_timeseries","title":"<code>get_timeseries(variable_name, key_value='*', frequency=None, environment=None)</code>","text":"<p>Retrieve a time series for a variable.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>The output variable name.</p> required <code>key_value</code> <code>str</code> <p>The key value (e.g. zone name). Use <code>\"*\"</code> for environment-level variables. Case-insensitive matching.</p> <code>'*'</code> <code>frequency</code> <code>str | None</code> <p>Optional frequency filter (e.g. <code>\"Hourly\"</code>).</p> <code>None</code> <code>environment</code> <code>Environment | None</code> <p>Filter by environment type. <code>None</code> (default) returns all data, <code>\"annual\"</code> returns only weather-file run period data, and <code>\"sizing\"</code> returns only design-day data.</p> <code>None</code> <p>Returns:</p> Type Description <code>TimeSeriesResult</code> <p>A TimeSeriesResult with timestamps and values.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the variable is not found in the database.</p> <code>ValueError</code> <p>If environment is not a recognized value.</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.get_tabular_data","title":"<code>get_tabular_data(report_name=None, table_name=None, row_name=None, column_name=None, report_for=None)</code>","text":"<p>Retrieve tabular report data.</p> <p>Parameters:</p> Name Type Description Default <code>report_name</code> <code>str | None</code> <p>Optional filter by report name.</p> <code>None</code> <code>table_name</code> <code>str | None</code> <p>Optional filter by table name.</p> <code>None</code> <code>row_name</code> <code>str | None</code> <p>Optional filter by row label.</p> <code>None</code> <code>column_name</code> <code>str | None</code> <p>Optional filter by column label.</p> <code>None</code> <code>report_for</code> <code>str | None</code> <p>Optional filter by report scope (e.g. <code>\"Entire Facility\"</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TabularRow]</code> <p>List of TabularRow entries matching the filters.</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.get_tabular_value","title":"<code>get_tabular_value(report_name, table_name, row_name, column_name, report_for='Entire Facility')</code>","text":"<p>Retrieve a single tabular cell value.</p> <p>Convenience wrapper around get_tabular_data that returns exactly one cell value.</p> <p>Parameters:</p> Name Type Description Default <code>report_name</code> <code>str</code> <p>Report name (e.g. <code>\"AnnualBuildingUtilityPerformanceSummary\"</code>).</p> required <code>table_name</code> <code>str</code> <p>Table name (e.g. <code>\"End Uses\"</code>).</p> required <code>row_name</code> <code>str</code> <p>Row label (e.g. <code>\"Heating\"</code>).</p> required <code>column_name</code> <code>str</code> <p>Column label (e.g. <code>\"Electricity\"</code>).</p> required <code>report_for</code> <code>str</code> <p>Report scope (default <code>\"Entire Facility\"</code>).</p> <code>'Entire Facility'</code> <p>Returns:</p> Type Description <code>str</code> <p>The cell value as a string.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no matching row is found or if multiple rows match.</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.list_variables","title":"<code>list_variables()</code>","text":"<p>List all available variables in the database.</p> <p>Returns:</p> Type Description <code>list[VariableInfo]</code> <p>List of VariableInfo entries describing each variable.</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.list_environments","title":"<code>list_environments()</code>","text":"<p>List all environment periods in the database.</p> <p>Returns:</p> Type Description <code>list[EnvironmentInfo]</code> <p>List of EnvironmentInfo entries describing each period (e.g.</p> <code>list[EnvironmentInfo]</code> <p>design days and run periods).</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.list_reports","title":"<code>list_reports()</code>","text":"<p>List all available tabular report names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of unique report names.</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.to_dataframe","title":"<code>to_dataframe(variable_name, key_value='*', frequency=None, environment=None)</code>","text":"<p>Retrieve a time series as a pandas DataFrame.</p> <p>This is a convenience wrapper around get_timeseries that directly returns a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>The output variable name.</p> required <code>key_value</code> <code>str</code> <p>The key value. Use <code>\"*\"</code> for environment-level variables.</p> <code>'*'</code> <code>frequency</code> <code>str | None</code> <p>Optional frequency filter.</p> <code>None</code> <code>environment</code> <code>Environment | None</code> <p>Filter by environment type (<code>None</code> by default for all data, <code>\"annual\"</code> for run periods, <code>\"sizing\"</code> for design days).</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A pandas DataFrame with a <code>timestamp</code> index.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pandas is not installed.</p> <code>KeyError</code> <p>If the variable is not found.</p>"},{"location":"api/simulation/#idfkit.simulation.SQLResult.query","title":"<code>query(sql, parameters=())</code>","text":"<p>Execute a raw SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL query string.</p> required <code>parameters</code> <code>tuple[object, ...]</code> <p>Query parameters for parameterized queries.</p> <code>()</code> <p>Returns:</p> Type Description <code>list[tuple[object, ...]]</code> <p>List of result tuples.</p>"},{"location":"api/simulation/#idfkit.simulation.TabularRow","title":"<code>TabularRow</code>  <code>dataclass</code>","text":"<p>A single row from an EnergyPlus tabular report.</p> <p>Attributes:</p> Name Type Description <code>report_name</code> <code>str</code> <p>The report name (e.g. <code>\"AnnualBuildingUtilityPerformanceSummary\"</code>).</p> <code>report_for</code> <code>str</code> <p>The report scope (e.g. <code>\"Entire Facility\"</code>).</p> <code>table_name</code> <code>str</code> <p>The table name within the report.</p> <code>row_name</code> <code>str</code> <p>The row label.</p> <code>column_name</code> <code>str</code> <p>The column label.</p> <code>units</code> <code>str</code> <p>The value units.</p> <code>value</code> <code>str</code> <p>The cell value as a string.</p>"},{"location":"api/simulation/#idfkit.simulation.TimeSeriesResult","title":"<code>TimeSeriesResult</code>  <code>dataclass</code>","text":"<p>A single time series extracted from an EnergyPlus SQL database.</p> <p>Attributes:</p> Name Type Description <code>variable_name</code> <code>str</code> <p>The output variable name.</p> <code>key_value</code> <code>str</code> <p>The key value (e.g. zone or surface name).</p> <code>units</code> <code>str</code> <p>The variable units.</p> <code>frequency</code> <code>str</code> <p>The reporting frequency.</p> <code>timestamps</code> <code>tuple[datetime, ...]</code> <p>Timestamp for each data point.</p> <code>values</code> <code>tuple[float, ...]</code> <p>Numeric values for each data point.</p>"},{"location":"api/simulation/#idfkit.simulation.TimeSeriesResult.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to a pandas DataFrame.</p> <p>Requires pandas to be installed.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A DataFrame with a <code>timestamp</code> index and a column for the values.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pandas is not installed.</p>"},{"location":"api/simulation/#idfkit.simulation.TimeSeriesResult.plot","title":"<code>plot(*, backend=None, title=None)</code>","text":"<p>Plot this time series as a line chart.</p> <p>Auto-detects the plotting backend if not provided. Requires matplotlib or plotly to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Any</code> <p>A PlotBackend instance. If not provided, auto-detects.</p> <code>None</code> <code>title</code> <code>str | None</code> <p>Optional plot title. Defaults to <code>\"key_value: variable_name\"</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object from the backend.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If no plotting backend is available.</p>"},{"location":"api/simulation/#idfkit.simulation.VariableInfo","title":"<code>VariableInfo</code>  <code>dataclass</code>","text":"<p>Metadata about an available variable or meter in the SQL database.</p> <p>This class represents both regular variables and meters because EnergyPlus stores them in a single <code>ReportDataDictionary</code> table, distinguished only by an <code>IsMeter</code> column.  Use the is_meter flag to tell them apart.</p> <p>For pre-simulation discovery from <code>.rdd</code> / <code>.mdd</code> files, see the separate OutputVariable and OutputMeter classes instead.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The variable name.</p> <code>key_value</code> <code>str</code> <p>The key value.  Empty for meters.</p> <code>frequency</code> <code>str</code> <p>The reporting frequency.</p> <code>units</code> <code>str</code> <p>The variable units.</p> <code>is_meter</code> <code>bool</code> <p>Whether this is a meter (vs. a regular variable).</p> <code>variable_type</code> <code>str</code> <p>The variable type string (e.g. <code>\"Zone\"</code>, <code>\"HVAC\"</code>).</p>"},{"location":"api/simulation/#idfkit.simulation.PlotBackend","title":"<code>PlotBackend</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for plotting backends used by the simulation module.</p> <p>Implementations must provide methods for common chart types. Each method returns a figure object native to the backend (e.g. matplotlib <code>Figure</code> or plotly <code>Figure</code>).</p>"},{"location":"api/simulation/#idfkit.simulation.PlotBackend.line","title":"<code>line(x, y, *, title=None, xlabel=None, ylabel=None, label=None)</code>","text":"<p>Create a single line plot.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[Any]</code> <p>X-axis values (e.g. timestamps).</p> required <code>y</code> <code>Sequence[float]</code> <p>Y-axis values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <code>label</code> <code>str | None</code> <p>Optional line label for legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p>"},{"location":"api/simulation/#idfkit.simulation.PlotBackend.multi_line","title":"<code>multi_line(x, y_series, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a multi-line plot with legend.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[Any]</code> <p>Shared X-axis values.</p> required <code>y_series</code> <code>dict[str, Sequence[float]]</code> <p>Mapping of label to Y values for each line.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p>"},{"location":"api/simulation/#idfkit.simulation.PlotBackend.heatmap","title":"<code>heatmap(data, *, x_labels=None, y_labels=None, title=None, colorbar_label=None)</code>","text":"<p>Create a 2D heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[Sequence[float]]</code> <p>2D array of values (rows, columns).</p> required <code>x_labels</code> <code>Sequence[str] | None</code> <p>Optional labels for columns.</p> <code>None</code> <code>y_labels</code> <code>Sequence[str] | None</code> <p>Optional labels for rows.</p> <code>None</code> <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>colorbar_label</code> <code>str | None</code> <p>Optional label for the colorbar.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p>"},{"location":"api/simulation/#idfkit.simulation.PlotBackend.bar","title":"<code>bar(categories, values, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>Sequence[str]</code> <p>Category labels for each bar.</p> required <code>values</code> <code>Sequence[float]</code> <p>Values for each bar.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p>"},{"location":"api/simulation/#idfkit.simulation.PlotBackend.stacked_bar","title":"<code>stacked_bar(categories, series, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a stacked bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>Sequence[str]</code> <p>Category labels for each bar group.</p> required <code>series</code> <code>dict[str, Sequence[float]]</code> <p>Mapping of series label to values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p>"},{"location":"api/simulation/#idfkit.simulation.ProgressParser","title":"<code>ProgressParser</code>","text":"<p>Parse EnergyPlus stdout lines into SimulationProgress events.</p> <p>Maintains internal state to track the current environment, warmup iteration count, and simulation day for percentage estimation.</p> <p>A new instance should be created for each simulation run.  The parser is designed to be defensive \u2014 unrecognised lines return <code>None</code> and never raise.</p> <p>Examples:</p> <pre><code>parser = ProgressParser()\nfor line in energyplus_stdout_lines:\n    event = parser.parse_line(line)\n    if event is not None:\n        print(event.phase, event.percent)\n</code></pre>"},{"location":"api/simulation/#idfkit.simulation.ProgressParser.set_job_context","title":"<code>set_job_context(index, label)</code>","text":"<p>Set batch job context that will be included in all emitted events.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Job index within the batch.</p> required <code>label</code> <code>str</code> <p>Human-readable job label.</p> required"},{"location":"api/simulation/#idfkit.simulation.ProgressParser.parse_line","title":"<code>parse_line(line)</code>","text":"<p>Parse a single stdout line into a progress event.</p> <p>Parameters:</p> Name Type Description Default <code>line</code> <code>str</code> <p>A single line from EnergyPlus stdout.</p> required <p>Returns:</p> Type Description <code>SimulationProgress | None</code> <p>A SimulationProgress event, or <code>None</code> if the line</p> <code>SimulationProgress | None</code> <p>does not contain progress information.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationProgress","title":"<code>SimulationProgress</code>  <code>dataclass</code>","text":"<p>Progress event emitted during a single EnergyPlus simulation.</p> <p>This dataclass represents a progress update parsed from EnergyPlus stdout output.  It is passed to user-supplied <code>on_progress</code> callbacks on simulate and async_simulate.</p> <p>Attributes:</p> Name Type Description <code>phase</code> <code>Literal['preprocessing', 'initializing', 'warmup', 'simulating', 'postprocessing', 'complete']</code> <p>Current simulation phase.</p> <code>message</code> <code>str</code> <p>Raw EnergyPlus stdout line (stripped).</p> <code>percent</code> <code>float | None</code> <p>Estimated completion percentage (0.0-100.0), or <code>None</code> when progress is indeterminate (e.g. during warmup).</p> <code>environment</code> <code>str | None</code> <p>Name of the current simulation environment, if known.</p> <code>warmup_day</code> <code>int | None</code> <p>Current warmup iteration (1-based), only set during the <code>\"warmup\"</code> phase.</p> <code>sim_day</code> <code>int | None</code> <p>Current simulation day-of-year (1-based), only set during the <code>\"simulating\"</code> phase.</p> <code>sim_total_days</code> <code>int | None</code> <p>Total number of simulation days, only set when the simulation period is known.</p> <code>job_index</code> <code>int | None</code> <p>Index of this job in a batch, or <code>None</code> for single simulations.</p> <code>job_label</code> <code>str | None</code> <p>Label of this job in a batch, or <code>None</code> for single simulations.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult","title":"<code>SimulationResult</code>  <code>dataclass</code>","text":"<p>Result of an EnergyPlus simulation run.</p> <p>Attributes:</p> Name Type Description <code>run_dir</code> <code>Path</code> <p>Directory containing all simulation output.</p> <code>success</code> <code>bool</code> <p>Whether the simulation exited successfully.</p> <code>exit_code</code> <code>int | None</code> <p>Process exit code (None if timed out).</p> <code>stdout</code> <code>str</code> <p>Captured standard output.</p> <code>stderr</code> <code>str</code> <p>Captured standard error.</p> <code>runtime_seconds</code> <code>float</code> <p>Wall-clock execution time in seconds.</p> <code>output_prefix</code> <code>str</code> <p>Output file prefix (default \"eplus\").</p> <code>fs</code> <code>FileSystem | None</code> <p>Optional sync file system backend for reading output files.</p> <code>async_fs</code> <code>AsyncFileSystem | None</code> <p>Optional async file system backend for non-blocking reads. Set automatically by async_simulate when an AsyncFileSystem is provided.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.errors","title":"<code>errors</code>  <code>property</code>","text":"<p>Parsed error report from the .err file (lazily cached).</p> <p>Returns:</p> Type Description <code>ErrorReport</code> <p>Parsed ErrorReport from the simulation's .err output.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.sql","title":"<code>sql</code>  <code>property</code>","text":"<p>Parsed SQL output database (lazily cached).</p> <p>Returns:</p> Type Description <code>SQLResult | None</code> <p>An SQLResult for querying time-series and tabular data,</p> <code>SQLResult | None</code> <p>or None if no .sql file was produced.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.variables","title":"<code>variables</code>  <code>property</code>","text":"<p>Output variable/meter index from .rdd/.mdd files (lazily cached).</p> <p>Returns:</p> Type Description <code>OutputVariableIndex | None</code> <p>An OutputVariableIndex for searching and injecting output</p> <code>OutputVariableIndex | None</code> <p>variables, or None if no .rdd file was produced.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.csv","title":"<code>csv</code>  <code>property</code>","text":"<p>Parsed CSV output (lazily cached).</p> <p>Returns:</p> Type Description <code>CSVResult | None</code> <p>A CSVResult with extracted column metadata and values,</p> <code>CSVResult | None</code> <p>or None if no .csv file was produced.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.html","title":"<code>html</code>  <code>property</code>","text":"<p>Parsed HTML tabular output (lazily cached).</p> <p>Returns:</p> Type Description <code>HTMLResult | None</code> <p>An HTMLResult with extracted tables and titles,</p> <code>HTMLResult | None</code> <p>or None if no HTML file was produced.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.sql_path","title":"<code>sql_path</code>  <code>property</code>","text":"<p>Path to the SQLite output file, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.err_path","title":"<code>err_path</code>  <code>property</code>","text":"<p>Path to the .err output file, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.eso_path","title":"<code>eso_path</code>  <code>property</code>","text":"<p>Path to the .eso output file, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.csv_path","title":"<code>csv_path</code>  <code>property</code>","text":"<p>Path to the .csv output file, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.html_path","title":"<code>html_path</code>  <code>property</code>","text":"<p>Path to the HTML tabular output file, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.rdd_path","title":"<code>rdd_path</code>  <code>property</code>","text":"<p>Path to the .rdd output file, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.mdd_path","title":"<code>mdd_path</code>  <code>property</code>","text":"<p>Path to the .mdd output file, if present.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.async_errors","title":"<code>async_errors()</code>  <code>async</code>","text":"<p>Parsed error report from the .err file (async, lazily cached).</p> <p>Non-blocking counterpart to errors that uses async_fs for file reads.</p> <p>Returns:</p> Type Description <code>ErrorReport</code> <p>Parsed ErrorReport from the simulation's .err output.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.async_sql","title":"<code>async_sql()</code>  <code>async</code>","text":"<p>Parsed SQL output database (async, lazily cached).</p> <p>Non-blocking counterpart to sql that uses async_fs for file reads.</p> <p>Returns:</p> Type Description <code>SQLResult | None</code> <p>An SQLResult for querying time-series and tabular data,</p> <code>SQLResult | None</code> <p>or None if no .sql file was produced.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.async_variables","title":"<code>async_variables()</code>  <code>async</code>","text":"<p>Output variable/meter index (async, lazily cached).</p> <p>Non-blocking counterpart to variables that uses async_fs for file reads.</p> <p>Returns:</p> Type Description <code>OutputVariableIndex | None</code> <p>An OutputVariableIndex for searching and injecting output</p> <code>OutputVariableIndex | None</code> <p>variables, or None if no .rdd file was produced.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.async_csv","title":"<code>async_csv()</code>  <code>async</code>","text":"<p>Parsed CSV output (async, lazily cached).</p> <p>Non-blocking counterpart to csv that uses async_fs for file reads.</p> <p>Returns:</p> Type Description <code>CSVResult | None</code> <p>A CSVResult with extracted column metadata and values,</p> <code>CSVResult | None</code> <p>or None if no .csv file was produced.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.async_html","title":"<code>async_html()</code>  <code>async</code>","text":"<p>Parsed HTML tabular output (async, lazily cached).</p> <p>Non-blocking counterpart to html that uses async_fs for file reads.</p> <p>Returns:</p> Type Description <code>HTMLResult | None</code> <p>An HTMLResult with extracted tables and titles,</p> <code>HTMLResult | None</code> <p>or None if no HTML file was produced.</p>"},{"location":"api/simulation/#idfkit.simulation.SimulationResult.from_directory","title":"<code>from_directory(path, *, output_prefix='eplus', fs=None, async_fs=None)</code>  <code>classmethod</code>","text":"<p>Reconstruct a SimulationResult from an existing output directory.</p> <p>Useful for inspecting results from a previous simulation run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the simulation output directory.</p> required <code>output_prefix</code> <code>str</code> <p>Output file prefix used during the run.</p> <code>'eplus'</code> <code>fs</code> <code>FileSystem | None</code> <p>Optional sync file system backend for reading output files.</p> <code>None</code> <code>async_fs</code> <code>AsyncFileSystem | None</code> <p>Optional async file system backend for non-blocking reads.</p> <code>None</code> <p>Returns:</p> Type Description <code>SimulationResult</code> <p>SimulationResult pointing to the existing output.</p>"},{"location":"api/simulation/#idfkit.simulation.prep_outputs","title":"<code>prep_outputs(model)</code>","text":"<p>Add standard output objects to the model if not already present.</p> <p>Ensures the model includes:</p> <ul> <li><code>Output:SQLite</code> (SimpleAndTabular) \u2014 for SQL-based result queries</li> <li><code>Output:Table:SummaryReports</code> (AllSummary) \u2014 for tabular reports</li> <li><code>Output:VariableDictionary</code> (Regular) \u2014 for <code>.rdd</code> / <code>.mdd</code> generation</li> </ul> <p>This is a superset of <code>ensure_sql_output</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The model to modify in place.</p> required"},{"location":"api/simulation/#idfkit.simulation.async_simulate_batch","title":"<code>async_simulate_batch(jobs, *, energyplus=None, max_concurrent=None, cache=None, fs=None, on_progress=None)</code>  <code>async</code>","text":"<p>Run multiple EnergyPlus simulations concurrently using asyncio.</p> <p>This is the async counterpart to simulate_batch.  Concurrency is controlled with an asyncio.Semaphore instead of a thread pool.</p> <p>Individual job failures are captured as failed SimulationResult entries -- the batch never raises due to a single job failing.</p> <p>Parameters:</p> Name Type Description Default <code>jobs</code> <code>Sequence[SimulationJob]</code> <p>Sequence of simulation jobs to execute.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Shared EnergyPlus configuration (auto-discovered if <code>None</code>).</p> <code>None</code> <code>max_concurrent</code> <code>int | None</code> <p>Maximum number of concurrent simulations.  Defaults to <code>min(len(jobs), os.cpu_count() or 1)</code>.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>fs</code> <code>FileSystem | AsyncFileSystem | None</code> <p>Optional file system backend passed through to each async_simulate call.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], Any] | None</code> <p>Optional callback invoked with SimulationProgress events during each individual simulation.  Events include <code>job_index</code> and <code>job_label</code> to identify which batch job they belong to.  Both sync and async callables are accepted. The <code>\"tqdm\"</code> shorthand is not supported for batch runners; use tqdm_progress with a custom per-job callback instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>BatchResult</code> <p>A BatchResult with results in the</p> <code>BatchResult</code> <p>same order as jobs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If jobs is empty.</p>"},{"location":"api/simulation/#idfkit.simulation.async_simulate_batch_stream","title":"<code>async_simulate_batch_stream(jobs, *, energyplus=None, max_concurrent=None, cache=None, fs=None, on_progress=None)</code>  <code>async</code>","text":"<p>Run simulations concurrently, yielding events as each one completes.</p> <p>This is an async generator variant of async_simulate_batch that yields SimulationEvent objects in completion order.  This enables real-time progress reporting without needing a callback:</p> <p>.. code-block:: python</p> <pre><code>async for event in async_simulate_batch_stream(jobs, max_concurrent=4):\n    print(f\"[{event.completed}/{event.total}] {event.label}\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>jobs</code> <code>Sequence[SimulationJob]</code> <p>Sequence of simulation jobs to execute.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Shared EnergyPlus configuration (auto-discovered if <code>None</code>).</p> <code>None</code> <code>max_concurrent</code> <code>int | None</code> <p>Maximum number of concurrent simulations.  Defaults to <code>min(len(jobs), os.cpu_count() or 1)</code>.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>fs</code> <code>FileSystem | AsyncFileSystem | None</code> <p>Optional file system backend.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], Any] | None</code> <p>Optional callback invoked with SimulationProgress events during each individual simulation.  Events include <code>job_index</code> and <code>job_label</code>.  The <code>\"tqdm\"</code> shorthand is not supported for batch runners; use tqdm_progress with a custom per-job callback instead.</p> <code>None</code> <p>Yields:</p> Type Description <code>AsyncIterator[SimulationEvent]</code> <p>SimulationEvent for each completed simulation, in the order</p> <code>AsyncIterator[SimulationEvent]</code> <p>they finish.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If jobs is empty.</p>"},{"location":"api/simulation/#idfkit.simulation.async_simulate","title":"<code>async_simulate(model, weather, *, output_dir=None, energyplus=None, expand_objects=True, annual=False, design_day=False, output_prefix='eplus', output_suffix='C', readvars=False, timeout=3600.0, extra_args=None, cache=None, fs=None, on_progress=None)</code>  <code>async</code>","text":"<p>Run an EnergyPlus simulation without blocking the event loop.</p> <p>This is the async counterpart to simulate. All parameters and return values are identical; the only difference is that EnergyPlus runs as an asyncio subprocess, allowing the caller to <code>await</code> the result while other coroutines continue executing.</p> <p>Cancellation is supported: if the wrapping asyncio.Task is cancelled, the EnergyPlus subprocess is killed and cleaned up.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model to simulate.</p> required <code>weather</code> <code>str | Path</code> <p>Path to the weather file (.epw).</p> required <code>output_dir</code> <code>str | Path | None</code> <p>Directory for output files (default: auto temp dir).</p> <code>None</code> <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation. If None, uses find_energyplus for auto-discovery.</p> <code>None</code> <code>expand_objects</code> <code>bool</code> <p>Run ExpandObjects before simulation.  When <code>True</code>, also runs the Slab and Basement ground heat-transfer preprocessors if the model contains the corresponding objects.</p> <code>True</code> <code>annual</code> <code>bool</code> <p>Run annual simulation (<code>-a</code> flag).</p> <code>False</code> <code>design_day</code> <code>bool</code> <p>Run design-day-only simulation (<code>-D</code> flag).</p> <code>False</code> <code>output_prefix</code> <code>str</code> <p>Prefix for output files (default \"eplus\").</p> <code>'eplus'</code> <code>output_suffix</code> <code>Literal['C', 'L', 'D']</code> <p>Output file naming suffix: <code>\"C\"</code> for combined table files (default), <code>\"L\"</code> for legacy separate table files, or <code>\"D\"</code> for timestamped separate files.</p> <code>'C'</code> <code>readvars</code> <code>bool</code> <p>Run ReadVarsESO after simulation (<code>-r</code> flag).</p> <code>False</code> <code>timeout</code> <code>float</code> <p>Maximum runtime in seconds (default 3600).</p> <code>3600.0</code> <code>extra_args</code> <code>list[str] | None</code> <p>Additional command-line arguments.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>fs</code> <code>FileSystem | AsyncFileSystem | None</code> <p>Optional file system backend for storing results on remote storage (e.g., S3).  Both sync FileSystem and async AsyncFileSystem are accepted. When an <code>AsyncFileSystem</code> is provided, uploads and result reads are truly non-blocking.  A sync <code>FileSystem</code> is automatically wrapped in asyncio.to_thread to avoid blocking the event loop.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], Any] | Literal['tqdm'] | None</code> <p>Optional callback invoked with a SimulationProgress event each time EnergyPlus emits a progress line.  Both synchronous and async callables are accepted -- async callables are awaited. Pass <code>\"tqdm\"</code> to use a built-in tqdm progress bar (requires <code>pip install idfkit[progress]</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>SimulationResult</code> <p>SimulationResult with paths to output files.</p> <p>Raises:</p> Type Description <code>SimulationError</code> <p>On timeout, OS error, or missing weather file.</p> <code>ExpandObjectsError</code> <p>If a preprocessing step fails.</p> <code>EnergyPlusNotFoundError</code> <p>If EnergyPlus cannot be found.</p>"},{"location":"api/simulation/#idfkit.simulation.simulate_batch","title":"<code>simulate_batch(jobs, *, energyplus=None, max_workers=None, cache=None, progress=None, fs=None, on_progress=None)</code>","text":"<p>Run multiple EnergyPlus simulations in parallel.</p> <p>Uses ThreadPoolExecutor to dispatch simulations concurrently.  Individual job failures are captured as failed SimulationResult entries -- the batch never raises due to a single job failing.</p> <p>Parameters:</p> Name Type Description Default <code>jobs</code> <code>Sequence[SimulationJob]</code> <p>Sequence of simulation jobs to execute.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Shared EnergyPlus configuration (auto-discovered if <code>None</code>).</p> <code>None</code> <code>max_workers</code> <code>int | None</code> <p>Maximum number of concurrent simulations.  Defaults to <code>min(len(jobs), os.cpu_count() or 1)</code>.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>progress</code> <code>Callable[..., None] | None</code> <p>Optional callback invoked after each job completes. Called as <code>progress(completed=N, total=M, label=label, success=bool)</code>.</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>Optional file system backend passed through to each simulate call.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], None] | None</code> <p>Optional callback invoked with SimulationProgress events during each individual simulation.  Events include <code>job_index</code> and <code>job_label</code> to identify which batch job they belong to.  The <code>\"tqdm\"</code> shorthand is not supported for batch runners; use tqdm_progress with a custom per-job callback instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>BatchResult</code> <p>A BatchResult with results in the same order as jobs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If jobs is empty.</p>"},{"location":"api/simulation/#idfkit.simulation.find_energyplus","title":"<code>find_energyplus(*, version=None, path=None)</code>","text":"<p>Find an EnergyPlus installation.</p> Discovery order <ol> <li>Explicit path argument.</li> <li><code>ENERGYPLUS_DIR</code> environment variable.</li> <li><code>energyplus</code> on <code>PATH</code> (via shutil.which).</li> <li>Platform-specific default directories (newest version first).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>tuple[int, int, int] | str | None</code> <p>Optional version filter. Accepts <code>(major, minor, patch)</code> tuple or a string like <code>\"24.1.0\"</code> or <code>\"24.1\"</code>.</p> <code>None</code> <code>path</code> <code>str | Path | None</code> <p>Explicit path to EnergyPlus install directory or executable.</p> <code>None</code> <p>Returns:</p> Type Description <code>EnergyPlusConfig</code> <p>Validated EnergyPlusConfig.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no matching installation is found.</p>"},{"location":"api/simulation/#idfkit.simulation.expand_objects","title":"<code>expand_objects(model, *, energyplus=None, timeout=120.0)</code>","text":"<p>Run the EnergyPlus ExpandObjects preprocessor and return the expanded document.</p> <p><code>ExpandObjects</code> replaces <code>HVACTemplate:*</code> objects with their fully specified low-level HVAC equivalents.  The original model is not mutated.</p> <p>If the document contains no <code>HVACTemplate:*</code> objects a copy is returned immediately without invoking the preprocessor (no EnergyPlus installation required).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model to expand.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, find_energyplus is used for auto-discovery.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for the preprocessor (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument containing the expanded</p> <code>IDFDocument</code> <p>objects.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation (and therefore no <code>ExpandObjects</code> executable) can be found.</p> <code>ExpandObjectsError</code> <p>If the <code>ExpandObjects</code> executable is missing from the installation or the preprocessor exits with an error.</p>"},{"location":"api/simulation/#idfkit.simulation.needs_ground_heat_preprocessing","title":"<code>needs_ground_heat_preprocessing(model)</code>","text":"<p>Return <code>True</code> if model contains ground heat-transfer objects.</p> <p>Checks for <code>GroundHeatTransfer:Slab:*</code> and <code>GroundHeatTransfer:Basement:*</code> objects that require the Slab or Basement preprocessor before simulation.</p> <p>This is used by simulate to decide whether to auto-run the preprocessing pipeline.</p>"},{"location":"api/simulation/#idfkit.simulation.run_basement_preprocessor","title":"<code>run_basement_preprocessor(model, *, energyplus=None, weather=None, timeout=120.0)</code>","text":"<p>Run the Basement ground heat-transfer preprocessor and return the expanded document.</p> <p>The workflow is:</p> <ol> <li><code>ExpandObjects</code> extracts <code>GroundHeatTransfer:Basement:*</code> objects    from the model into <code>BasementGHTIn.idf</code>.</li> <li>The Basement preprocessor reads <code>BasementGHTIn.idf</code> and computes    ground temperatures, writing <code>EPObjects.TXT</code>.</li> <li>The resulting boundary conditions are appended to the expanded IDF.</li> </ol> <p>The original model is not mutated.</p> <p>If the document contains no <code>GroundHeatTransfer:Basement:*</code> objects a copy is returned immediately.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model containing <code>GroundHeatTransfer:Basement:*</code> objects.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, auto-discovery is used.</p> <code>None</code> <code>weather</code> <code>str | Path | None</code> <p>Path to a weather file (<code>.epw</code>).  The Basement preprocessor requires weather data to compute ground temperatures.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds for each subprocess invocation (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument with basement ground</p> <code>IDFDocument</code> <p>temperatures appended.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation is found.</p> <code>ExpandObjectsError</code> <p>If any preprocessor step fails.</p>"},{"location":"api/simulation/#idfkit.simulation.run_preprocessing","title":"<code>run_preprocessing(model, *, energyplus=None, weather=None, timeout=120.0)</code>","text":"<p>Run ExpandObjects and any required ground heat-transfer preprocessors.</p> <p>This is a convenience function that runs all needed preprocessors in a single call.  It runs ExpandObjects once, then checks which preprocessor input files were produced (<code>GHTIn.idf</code> and/or <code>BasementGHTIn.idf</code>) and runs the corresponding Fortran solvers.</p> <p>simulate calls this automatically when the model contains ground heat-transfer objects and expand_objects is <code>True</code>.  Call it directly only when you need to inspect or modify the preprocessed model before simulation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model to preprocess.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, auto-discovery is used.</p> <code>None</code> <code>weather</code> <code>str | Path | None</code> <p>Path to a weather file (<code>.epw</code>).  Required by the Slab and Basement solvers.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds for each subprocess invocation (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument with all</p> <code>IDFDocument</code> <p>preprocessing applied.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation is found.</p> <code>ExpandObjectsError</code> <p>If any preprocessor step fails.</p>"},{"location":"api/simulation/#idfkit.simulation.run_slab_preprocessor","title":"<code>run_slab_preprocessor(model, *, energyplus=None, weather=None, timeout=120.0)</code>","text":"<p>Run the Slab ground heat-transfer preprocessor and return the expanded document.</p> <p>The workflow is:</p> <ol> <li><code>ExpandObjects</code> extracts <code>GroundHeatTransfer:Slab:*</code> objects from    the model into <code>GHTIn.idf</code>.</li> <li>The Slab preprocessor reads <code>GHTIn.idf</code> and computes monthly    ground surface temperatures, writing <code>SLABSurfaceTemps.TXT</code>.</li> <li>The resulting temperature schedules are appended to the expanded IDF.</li> </ol> <p>The original model is not mutated.</p> <p>If the document contains no <code>GroundHeatTransfer:Slab:*</code> objects a copy is returned immediately.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model containing <code>GroundHeatTransfer:Slab:*</code> objects.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, auto-discovery is used.</p> <code>None</code> <code>weather</code> <code>str | Path | None</code> <p>Path to a weather file (<code>.epw</code>).  Some Slab configurations may require weather data.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds for each subprocess invocation (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument with slab ground</p> <code>IDFDocument</code> <p>temperatures appended.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation is found.</p> <code>ExpandObjectsError</code> <p>If any preprocessor step fails.</p>"},{"location":"api/simulation/#idfkit.simulation.get_default_backend","title":"<code>get_default_backend()</code>","text":"<p>Auto-detect and return an available plotting backend.</p> <p>Tries matplotlib first, then plotly. Raises ImportError if neither is available.</p> <p>Returns:</p> Type Description <code>PlotBackend</code> <p>A PlotBackend instance.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If neither matplotlib nor plotly is installed.</p>"},{"location":"api/simulation/#idfkit.simulation.plot_comfort_hours","title":"<code>plot_comfort_hours(sql, zones, *, comfort_min=20.0, comfort_max=26.0, backend=None, title='Comfort Hours by Zone and Month')</code>","text":"<p>Create a heatmap of comfort hours by zone and month.</p> <p>For each zone, calculates the percentage of hours within the comfort range for each month and displays as a heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>SQLResult</code> <p>An open SQLResult database.</p> required <code>zones</code> <code>Sequence[str]</code> <p>Zone names to analyze.</p> required <code>comfort_min</code> <code>float</code> <p>Minimum comfort temperature (default 20C).</p> <code>20.0</code> <code>comfort_max</code> <code>float</code> <p>Maximum comfort temperature (default 26C).</p> <code>26.0</code> <code>backend</code> <code>PlotBackend | None</code> <p>Plotting backend to use. Auto-detects if not provided.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Comfort Hours by Zone and Month'</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object from the backend.</p>"},{"location":"api/simulation/#idfkit.simulation.plot_energy_balance","title":"<code>plot_energy_balance(sql, *, backend=None, title='End-Use Energy by Category')</code>","text":"<p>Create a bar chart of end-use energy consumption.</p> <p>Extracts data from the <code>AnnualBuildingUtilityPerformanceSummary</code> report and plots energy consumption by end-use category.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>SQLResult</code> <p>An open SQLResult database.</p> required <code>backend</code> <code>PlotBackend | None</code> <p>Plotting backend to use. Auto-detects if not provided.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'End-Use Energy by Category'</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object from the backend.</p>"},{"location":"api/simulation/#idfkit.simulation.plot_temperature_profile","title":"<code>plot_temperature_profile(sql, zones, *, backend=None, title='Zone Air Temperatures', frequency=None)</code>","text":"<p>Create a multi-line plot of zone air temperatures.</p> <p>Queries <code>Zone Mean Air Temperature</code> for each specified zone and plots them on a shared time axis.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>SQLResult</code> <p>An open SQLResult database.</p> required <code>zones</code> <code>Sequence[str]</code> <p>Zone names to plot.</p> required <code>backend</code> <code>PlotBackend | None</code> <p>Plotting backend to use. Auto-detects if not provided.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Zone Air Temperatures'</code> <code>frequency</code> <code>str | None</code> <p>Optional frequency filter (e.g. <code>\"Hourly\"</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object from the backend.</p>"},{"location":"api/simulation/#idfkit.simulation.tqdm_progress","title":"<code>tqdm_progress(*, desc='Simulating', bar_format='{l_bar}{bar}| {n:.0f}/{total_fmt}% [{elapsed}&lt;{remaining}]', leave=True, position=None, file=None, **tqdm_kwargs)</code>","text":"<p>Context manager that yields a tqdm-based <code>on_progress</code> callback.</p> <p>The progress bar is automatically closed when the context exits, even if an exception is raised.</p> <p>Parameters:</p> Name Type Description Default <code>desc</code> <code>str</code> <p>Progress bar description (left label).</p> <code>'Simulating'</code> <code>bar_format</code> <code>str</code> <p>tqdm bar format string.  The default shows percentage, elapsed and estimated remaining time.</p> <code>'{l_bar}{bar}| {n:.0f}/{total_fmt}% [{elapsed}&lt;{remaining}]'</code> <code>leave</code> <code>bool</code> <p>Whether the bar remains visible after completion.</p> <code>True</code> <code>position</code> <code>int | None</code> <p>Line position for the bar (useful for nested bars).</p> <code>None</code> <code>file</code> <code>Any</code> <p>Output stream (default: <code>sys.stderr</code>).</p> <code>None</code> <code>**tqdm_kwargs</code> <code>Any</code> <p>Extra keyword arguments forwarded to <code>tqdm.tqdm</code>.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Callable[[SimulationProgress], None]</code> <p>A callback suitable for the <code>on_progress</code> parameter.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If tqdm is not installed.</p> <p>Examples:</p> <pre><code>from idfkit.simulation import simulate\nfrom idfkit.simulation.progress_bars import tqdm_progress\n\nwith tqdm_progress(desc=\"Annual run\") as cb:\n    result = simulate(model, \"weather.epw\", annual=True, on_progress=cb)\n</code></pre>"},{"location":"api/simulation/#idfkit.simulation.simulate","title":"<code>simulate(model, weather, *, output_dir=None, energyplus=None, expand_objects=True, annual=False, design_day=False, output_prefix='eplus', output_suffix='C', readvars=False, timeout=3600.0, extra_args=None, cache=None, fs=None, on_progress=None)</code>","text":"<p>Run an EnergyPlus simulation.</p> <p>Creates an isolated run directory, writes the model, and executes EnergyPlus as a subprocess. The caller's model is not mutated.</p> <p>When expand_objects is <code>True</code> (the default) and the model contains <code>GroundHeatTransfer:Slab:*</code> or <code>GroundHeatTransfer:Basement:*</code> objects, the Slab and/or Basement ground heat-transfer preprocessors are run automatically before simulation.  This is equivalent to calling run_slab_preprocessor or run_basement_preprocessor individually, but happens transparently.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model to simulate.</p> required <code>weather</code> <code>str | Path</code> <p>Path to the weather file (.epw).</p> required <code>output_dir</code> <code>str | Path | None</code> <p>Directory for output files (default: auto temp dir).</p> <code>None</code> <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation. If None, uses find_energyplus for auto-discovery.</p> <code>None</code> <code>expand_objects</code> <code>bool</code> <p>Run ExpandObjects before simulation.  When <code>True</code>, also runs the Slab and Basement ground heat-transfer preprocessors if the model contains the corresponding objects.</p> <code>True</code> <code>annual</code> <code>bool</code> <p>Run annual simulation (<code>-a</code> flag).</p> <code>False</code> <code>design_day</code> <code>bool</code> <p>Run design-day-only simulation (<code>-D</code> flag).</p> <code>False</code> <code>output_prefix</code> <code>str</code> <p>Prefix for output files (default \"eplus\").</p> <code>'eplus'</code> <code>output_suffix</code> <code>Literal['C', 'L', 'D']</code> <p>Output file naming suffix: <code>\"C\"</code> for combined table files (default), <code>\"L\"</code> for legacy separate table files, or <code>\"D\"</code> for timestamped separate files.</p> <code>'C'</code> <code>readvars</code> <code>bool</code> <p>Run ReadVarsESO after simulation (<code>-r</code> flag).</p> <code>False</code> <code>timeout</code> <code>float</code> <p>Maximum runtime in seconds (default 3600).</p> <code>3600.0</code> <code>extra_args</code> <code>list[str] | None</code> <p>Additional command-line arguments.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>Optional file system backend for storing results on remote storage (e.g., S3). When provided, <code>output_dir</code> is required and specifies the remote destination path. EnergyPlus runs locally in a temp directory; results are then uploaded to <code>output_dir</code> via fs after execution.</p> <p>Note</p> <p>The <code>fs</code> parameter handles output storage only. The <code>weather</code> file must be a local path \u2014 remote weather files are not automatically downloaded. For cloud workflows, download weather files first using WeatherDownloader or pre-stage them locally before calling <code>simulate()</code>.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], Any] | Literal['tqdm'] | None</code> <p>Optional callback invoked with a SimulationProgress event each time EnergyPlus emits a progress line (warmup iterations, simulation day changes, post-processing steps, etc.).  Pass <code>\"tqdm\"</code> to use a built-in tqdm progress bar (requires <code>pip install idfkit[progress]</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>SimulationResult</code> <p>SimulationResult with paths to output files.</p> <p>Raises:</p> Type Description <code>SimulationError</code> <p>On timeout, OS error, or missing weather file.</p> <code>ExpandObjectsError</code> <p>If a preprocessing step (ExpandObjects, Slab, or Basement) fails during automatic preprocessing.</p> <code>EnergyPlusNotFoundError</code> <p>If EnergyPlus cannot be found.</p>"},{"location":"api/simulation/async/","title":"Async API","text":"<p>Non-blocking simulation execution using <code>asyncio</code>.</p>"},{"location":"api/simulation/async/#async_simulate","title":"async_simulate","text":""},{"location":"api/simulation/async/#idfkit.simulation.async_runner.async_simulate","title":"<code>idfkit.simulation.async_runner.async_simulate(model, weather, *, output_dir=None, energyplus=None, expand_objects=True, annual=False, design_day=False, output_prefix='eplus', output_suffix='C', readvars=False, timeout=3600.0, extra_args=None, cache=None, fs=None, on_progress=None)</code>  <code>async</code>","text":"<p>Run an EnergyPlus simulation without blocking the event loop.</p> <p>This is the async counterpart to simulate. All parameters and return values are identical; the only difference is that EnergyPlus runs as an asyncio subprocess, allowing the caller to <code>await</code> the result while other coroutines continue executing.</p> <p>Cancellation is supported: if the wrapping asyncio.Task is cancelled, the EnergyPlus subprocess is killed and cleaned up.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model to simulate.</p> required <code>weather</code> <code>str | Path</code> <p>Path to the weather file (.epw).</p> required <code>output_dir</code> <code>str | Path | None</code> <p>Directory for output files (default: auto temp dir).</p> <code>None</code> <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation. If None, uses find_energyplus for auto-discovery.</p> <code>None</code> <code>expand_objects</code> <code>bool</code> <p>Run ExpandObjects before simulation.  When <code>True</code>, also runs the Slab and Basement ground heat-transfer preprocessors if the model contains the corresponding objects.</p> <code>True</code> <code>annual</code> <code>bool</code> <p>Run annual simulation (<code>-a</code> flag).</p> <code>False</code> <code>design_day</code> <code>bool</code> <p>Run design-day-only simulation (<code>-D</code> flag).</p> <code>False</code> <code>output_prefix</code> <code>str</code> <p>Prefix for output files (default \"eplus\").</p> <code>'eplus'</code> <code>output_suffix</code> <code>Literal['C', 'L', 'D']</code> <p>Output file naming suffix: <code>\"C\"</code> for combined table files (default), <code>\"L\"</code> for legacy separate table files, or <code>\"D\"</code> for timestamped separate files.</p> <code>'C'</code> <code>readvars</code> <code>bool</code> <p>Run ReadVarsESO after simulation (<code>-r</code> flag).</p> <code>False</code> <code>timeout</code> <code>float</code> <p>Maximum runtime in seconds (default 3600).</p> <code>3600.0</code> <code>extra_args</code> <code>list[str] | None</code> <p>Additional command-line arguments.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>fs</code> <code>FileSystem | AsyncFileSystem | None</code> <p>Optional file system backend for storing results on remote storage (e.g., S3).  Both sync FileSystem and async AsyncFileSystem are accepted. When an <code>AsyncFileSystem</code> is provided, uploads and result reads are truly non-blocking.  A sync <code>FileSystem</code> is automatically wrapped in asyncio.to_thread to avoid blocking the event loop.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], Any] | Literal['tqdm'] | None</code> <p>Optional callback invoked with a SimulationProgress event each time EnergyPlus emits a progress line.  Both synchronous and async callables are accepted -- async callables are awaited. Pass <code>\"tqdm\"</code> to use a built-in tqdm progress bar (requires <code>pip install idfkit[progress]</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>SimulationResult</code> <p>SimulationResult with paths to output files.</p> <p>Raises:</p> Type Description <code>SimulationError</code> <p>On timeout, OS error, or missing weather file.</p> <code>ExpandObjectsError</code> <p>If a preprocessing step fails.</p> <code>EnergyPlusNotFoundError</code> <p>If EnergyPlus cannot be found.</p> Source code in <code>src/idfkit/simulation/async_runner.py</code> <pre><code>async def async_simulate(\n    model: IDFDocument,\n    weather: str | Path,\n    *,\n    output_dir: str | Path | None = None,\n    energyplus: EnergyPlusConfig | None = None,\n    expand_objects: bool = True,\n    annual: bool = False,\n    design_day: bool = False,\n    output_prefix: str = \"eplus\",\n    output_suffix: Literal[\"C\", \"L\", \"D\"] = \"C\",\n    readvars: bool = False,\n    timeout: float = 3600.0,\n    extra_args: list[str] | None = None,\n    cache: SimulationCache | None = None,\n    fs: FileSystem | AsyncFileSystem | None = None,\n    on_progress: Callable[[SimulationProgress], Any] | Literal[\"tqdm\"] | None = None,\n) -&gt; SimulationResult:\n    \"\"\"Run an EnergyPlus simulation without blocking the event loop.\n\n    This is the async counterpart to [simulate][idfkit.simulation.runner.simulate].\n    All parameters and return values are identical; the only difference is that\n    EnergyPlus runs as an [asyncio][] subprocess, allowing the caller to\n    ``await`` the result while other coroutines continue executing.\n\n    Cancellation is supported: if the wrapping [asyncio.Task][] is\n    cancelled, the EnergyPlus subprocess is killed and cleaned up.\n\n    Args:\n        model: The EnergyPlus model to simulate.\n        weather: Path to the weather file (.epw).\n        output_dir: Directory for output files (default: auto temp dir).\n        energyplus: Pre-configured EnergyPlus installation. If None,\n            uses [find_energyplus][idfkit.simulation.config.find_energyplus] for auto-discovery.\n        expand_objects: Run ExpandObjects before simulation.  When\n            ``True``, also runs the Slab and Basement ground heat-transfer\n            preprocessors if the model contains the corresponding objects.\n        annual: Run annual simulation (``-a`` flag).\n        design_day: Run design-day-only simulation (``-D`` flag).\n        output_prefix: Prefix for output files (default \"eplus\").\n        output_suffix: Output file naming suffix: ``\"C\"`` for combined table\n            files (default), ``\"L\"`` for legacy separate table files, or\n            ``\"D\"`` for timestamped separate files.\n        readvars: Run ReadVarsESO after simulation (``-r`` flag).\n        timeout: Maximum runtime in seconds (default 3600).\n        extra_args: Additional command-line arguments.\n        cache: Optional simulation cache for content-hash lookups.\n        fs: Optional file system backend for storing results on remote\n            storage (e.g., S3).  Both sync [FileSystem][idfkit.simulation.fs.FileSystem]\n            and async [AsyncFileSystem][idfkit.simulation.fs.AsyncFileSystem] are accepted.\n            When an ``AsyncFileSystem`` is provided, uploads and result reads\n            are truly non-blocking.  A sync ``FileSystem`` is automatically\n            wrapped in [asyncio.to_thread][] to avoid blocking the event\n            loop.\n        on_progress: Optional callback invoked with a\n            [SimulationProgress][idfkit.simulation.progress.SimulationProgress] event\n            each time EnergyPlus emits a progress line.  Both synchronous\n            and async callables are accepted -- async callables are awaited.\n            Pass ``\"tqdm\"`` to use a built-in tqdm progress bar (requires\n            ``pip install idfkit[progress]``).\n\n    Returns:\n        SimulationResult with paths to output files.\n\n    Raises:\n        SimulationError: On timeout, OS error, or missing weather file.\n        ExpandObjectsError: If a preprocessing step fails.\n        EnergyPlusNotFoundError: If EnergyPlus cannot be found.\n    \"\"\"\n    if fs is not None and output_dir is None:\n        msg = \"output_dir is required when using a file system backend\"\n        raise ValueError(msg)\n\n    progress_cb, progress_cleanup = resolve_on_progress(on_progress)\n\n    try:\n        config = resolve_config(energyplus)\n        weather_path = Path(weather).resolve()\n\n        if not weather_path.is_file():\n            msg = f\"Weather file not found: {weather_path}\"\n            raise SimulationError(msg)\n\n        cache_key: CacheKey | None = None\n        if cache is not None:\n            cache_key = cache.compute_key(\n                model,\n                weather_path,\n                expand_objects=expand_objects,\n                annual=annual,\n                design_day=design_day,\n                output_suffix=output_suffix,\n                readvars=readvars,\n                extra_args=extra_args,\n            )\n            cached = cache.get(cache_key)\n            if cached is not None:\n                return cached\n\n        # Copy model to avoid mutation\n        sim_model = model.copy()\n        ensure_sql_output(sim_model)\n\n        # Preprocessing may invoke subprocesses synchronously \u2014 delegate to a\n        # thread so we don't block the event loop.\n        sim_model, ep_expand = await asyncio.to_thread(\n            maybe_preprocess, model, sim_model, config, weather_path, expand_objects\n        )\n\n        # When using a remote fs, always run locally in a temp dir\n        local_output_dir = None if fs is not None else output_dir\n        run_dir = prepare_run_directory(local_output_dir, weather_path)\n        idf_path = run_dir / \"model.idf\"\n\n        from ..writers import write_idf\n\n        write_idf(sim_model, idf_path)\n\n        cmd = build_command(\n            config=config,\n            idf_path=idf_path,\n            weather_path=run_dir / weather_path.name,\n            output_dir=run_dir,\n            output_prefix=output_prefix,\n            output_suffix=output_suffix,\n            expand_objects=ep_expand,\n            annual=annual,\n            design_day=design_day,\n            readvars=readvars,\n            extra_args=extra_args,\n        )\n\n        start = time.monotonic()\n\n        if progress_cb is not None:\n            stdout, stderr, returncode = await _run_with_progress(cmd, run_dir, timeout, progress_cb)\n        else:\n            stdout, stderr, returncode = await _run_simple(cmd, run_dir, timeout)\n    finally:\n        if progress_cleanup is not None:\n            progress_cleanup()\n\n    elapsed = time.monotonic() - start\n\n    if fs is not None:\n        remote_dir = Path(str(output_dir))\n        if _is_async_fs(fs):\n            await async_upload_results(run_dir, remote_dir, fs)  # type: ignore[arg-type]\n            result = SimulationResult(\n                run_dir=remote_dir,\n                success=returncode == 0,\n                exit_code=returncode,\n                stdout=stdout,\n                stderr=stderr,\n                runtime_seconds=elapsed,\n                output_prefix=output_prefix,\n                async_fs=fs,  # type: ignore[arg-type]\n            )\n        else:\n            await asyncio.to_thread(upload_results, run_dir, remote_dir, fs)  # type: ignore[arg-type]\n            result = SimulationResult(\n                run_dir=remote_dir,\n                success=returncode == 0,\n                exit_code=returncode,\n                stdout=stdout,\n                stderr=stderr,\n                runtime_seconds=elapsed,\n                output_prefix=output_prefix,\n                fs=fs,  # type: ignore[arg-type]\n            )\n    else:\n        result = SimulationResult(\n            run_dir=run_dir,\n            success=returncode == 0,\n            exit_code=returncode,\n            stdout=stdout,\n            stderr=stderr,\n            runtime_seconds=elapsed,\n            output_prefix=output_prefix,\n        )\n    if cache is not None and cache_key is not None and result.success:\n        cache.put(cache_key, result)\n    return result\n</code></pre>"},{"location":"api/simulation/async/#async_simulate_batch","title":"async_simulate_batch","text":""},{"location":"api/simulation/async/#idfkit.simulation.async_batch.async_simulate_batch","title":"<code>idfkit.simulation.async_batch.async_simulate_batch(jobs, *, energyplus=None, max_concurrent=None, cache=None, fs=None, on_progress=None)</code>  <code>async</code>","text":"<p>Run multiple EnergyPlus simulations concurrently using asyncio.</p> <p>This is the async counterpart to simulate_batch.  Concurrency is controlled with an asyncio.Semaphore instead of a thread pool.</p> <p>Individual job failures are captured as failed SimulationResult entries -- the batch never raises due to a single job failing.</p> <p>Parameters:</p> Name Type Description Default <code>jobs</code> <code>Sequence[SimulationJob]</code> <p>Sequence of simulation jobs to execute.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Shared EnergyPlus configuration (auto-discovered if <code>None</code>).</p> <code>None</code> <code>max_concurrent</code> <code>int | None</code> <p>Maximum number of concurrent simulations.  Defaults to <code>min(len(jobs), os.cpu_count() or 1)</code>.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>fs</code> <code>FileSystem | AsyncFileSystem | None</code> <p>Optional file system backend passed through to each async_simulate call.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], Any] | None</code> <p>Optional callback invoked with SimulationProgress events during each individual simulation.  Events include <code>job_index</code> and <code>job_label</code> to identify which batch job they belong to.  Both sync and async callables are accepted. The <code>\"tqdm\"</code> shorthand is not supported for batch runners; use tqdm_progress with a custom per-job callback instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>BatchResult</code> <p>A BatchResult with results in the</p> <code>BatchResult</code> <p>same order as jobs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If jobs is empty.</p> Source code in <code>src/idfkit/simulation/async_batch.py</code> <pre><code>async def async_simulate_batch(\n    jobs: Sequence[SimulationJob],\n    *,\n    energyplus: EnergyPlusConfig | None = None,\n    max_concurrent: int | None = None,\n    cache: SimulationCache | None = None,\n    fs: FileSystem | AsyncFileSystem | None = None,\n    on_progress: Callable[[SimulationProgress], Any] | None = None,\n) -&gt; BatchResult:\n    \"\"\"Run multiple EnergyPlus simulations concurrently using asyncio.\n\n    This is the async counterpart to\n    [simulate_batch][idfkit.simulation.batch.simulate_batch].  Concurrency is\n    controlled with an [asyncio.Semaphore][] instead of a thread pool.\n\n    Individual job failures are captured as failed\n    [SimulationResult][idfkit.simulation.result.SimulationResult] entries -- the batch\n    never raises due to a single job failing.\n\n    Args:\n        jobs: Sequence of simulation jobs to execute.\n        energyplus: Shared EnergyPlus configuration (auto-discovered if\n            ``None``).\n        max_concurrent: Maximum number of concurrent simulations.  Defaults\n            to ``min(len(jobs), os.cpu_count() or 1)``.\n        cache: Optional simulation cache for content-hash lookups.\n        fs: Optional file system backend passed through to each\n            [async_simulate][idfkit.simulation.async_runner.async_simulate] call.\n        on_progress: Optional callback invoked with\n            [SimulationProgress][idfkit.simulation.progress.SimulationProgress] events\n            during each individual simulation.  Events include\n            ``job_index`` and ``job_label`` to identify which batch job\n            they belong to.  Both sync and async callables are accepted.\n            The ``\"tqdm\"`` shorthand is not supported for batch runners;\n            use [tqdm_progress][idfkit.simulation.progress_bars.tqdm_progress]\n            with a custom per-job callback instead.\n\n    Returns:\n        A [BatchResult][idfkit.simulation.batch.BatchResult] with results in the\n        same order as *jobs*.\n\n    Raises:\n        ValueError: If *jobs* is empty.\n    \"\"\"\n    if not jobs:\n        msg = \"jobs must not be empty\"\n        raise ValueError(msg)\n\n    if on_progress == \"tqdm\":\n        msg = (\n            'on_progress=\"tqdm\" is not supported for batch simulations because a single '\n            \"progress bar cannot represent multiple concurrent jobs. Use the tqdm_progress() \"\n            \"context manager with a custom callback instead.\"\n        )\n        raise ValueError(msg)\n\n    progress_cb, progress_cleanup = resolve_on_progress(on_progress)\n\n    try:\n        if max_concurrent is None:\n            max_concurrent = min(len(jobs), os.cpu_count() or 1)\n\n        semaphore = asyncio.Semaphore(max_concurrent)\n        results: list[SimulationResult | None] = [None] * len(jobs)\n        start = time.monotonic()\n\n        async def _run_one(idx: int, job: SimulationJob) -&gt; None:\n            async with semaphore:\n                results[idx] = await _async_run_job(idx, job, energyplus, cache, fs, progress_cb)\n\n        tasks = [asyncio.create_task(_run_one(i, job)) for i, job in enumerate(jobs)]\n        await asyncio.gather(*tasks)\n    finally:\n        if progress_cleanup is not None:\n            progress_cleanup()\n\n    elapsed = time.monotonic() - start\n\n    final: list[SimulationResult] = []\n    for r in results:\n        assert r is not None  # noqa: S101\n        final.append(r)\n\n    return BatchResult(results=tuple(final), total_runtime_seconds=elapsed)\n</code></pre>"},{"location":"api/simulation/async/#async_simulate_batch_stream","title":"async_simulate_batch_stream","text":""},{"location":"api/simulation/async/#idfkit.simulation.async_batch.async_simulate_batch_stream","title":"<code>idfkit.simulation.async_batch.async_simulate_batch_stream(jobs, *, energyplus=None, max_concurrent=None, cache=None, fs=None, on_progress=None)</code>  <code>async</code>","text":"<p>Run simulations concurrently, yielding events as each one completes.</p> <p>This is an async generator variant of async_simulate_batch that yields SimulationEvent objects in completion order.  This enables real-time progress reporting without needing a callback:</p> <p>.. code-block:: python</p> <pre><code>async for event in async_simulate_batch_stream(jobs, max_concurrent=4):\n    print(f\"[{event.completed}/{event.total}] {event.label}\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>jobs</code> <code>Sequence[SimulationJob]</code> <p>Sequence of simulation jobs to execute.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Shared EnergyPlus configuration (auto-discovered if <code>None</code>).</p> <code>None</code> <code>max_concurrent</code> <code>int | None</code> <p>Maximum number of concurrent simulations.  Defaults to <code>min(len(jobs), os.cpu_count() or 1)</code>.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>fs</code> <code>FileSystem | AsyncFileSystem | None</code> <p>Optional file system backend.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], Any] | None</code> <p>Optional callback invoked with SimulationProgress events during each individual simulation.  Events include <code>job_index</code> and <code>job_label</code>.  The <code>\"tqdm\"</code> shorthand is not supported for batch runners; use tqdm_progress with a custom per-job callback instead.</p> <code>None</code> <p>Yields:</p> Type Description <code>AsyncIterator[SimulationEvent]</code> <p>SimulationEvent for each completed simulation, in the order</p> <code>AsyncIterator[SimulationEvent]</code> <p>they finish.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If jobs is empty.</p> Source code in <code>src/idfkit/simulation/async_batch.py</code> <pre><code>async def async_simulate_batch_stream(\n    jobs: Sequence[SimulationJob],\n    *,\n    energyplus: EnergyPlusConfig | None = None,\n    max_concurrent: int | None = None,\n    cache: SimulationCache | None = None,\n    fs: FileSystem | AsyncFileSystem | None = None,\n    on_progress: Callable[[SimulationProgress], Any] | None = None,\n) -&gt; AsyncIterator[SimulationEvent]:\n    \"\"\"Run simulations concurrently, yielding events as each one completes.\n\n    This is an async generator variant of [async_simulate_batch][idfkit.simulation.async_batch.async_simulate_batch] that\n    yields [SimulationEvent][idfkit.simulation.async_batch.SimulationEvent] objects in *completion order*.  This\n    enables real-time progress reporting without needing a callback:\n\n    .. code-block:: python\n\n        async for event in async_simulate_batch_stream(jobs, max_concurrent=4):\n            print(f\"[{event.completed}/{event.total}] {event.label}\")\n\n    Args:\n        jobs: Sequence of simulation jobs to execute.\n        energyplus: Shared EnergyPlus configuration (auto-discovered if\n            ``None``).\n        max_concurrent: Maximum number of concurrent simulations.  Defaults\n            to ``min(len(jobs), os.cpu_count() or 1)``.\n        cache: Optional simulation cache for content-hash lookups.\n        fs: Optional file system backend.\n        on_progress: Optional callback invoked with\n            [SimulationProgress][idfkit.simulation.progress.SimulationProgress] events\n            during each individual simulation.  Events include\n            ``job_index`` and ``job_label``.  The ``\"tqdm\"`` shorthand\n            is not supported for batch runners; use\n            [tqdm_progress][idfkit.simulation.progress_bars.tqdm_progress]\n            with a custom per-job callback instead.\n\n    Yields:\n        [SimulationEvent][idfkit.simulation.async_batch.SimulationEvent] for each completed simulation, in the order\n        they finish.\n\n    Raises:\n        ValueError: If *jobs* is empty.\n    \"\"\"\n    if not jobs:\n        msg = \"jobs must not be empty\"\n        raise ValueError(msg)\n\n    if on_progress == \"tqdm\":\n        msg = (\n            'on_progress=\"tqdm\" is not supported for batch simulations because a single '\n            \"progress bar cannot represent multiple concurrent jobs. Use the tqdm_progress() \"\n            \"context manager with a custom callback instead.\"\n        )\n        raise ValueError(msg)\n\n    progress_cb, progress_cleanup = resolve_on_progress(on_progress)\n\n    if max_concurrent is None:\n        max_concurrent = min(len(jobs), os.cpu_count() or 1)\n\n    semaphore = asyncio.Semaphore(max_concurrent)\n    total = len(jobs)\n    queue: asyncio.Queue[SimulationEvent] = asyncio.Queue()\n    completed_count = 0\n\n    async def _run_one(idx: int, job: SimulationJob) -&gt; None:\n        nonlocal completed_count\n        async with semaphore:\n            result = await _async_run_job(idx, job, energyplus, cache, fs, progress_cb)\n        completed_count += 1\n        await queue.put(\n            SimulationEvent(\n                index=idx,\n                label=job.label,\n                result=result,\n                completed=completed_count,\n                total=total,\n            )\n        )\n\n    tasks = [asyncio.create_task(_run_one(i, job)) for i, job in enumerate(jobs)]\n\n    try:\n        for _ in range(total):\n            event = await queue.get()\n            yield event\n    finally:\n        if progress_cleanup is not None:\n            progress_cleanup()\n        # If the consumer breaks out early, cancel remaining tasks.\n        for task in tasks:\n            task.cancel()\n        await asyncio.gather(*tasks, return_exceptions=True)\n</code></pre>"},{"location":"api/simulation/async/#simulationevent","title":"SimulationEvent","text":""},{"location":"api/simulation/async/#idfkit.simulation.async_batch.SimulationEvent","title":"<code>idfkit.simulation.async_batch.SimulationEvent</code>  <code>dataclass</code>","text":"<p>Progress event emitted by async_simulate_batch_stream.</p> <p>Each event represents a single simulation that has finished (successfully or not).  Events are yielded in completion order, not submission order.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>Zero-based position of this job in the original jobs sequence.</p> <code>label</code> <code>str</code> <p>Human-readable label from the SimulationJob.</p> <code>result</code> <code>SimulationResult</code> <p>The simulation result.</p> <code>completed</code> <code>int</code> <p>Number of jobs completed so far (including this one).</p> <code>total</code> <code>int</code> <p>Total number of jobs in the batch.</p> Source code in <code>src/idfkit/simulation/async_batch.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass SimulationEvent:\n    \"\"\"Progress event emitted by [async_simulate_batch_stream][idfkit.simulation.async_batch.async_simulate_batch_stream].\n\n    Each event represents a single simulation that has finished (successfully\n    or not).  Events are yielded in *completion order*, not submission order.\n\n    Attributes:\n        index: Zero-based position of this job in the original *jobs* sequence.\n        label: Human-readable label from the [SimulationJob][idfkit.simulation.batch.SimulationJob].\n        result: The simulation result.\n        completed: Number of jobs completed so far (including this one).\n        total: Total number of jobs in the batch.\n    \"\"\"\n\n    index: int\n    label: str\n    result: SimulationResult\n    completed: int\n    total: int\n</code></pre>"},{"location":"api/simulation/async/#idfkit.simulation.async_batch.SimulationEvent.index","title":"<code>index</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/async/#idfkit.simulation.async_batch.SimulationEvent.label","title":"<code>label</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/async/#idfkit.simulation.async_batch.SimulationEvent.result","title":"<code>result</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/async/#idfkit.simulation.async_batch.SimulationEvent.completed","title":"<code>completed</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/async/#idfkit.simulation.async_batch.SimulationEvent.total","title":"<code>total</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/","title":"Batch API","text":"<p>Parallel simulation execution with thread-pool parallelism.</p>"},{"location":"api/simulation/batch/#simulate_batch","title":"simulate_batch","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.simulate_batch","title":"<code>idfkit.simulation.batch.simulate_batch(jobs, *, energyplus=None, max_workers=None, cache=None, progress=None, fs=None, on_progress=None)</code>","text":"<p>Run multiple EnergyPlus simulations in parallel.</p> <p>Uses ThreadPoolExecutor to dispatch simulations concurrently.  Individual job failures are captured as failed SimulationResult entries -- the batch never raises due to a single job failing.</p> <p>Parameters:</p> Name Type Description Default <code>jobs</code> <code>Sequence[SimulationJob]</code> <p>Sequence of simulation jobs to execute.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Shared EnergyPlus configuration (auto-discovered if <code>None</code>).</p> <code>None</code> <code>max_workers</code> <code>int | None</code> <p>Maximum number of concurrent simulations.  Defaults to <code>min(len(jobs), os.cpu_count() or 1)</code>.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>progress</code> <code>Callable[..., None] | None</code> <p>Optional callback invoked after each job completes. Called as <code>progress(completed=N, total=M, label=label, success=bool)</code>.</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>Optional file system backend passed through to each simulate call.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], None] | None</code> <p>Optional callback invoked with SimulationProgress events during each individual simulation.  Events include <code>job_index</code> and <code>job_label</code> to identify which batch job they belong to.  The <code>\"tqdm\"</code> shorthand is not supported for batch runners; use tqdm_progress with a custom per-job callback instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>BatchResult</code> <p>A BatchResult with results in the same order as jobs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If jobs is empty.</p> Source code in <code>src/idfkit/simulation/batch.py</code> <pre><code>def simulate_batch(\n    jobs: Sequence[SimulationJob],\n    *,\n    energyplus: EnergyPlusConfig | None = None,\n    max_workers: int | None = None,\n    cache: SimulationCache | None = None,\n    progress: Callable[..., None] | None = None,\n    fs: FileSystem | None = None,\n    on_progress: Callable[[SimulationProgress], None] | None = None,\n) -&gt; BatchResult:\n    \"\"\"Run multiple EnergyPlus simulations in parallel.\n\n    Uses [ThreadPoolExecutor][concurrent.futures.ThreadPoolExecutor] to dispatch\n    simulations concurrently.  Individual job failures are captured as\n    failed [SimulationResult][idfkit.simulation.result.SimulationResult] entries -- the batch never raises\n    due to a single job failing.\n\n    Args:\n        jobs: Sequence of simulation jobs to execute.\n        energyplus: Shared EnergyPlus configuration (auto-discovered if\n            ``None``).\n        max_workers: Maximum number of concurrent simulations.  Defaults\n            to ``min(len(jobs), os.cpu_count() or 1)``.\n        cache: Optional simulation cache for content-hash lookups.\n        progress: Optional callback invoked after each job completes.\n            Called as ``progress(completed=N, total=M, label=label,\n            success=bool)``.\n        fs: Optional file system backend passed through to each\n            [simulate][idfkit.simulation.runner.simulate] call.\n        on_progress: Optional callback invoked with\n            [SimulationProgress][idfkit.simulation.progress.SimulationProgress] events\n            during each individual simulation.  Events include\n            ``job_index`` and ``job_label`` to identify which batch job\n            they belong to.  The ``\"tqdm\"`` shorthand is not supported\n            for batch runners; use\n            [tqdm_progress][idfkit.simulation.progress_bars.tqdm_progress]\n            with a custom per-job callback instead.\n\n    Returns:\n        A [BatchResult][idfkit.simulation.batch.BatchResult] with results in the same order as *jobs*.\n\n    Raises:\n        ValueError: If *jobs* is empty.\n    \"\"\"\n    if not jobs:\n        msg = \"jobs must not be empty\"\n        raise ValueError(msg)\n\n    if on_progress == \"tqdm\":\n        msg = (\n            'on_progress=\"tqdm\" is not supported for batch simulations because a single '\n            \"progress bar cannot represent multiple concurrent jobs. Use the tqdm_progress() \"\n            \"context manager with a custom callback instead.\"\n        )\n        raise ValueError(msg)\n\n    progress_cb, progress_cleanup = resolve_on_progress(on_progress)\n\n    try:\n        if max_workers is None:\n            max_workers = min(len(jobs), os.cpu_count() or 1)\n\n        results: list[SimulationResult | None] = [None] * len(jobs)\n        completed_count = 0\n        total = len(jobs)\n\n        start = time.monotonic()\n\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            future_to_index = {\n                executor.submit(_run_job, idx, job, energyplus, cache, fs, progress_cb): idx\n                for idx, job in enumerate(jobs)\n            }\n\n            for future in as_completed(future_to_index):\n                idx = future_to_index[future]\n                job = jobs[idx]\n                result = future.result()  # _run_job never raises\n                results[idx] = result\n                completed_count += 1\n\n                if progress is not None:\n                    progress(\n                        completed=completed_count,\n                        total=total,\n                        label=job.label,\n                        success=result.success,\n                    )\n    finally:\n        if progress_cleanup is not None:\n            progress_cleanup()\n\n    elapsed = time.monotonic() - start\n\n    # All slots filled \u2014 assert for type checker\n    final: list[SimulationResult] = []\n    for r in results:\n        assert r is not None  # noqa: S101\n        final.append(r)\n\n    return BatchResult(results=tuple(final), total_runtime_seconds=elapsed)\n</code></pre>"},{"location":"api/simulation/batch/#simulationjob","title":"SimulationJob","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob","title":"<code>idfkit.simulation.batch.SimulationJob</code>  <code>dataclass</code>","text":"<p>Specification for a single simulation within a batch.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>object</code> <p>The EnergyPlus model to simulate.</p> <code>weather</code> <code>str | Path</code> <p>Path to the weather file.</p> <code>label</code> <code>str</code> <p>Human-readable label for progress reporting.</p> <code>output_dir</code> <code>str | Path | None</code> <p>Directory for output files (default: auto temp dir).</p> <code>expand_objects</code> <code>bool</code> <p>Run ExpandObjects before simulation.</p> <code>annual</code> <code>bool</code> <p>Run annual simulation.</p> <code>design_day</code> <code>bool</code> <p>Run design-day-only simulation.</p> <code>output_prefix</code> <code>str</code> <p>Prefix for output files.</p> <code>output_suffix</code> <code>Literal['C', 'L', 'D']</code> <p>Output file naming suffix (<code>\"C\"</code>, <code>\"L\"</code>, or <code>\"D\"</code>).</p> <code>readvars</code> <code>bool</code> <p>Run ReadVarsESO after simulation.</p> <code>timeout</code> <code>float</code> <p>Maximum runtime in seconds.</p> <code>extra_args</code> <code>tuple[str, ...] | None</code> <p>Additional command-line arguments.</p> Source code in <code>src/idfkit/simulation/batch.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass SimulationJob:\n    \"\"\"Specification for a single simulation within a batch.\n\n    Attributes:\n        model: The EnergyPlus model to simulate.\n        weather: Path to the weather file.\n        label: Human-readable label for progress reporting.\n        output_dir: Directory for output files (default: auto temp dir).\n        expand_objects: Run ExpandObjects before simulation.\n        annual: Run annual simulation.\n        design_day: Run design-day-only simulation.\n        output_prefix: Prefix for output files.\n        output_suffix: Output file naming suffix (``\"C\"``, ``\"L\"``, or ``\"D\"``).\n        readvars: Run ReadVarsESO after simulation.\n        timeout: Maximum runtime in seconds.\n        extra_args: Additional command-line arguments.\n    \"\"\"\n\n    model: object  # IDFDocument \u2014 use object to avoid import at class level\n    weather: str | Path\n    label: str = \"\"\n    output_dir: str | Path | None = None\n    expand_objects: bool = True\n    annual: bool = False\n    design_day: bool = False\n    output_prefix: str = \"eplus\"\n    output_suffix: Literal[\"C\", \"L\", \"D\"] = \"C\"\n    readvars: bool = False\n    timeout: float = 3600.0\n    extra_args: tuple[str, ...] | None = None\n</code></pre>"},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.model","title":"<code>model</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.weather","title":"<code>weather</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.label","title":"<code>label = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.output_dir","title":"<code>output_dir = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.expand_objects","title":"<code>expand_objects = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.annual","title":"<code>annual = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.design_day","title":"<code>design_day = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.output_prefix","title":"<code>output_prefix = 'eplus'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.output_suffix","title":"<code>output_suffix = 'C'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.readvars","title":"<code>readvars = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.timeout","title":"<code>timeout = 3600.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.SimulationJob.extra_args","title":"<code>extra_args = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#batchresult","title":"BatchResult","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.BatchResult","title":"<code>idfkit.simulation.batch.BatchResult</code>  <code>dataclass</code>","text":"<p>Aggregated results from a batch simulation run.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>tuple[SimulationResult, ...]</code> <p>Simulation results in the same order as the input jobs.</p> <code>total_runtime_seconds</code> <code>float</code> <p>Wall-clock time for the entire batch.</p> Source code in <code>src/idfkit/simulation/batch.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass BatchResult:\n    \"\"\"Aggregated results from a batch simulation run.\n\n    Attributes:\n        results: Simulation results in the same order as the input jobs.\n        total_runtime_seconds: Wall-clock time for the entire batch.\n    \"\"\"\n\n    results: tuple[SimulationResult, ...]\n    total_runtime_seconds: float\n\n    @property\n    def succeeded(self) -&gt; tuple[SimulationResult, ...]:\n        \"\"\"Results that completed successfully.\"\"\"\n        return tuple(r for r in self.results if r.success)\n\n    @property\n    def failed(self) -&gt; tuple[SimulationResult, ...]:\n        \"\"\"Results that failed.\"\"\"\n        return tuple(r for r in self.results if not r.success)\n\n    @property\n    def all_succeeded(self) -&gt; bool:\n        \"\"\"Whether every job in the batch succeeded.\"\"\"\n        return all(r.success for r in self.results)\n\n    def __len__(self) -&gt; int:\n        return len(self.results)\n\n    def __getitem__(self, index: int) -&gt; SimulationResult:\n        return self.results[index]\n</code></pre>"},{"location":"api/simulation/batch/#idfkit.simulation.batch.BatchResult.results","title":"<code>results</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.BatchResult.total_runtime_seconds","title":"<code>total_runtime_seconds</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/batch/#idfkit.simulation.batch.BatchResult.succeeded","title":"<code>succeeded</code>  <code>property</code>","text":"<p>Results that completed successfully.</p>"},{"location":"api/simulation/batch/#idfkit.simulation.batch.BatchResult.failed","title":"<code>failed</code>  <code>property</code>","text":"<p>Results that failed.</p>"},{"location":"api/simulation/batch/#idfkit.simulation.batch.BatchResult.all_succeeded","title":"<code>all_succeeded</code>  <code>property</code>","text":"<p>Whether every job in the batch succeeded.</p>"},{"location":"api/simulation/cache/","title":"Cache API","text":"<p>Content-addressed simulation result caching.</p>"},{"location":"api/simulation/cache/#simulationcache","title":"SimulationCache","text":""},{"location":"api/simulation/cache/#idfkit.simulation.cache.SimulationCache","title":"<code>idfkit.simulation.cache.SimulationCache</code>","text":"<p>Content-addressed simulation result cache.</p> <p>Each entry is a directory named by the cache key containing a full copy of the simulation run directory plus a <code>_cache_meta.json</code> manifest.</p> Source code in <code>src/idfkit/simulation/cache.py</code> <pre><code>class SimulationCache:\n    \"\"\"Content-addressed simulation result cache.\n\n    Each entry is a directory named by the cache key containing a full copy\n    of the simulation run directory plus a ``_cache_meta.json`` manifest.\n    \"\"\"\n\n    __slots__ = (\"_cache_dir\",)\n\n    _META_FILE = \"_cache_meta.json\"\n\n    def __init__(self, cache_dir: str | Path | None = None) -&gt; None:\n        self._cache_dir = Path(cache_dir) if cache_dir is not None else default_simulation_cache_dir()\n\n    @property\n    def cache_dir(self) -&gt; Path:\n        \"\"\"Root directory for cached simulation entries.\"\"\"\n        return self._cache_dir\n\n    def compute_key(\n        self,\n        model: IDFDocument,\n        weather: str | Path,\n        *,\n        expand_objects: bool = True,\n        annual: bool = False,\n        design_day: bool = False,\n        output_suffix: Literal[\"C\", \"L\", \"D\"] = \"C\",\n        readvars: bool = False,\n        extra_args: list[str] | tuple[str, ...] | None = None,\n    ) -&gt; CacheKey:\n        \"\"\"Compute a deterministic cache key for a simulation invocation.\n\n        The model is copied and normalised (``Output:SQLite`` is ensured) so\n        that models differing only in the presence of that object produce the\n        same key.\n\n        Args:\n            model: The EnergyPlus model.\n            weather: Path to the weather file.\n            expand_objects: Whether ExpandObjects will run.\n            annual: Whether annual simulation is used.\n            design_day: Whether design-day-only simulation is used.\n            output_suffix: Output file naming suffix (``\"C\"``, ``\"L\"``, or ``\"D\"``).\n            readvars: Whether ReadVarsESO post-processing will run.\n            extra_args: Additional command-line arguments.\n\n        Returns:\n            A [CacheKey][idfkit.simulation.cache.CacheKey] for use with [get][idfkit.simulation.cache.SimulationCache.get] / [put][idfkit.simulation.cache.SimulationCache.put].\n        \"\"\"\n        from ..writers import write_idf\n\n        normalised = model.copy()\n        if \"Output:SQLite\" not in normalised:\n            normalised.add(\"Output:SQLite\", \"\", data={\"option_type\": \"SimpleAndTabular\"})\n        idf_text: str = write_idf(normalised) or \"\"\n\n        weather_path = Path(weather).resolve()\n        weather_bytes = weather_path.read_bytes()\n\n        flags = json.dumps(\n            {\n                \"expand_objects\": expand_objects,\n                \"annual\": annual,\n                \"design_day\": design_day,\n                \"output_suffix\": output_suffix,\n                \"readvars\": readvars,\n                \"extra_args\": list(extra_args) if extra_args else [],\n            },\n            sort_keys=True,\n        )\n\n        h = hashlib.sha256()\n        h.update(idf_text.encode(\"utf-8\"))\n        h.update(weather_bytes)\n        h.update(flags.encode(\"utf-8\"))\n        return CacheKey(hex_digest=h.hexdigest())\n\n    def get(self, key: CacheKey) -&gt; SimulationResult | None:\n        \"\"\"Retrieve a cached simulation result.\n\n        Args:\n            key: Cache key from [compute_key][idfkit.simulation.cache.SimulationCache.compute_key].\n\n        Returns:\n            A [SimulationResult][idfkit.simulation.result.SimulationResult] if a cache hit exists, otherwise\n            ``None``.\n        \"\"\"\n        entry_dir = self._cache_dir / key.hex_digest\n        meta_path = entry_dir / self._META_FILE\n        if not meta_path.is_file():\n            return None\n\n        try:\n            meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n\n            from .result import SimulationResult\n\n            return SimulationResult(\n                run_dir=entry_dir,\n                success=meta[\"success\"],\n                exit_code=meta[\"exit_code\"],\n                stdout=\"\",\n                stderr=\"\",\n                runtime_seconds=meta[\"runtime_seconds\"],\n                output_prefix=meta[\"output_prefix\"],\n            )\n        except (json.JSONDecodeError, KeyError, OSError):\n            # Corrupted or incomplete cache entry \u2014 remove it so that\n            # a subsequent put() can write a fresh copy, then treat as miss.\n            shutil.rmtree(entry_dir, ignore_errors=True)\n            return None\n\n    def put(self, key: CacheKey, result: SimulationResult) -&gt; None:\n        \"\"\"Store a successful simulation result in the cache.\n\n        Only results with ``success=True`` are cached.  The entire run\n        directory is copied into the cache atomically.\n\n        Args:\n            key: Cache key from [compute_key][idfkit.simulation.cache.SimulationCache.compute_key].\n            result: Successful simulation result to cache.\n        \"\"\"\n        if not result.success:\n            return\n\n        target_dir = self._cache_dir / key.hex_digest\n        if target_dir.is_dir():\n            return  # already cached\n\n        self._cache_dir.mkdir(parents=True, exist_ok=True)\n\n        tmp_dir = Path(tempfile.mkdtemp(dir=self._cache_dir, prefix=\".tmp_\"))\n        try:\n            # Copy output files (ignore_dangling_symlinks for robustness)\n            shutil.copytree(result.run_dir, tmp_dir, dirs_exist_ok=True)\n\n            # Write metadata\n            meta = {\n                \"success\": result.success,\n                \"exit_code\": result.exit_code,\n                \"runtime_seconds\": result.runtime_seconds,\n                \"output_prefix\": result.output_prefix,\n            }\n            meta_path = tmp_dir / self._META_FILE\n            meta_path.write_text(json.dumps(meta), encoding=\"utf-8\")\n\n            # Atomic rename \u2014 os.rename fails if target_dir already exists\n            # (another process beat us), unlike shutil.move which would nest\n            # tmp_dir inside the existing target as a subdirectory.\n            os.rename(str(tmp_dir), str(target_dir))\n        except OSError:\n            # Another thread/process beat us, or a real filesystem error\n            # \u2014 clean up the temporary directory.\n            shutil.rmtree(tmp_dir, ignore_errors=True)\n\n    def contains(self, key: CacheKey) -&gt; bool:\n        \"\"\"Check whether a cache entry exists for *key*.\"\"\"\n        return (self._cache_dir / key.hex_digest / self._META_FILE).is_file()\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all cached entries.\"\"\"\n        if self._cache_dir.is_dir():\n            shutil.rmtree(self._cache_dir)\n</code></pre>"},{"location":"api/simulation/cache/#idfkit.simulation.cache.SimulationCache.cache_dir","title":"<code>cache_dir</code>  <code>property</code>","text":"<p>Root directory for cached simulation entries.</p>"},{"location":"api/simulation/cache/#idfkit.simulation.cache.SimulationCache.compute_key","title":"<code>compute_key(model, weather, *, expand_objects=True, annual=False, design_day=False, output_suffix='C', readvars=False, extra_args=None)</code>","text":"<p>Compute a deterministic cache key for a simulation invocation.</p> <p>The model is copied and normalised (<code>Output:SQLite</code> is ensured) so that models differing only in the presence of that object produce the same key.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model.</p> required <code>weather</code> <code>str | Path</code> <p>Path to the weather file.</p> required <code>expand_objects</code> <code>bool</code> <p>Whether ExpandObjects will run.</p> <code>True</code> <code>annual</code> <code>bool</code> <p>Whether annual simulation is used.</p> <code>False</code> <code>design_day</code> <code>bool</code> <p>Whether design-day-only simulation is used.</p> <code>False</code> <code>output_suffix</code> <code>Literal['C', 'L', 'D']</code> <p>Output file naming suffix (<code>\"C\"</code>, <code>\"L\"</code>, or <code>\"D\"</code>).</p> <code>'C'</code> <code>readvars</code> <code>bool</code> <p>Whether ReadVarsESO post-processing will run.</p> <code>False</code> <code>extra_args</code> <code>list[str] | tuple[str, ...] | None</code> <p>Additional command-line arguments.</p> <code>None</code> <p>Returns:</p> Type Description <code>CacheKey</code> <p>A CacheKey for use with get / put.</p> Source code in <code>src/idfkit/simulation/cache.py</code> <pre><code>def compute_key(\n    self,\n    model: IDFDocument,\n    weather: str | Path,\n    *,\n    expand_objects: bool = True,\n    annual: bool = False,\n    design_day: bool = False,\n    output_suffix: Literal[\"C\", \"L\", \"D\"] = \"C\",\n    readvars: bool = False,\n    extra_args: list[str] | tuple[str, ...] | None = None,\n) -&gt; CacheKey:\n    \"\"\"Compute a deterministic cache key for a simulation invocation.\n\n    The model is copied and normalised (``Output:SQLite`` is ensured) so\n    that models differing only in the presence of that object produce the\n    same key.\n\n    Args:\n        model: The EnergyPlus model.\n        weather: Path to the weather file.\n        expand_objects: Whether ExpandObjects will run.\n        annual: Whether annual simulation is used.\n        design_day: Whether design-day-only simulation is used.\n        output_suffix: Output file naming suffix (``\"C\"``, ``\"L\"``, or ``\"D\"``).\n        readvars: Whether ReadVarsESO post-processing will run.\n        extra_args: Additional command-line arguments.\n\n    Returns:\n        A [CacheKey][idfkit.simulation.cache.CacheKey] for use with [get][idfkit.simulation.cache.SimulationCache.get] / [put][idfkit.simulation.cache.SimulationCache.put].\n    \"\"\"\n    from ..writers import write_idf\n\n    normalised = model.copy()\n    if \"Output:SQLite\" not in normalised:\n        normalised.add(\"Output:SQLite\", \"\", data={\"option_type\": \"SimpleAndTabular\"})\n    idf_text: str = write_idf(normalised) or \"\"\n\n    weather_path = Path(weather).resolve()\n    weather_bytes = weather_path.read_bytes()\n\n    flags = json.dumps(\n        {\n            \"expand_objects\": expand_objects,\n            \"annual\": annual,\n            \"design_day\": design_day,\n            \"output_suffix\": output_suffix,\n            \"readvars\": readvars,\n            \"extra_args\": list(extra_args) if extra_args else [],\n        },\n        sort_keys=True,\n    )\n\n    h = hashlib.sha256()\n    h.update(idf_text.encode(\"utf-8\"))\n    h.update(weather_bytes)\n    h.update(flags.encode(\"utf-8\"))\n    return CacheKey(hex_digest=h.hexdigest())\n</code></pre>"},{"location":"api/simulation/cache/#idfkit.simulation.cache.SimulationCache.get","title":"<code>get(key)</code>","text":"<p>Retrieve a cached simulation result.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>CacheKey</code> <p>Cache key from compute_key.</p> required <p>Returns:</p> Type Description <code>SimulationResult | None</code> <p>A SimulationResult if a cache hit exists, otherwise</p> <code>SimulationResult | None</code> <p><code>None</code>.</p> Source code in <code>src/idfkit/simulation/cache.py</code> <pre><code>def get(self, key: CacheKey) -&gt; SimulationResult | None:\n    \"\"\"Retrieve a cached simulation result.\n\n    Args:\n        key: Cache key from [compute_key][idfkit.simulation.cache.SimulationCache.compute_key].\n\n    Returns:\n        A [SimulationResult][idfkit.simulation.result.SimulationResult] if a cache hit exists, otherwise\n        ``None``.\n    \"\"\"\n    entry_dir = self._cache_dir / key.hex_digest\n    meta_path = entry_dir / self._META_FILE\n    if not meta_path.is_file():\n        return None\n\n    try:\n        meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n\n        from .result import SimulationResult\n\n        return SimulationResult(\n            run_dir=entry_dir,\n            success=meta[\"success\"],\n            exit_code=meta[\"exit_code\"],\n            stdout=\"\",\n            stderr=\"\",\n            runtime_seconds=meta[\"runtime_seconds\"],\n            output_prefix=meta[\"output_prefix\"],\n        )\n    except (json.JSONDecodeError, KeyError, OSError):\n        # Corrupted or incomplete cache entry \u2014 remove it so that\n        # a subsequent put() can write a fresh copy, then treat as miss.\n        shutil.rmtree(entry_dir, ignore_errors=True)\n        return None\n</code></pre>"},{"location":"api/simulation/cache/#idfkit.simulation.cache.SimulationCache.put","title":"<code>put(key, result)</code>","text":"<p>Store a successful simulation result in the cache.</p> <p>Only results with <code>success=True</code> are cached.  The entire run directory is copied into the cache atomically.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>CacheKey</code> <p>Cache key from compute_key.</p> required <code>result</code> <code>SimulationResult</code> <p>Successful simulation result to cache.</p> required Source code in <code>src/idfkit/simulation/cache.py</code> <pre><code>def put(self, key: CacheKey, result: SimulationResult) -&gt; None:\n    \"\"\"Store a successful simulation result in the cache.\n\n    Only results with ``success=True`` are cached.  The entire run\n    directory is copied into the cache atomically.\n\n    Args:\n        key: Cache key from [compute_key][idfkit.simulation.cache.SimulationCache.compute_key].\n        result: Successful simulation result to cache.\n    \"\"\"\n    if not result.success:\n        return\n\n    target_dir = self._cache_dir / key.hex_digest\n    if target_dir.is_dir():\n        return  # already cached\n\n    self._cache_dir.mkdir(parents=True, exist_ok=True)\n\n    tmp_dir = Path(tempfile.mkdtemp(dir=self._cache_dir, prefix=\".tmp_\"))\n    try:\n        # Copy output files (ignore_dangling_symlinks for robustness)\n        shutil.copytree(result.run_dir, tmp_dir, dirs_exist_ok=True)\n\n        # Write metadata\n        meta = {\n            \"success\": result.success,\n            \"exit_code\": result.exit_code,\n            \"runtime_seconds\": result.runtime_seconds,\n            \"output_prefix\": result.output_prefix,\n        }\n        meta_path = tmp_dir / self._META_FILE\n        meta_path.write_text(json.dumps(meta), encoding=\"utf-8\")\n\n        # Atomic rename \u2014 os.rename fails if target_dir already exists\n        # (another process beat us), unlike shutil.move which would nest\n        # tmp_dir inside the existing target as a subdirectory.\n        os.rename(str(tmp_dir), str(target_dir))\n    except OSError:\n        # Another thread/process beat us, or a real filesystem error\n        # \u2014 clean up the temporary directory.\n        shutil.rmtree(tmp_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/simulation/cache/#idfkit.simulation.cache.SimulationCache.contains","title":"<code>contains(key)</code>","text":"<p>Check whether a cache entry exists for key.</p> Source code in <code>src/idfkit/simulation/cache.py</code> <pre><code>def contains(self, key: CacheKey) -&gt; bool:\n    \"\"\"Check whether a cache entry exists for *key*.\"\"\"\n    return (self._cache_dir / key.hex_digest / self._META_FILE).is_file()\n</code></pre>"},{"location":"api/simulation/cache/#idfkit.simulation.cache.SimulationCache.clear","title":"<code>clear()</code>","text":"<p>Remove all cached entries.</p> Source code in <code>src/idfkit/simulation/cache.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Remove all cached entries.\"\"\"\n    if self._cache_dir.is_dir():\n        shutil.rmtree(self._cache_dir)\n</code></pre>"},{"location":"api/simulation/cache/#cachekey","title":"CacheKey","text":""},{"location":"api/simulation/cache/#idfkit.simulation.cache.CacheKey","title":"<code>idfkit.simulation.cache.CacheKey</code>  <code>dataclass</code>","text":"<p>Opaque cache key wrapping a hex digest string.</p> Source code in <code>src/idfkit/simulation/cache.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass CacheKey:\n    \"\"\"Opaque cache key wrapping a hex digest string.\"\"\"\n\n    hex_digest: str\n</code></pre>"},{"location":"api/simulation/cache/#idfkit.simulation.cache.CacheKey.hex_digest","title":"<code>hex_digest</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/cache/#default_simulation_cache_dir","title":"default_simulation_cache_dir","text":""},{"location":"api/simulation/cache/#idfkit.simulation.cache.default_simulation_cache_dir","title":"<code>idfkit.simulation.cache.default_simulation_cache_dir()</code>","text":"<p>Return the platform-appropriate cache directory for simulation results.</p> Source code in <code>src/idfkit/simulation/cache.py</code> <pre><code>def default_simulation_cache_dir() -&gt; Path:\n    \"\"\"Return the platform-appropriate cache directory for simulation results.\"\"\"\n    if sys.platform == \"win32\":\n        base = Path(os.environ.get(\"LOCALAPPDATA\", Path.home() / \"AppData\" / \"Local\"))\n        return base / \"idfkit\" / \"cache\" / \"simulation\"\n    if sys.platform == \"darwin\":\n        return Path.home() / \"Library\" / \"Caches\" / \"idfkit\" / \"simulation\"\n    # Linux / other POSIX\n    xdg = os.environ.get(\"XDG_CACHE_HOME\")\n    base = Path(xdg) if xdg else Path.home() / \".cache\"\n    return base / \"idfkit\" / \"simulation\"\n</code></pre>"},{"location":"api/simulation/expand/","title":"Preprocessing API","text":"<p>Functions for expanding template and preprocessor objects before simulation.</p> <p>EnergyPlus models may contain high-level template objects that must be expanded into their low-level equivalents before a simulation can run. Three preprocessors are supported:</p> Function Preprocessor Objects handled <code>expand_objects()</code> ExpandObjects <code>HVACTemplate:*</code> <code>run_slab_preprocessor()</code> Slab <code>GroundHeatTransfer:Slab:*</code> <code>run_basement_preprocessor()</code> Basement <code>GroundHeatTransfer:Basement:*</code> <code>run_preprocessing()</code> All of the above Combined pipeline <code>needs_ground_heat_preprocessing()</code> \u2014 Detection helper <p>All functions return a new <code>IDFDocument</code> \u2014 the original model is never mutated. If the model contains no objects for a given preprocessor, a copy is returned immediately without invoking any external process.</p> <p>Note</p> <p><code>simulate()</code> calls <code>run_preprocessing()</code> automatically when <code>expand_objects=True</code> (the default) and the model contains ground heat-transfer objects.  Use the individual functions only when you need to inspect or modify the preprocessed model before simulation.</p>"},{"location":"api/simulation/expand/#expand_objects","title":"expand_objects","text":""},{"location":"api/simulation/expand/#idfkit.simulation.expand.expand_objects","title":"<code>idfkit.simulation.expand.expand_objects(model, *, energyplus=None, timeout=120.0)</code>","text":"<p>Run the EnergyPlus ExpandObjects preprocessor and return the expanded document.</p> <p><code>ExpandObjects</code> replaces <code>HVACTemplate:*</code> objects with their fully specified low-level HVAC equivalents.  The original model is not mutated.</p> <p>If the document contains no <code>HVACTemplate:*</code> objects a copy is returned immediately without invoking the preprocessor (no EnergyPlus installation required).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model to expand.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, find_energyplus is used for auto-discovery.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds to wait for the preprocessor (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument containing the expanded</p> <code>IDFDocument</code> <p>objects.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation (and therefore no <code>ExpandObjects</code> executable) can be found.</p> <code>ExpandObjectsError</code> <p>If the <code>ExpandObjects</code> executable is missing from the installation or the preprocessor exits with an error.</p> Source code in <code>src/idfkit/simulation/expand.py</code> <pre><code>def expand_objects(\n    model: IDFDocument,\n    *,\n    energyplus: EnergyPlusConfig | None = None,\n    timeout: float = 120.0,\n) -&gt; IDFDocument:\n    \"\"\"Run the EnergyPlus *ExpandObjects* preprocessor and return the expanded document.\n\n    ``ExpandObjects`` replaces ``HVACTemplate:*`` objects with their fully\n    specified low-level HVAC equivalents.  The original *model* is **not**\n    mutated.\n\n    If the document contains no ``HVACTemplate:*`` objects a\n    [copy][idfkit.document.IDFDocument.copy] is returned immediately without\n    invoking the preprocessor (no EnergyPlus installation required).\n\n    Args:\n        model: The EnergyPlus model to expand.\n        energyplus: Pre-configured EnergyPlus installation.  If ``None``,\n            [find_energyplus][idfkit.simulation.config.find_energyplus] is used for\n            auto-discovery.\n        timeout: Maximum time in seconds to wait for the preprocessor\n            (default 120).\n\n    Returns:\n        A new [IDFDocument][idfkit.document.IDFDocument] containing the expanded\n        objects.\n\n    Raises:\n        EnergyPlusNotFoundError: If no EnergyPlus installation (and therefore\n            no ``ExpandObjects`` executable) can be found.\n        ExpandObjectsError: If the ``ExpandObjects`` executable is missing\n            from the installation or the preprocessor exits with an error.\n    \"\"\"\n    if not _needs_expansion(model):\n        return model.copy()\n\n    config = energyplus if energyplus is not None else find_energyplus()\n    run_dir = _prepare_run_dir(model)\n    try:\n        _run_expand_objects(config, run_dir, timeout=timeout)\n        return _parse_expanded(run_dir)\n    finally:\n        shutil.rmtree(run_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/simulation/expand/#run_slab_preprocessor","title":"run_slab_preprocessor","text":""},{"location":"api/simulation/expand/#idfkit.simulation.expand.run_slab_preprocessor","title":"<code>idfkit.simulation.expand.run_slab_preprocessor(model, *, energyplus=None, weather=None, timeout=120.0)</code>","text":"<p>Run the Slab ground heat-transfer preprocessor and return the expanded document.</p> <p>The workflow is:</p> <ol> <li><code>ExpandObjects</code> extracts <code>GroundHeatTransfer:Slab:*</code> objects from    the model into <code>GHTIn.idf</code>.</li> <li>The Slab preprocessor reads <code>GHTIn.idf</code> and computes monthly    ground surface temperatures, writing <code>SLABSurfaceTemps.TXT</code>.</li> <li>The resulting temperature schedules are appended to the expanded IDF.</li> </ol> <p>The original model is not mutated.</p> <p>If the document contains no <code>GroundHeatTransfer:Slab:*</code> objects a copy is returned immediately.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model containing <code>GroundHeatTransfer:Slab:*</code> objects.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, auto-discovery is used.</p> <code>None</code> <code>weather</code> <code>str | Path | None</code> <p>Path to a weather file (<code>.epw</code>).  Some Slab configurations may require weather data.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds for each subprocess invocation (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument with slab ground</p> <code>IDFDocument</code> <p>temperatures appended.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation is found.</p> <code>ExpandObjectsError</code> <p>If any preprocessor step fails.</p> Source code in <code>src/idfkit/simulation/expand.py</code> <pre><code>def run_slab_preprocessor(\n    model: IDFDocument,\n    *,\n    energyplus: EnergyPlusConfig | None = None,\n    weather: str | Path | None = None,\n    timeout: float = 120.0,\n) -&gt; IDFDocument:\n    \"\"\"Run the *Slab* ground heat-transfer preprocessor and return the expanded document.\n\n    The workflow is:\n\n    1. ``ExpandObjects`` extracts ``GroundHeatTransfer:Slab:*`` objects from\n       the model into ``GHTIn.idf``.\n    2. The **Slab** preprocessor reads ``GHTIn.idf`` and computes monthly\n       ground surface temperatures, writing ``SLABSurfaceTemps.TXT``.\n    3. The resulting temperature schedules are appended to the expanded IDF.\n\n    The original *model* is **not** mutated.\n\n    If the document contains no ``GroundHeatTransfer:Slab:*`` objects a\n    [copy][idfkit.document.IDFDocument.copy] is returned immediately.\n\n    Args:\n        model: The EnergyPlus model containing ``GroundHeatTransfer:Slab:*``\n            objects.\n        energyplus: Pre-configured EnergyPlus installation.  If ``None``,\n            auto-discovery is used.\n        weather: Path to a weather file (``.epw``).  Some Slab configurations\n            may require weather data.\n        timeout: Maximum time in seconds for each subprocess invocation\n            (default 120).\n\n    Returns:\n        A new [IDFDocument][idfkit.document.IDFDocument] with slab ground\n        temperatures appended.\n\n    Raises:\n        EnergyPlusNotFoundError: If no EnergyPlus installation is found.\n        ExpandObjectsError: If any preprocessor step fails.\n    \"\"\"\n    if not _has_slab_objects(model):\n        return model.copy()\n\n    config = energyplus if energyplus is not None else find_energyplus()\n    run_dir = _prepare_run_dir(model, weather=weather)\n    try:\n        _run_expand_objects(config, run_dir, timeout=timeout)\n\n        ght_input = run_dir / \"GHTIn.idf\"\n        if not ght_input.is_file():\n            msg = (\n                \"ExpandObjects did not produce GHTIn.idf.  \"\n                \"Ensure the model contains GroundHeatTransfer:Slab:* objects and \"\n                \"GroundHeatTransfer:Control has run_slab_preprocessor set to Yes.\"\n            )\n            raise ExpandObjectsError(msg, preprocessor=\"ExpandObjects\")\n\n        _run_slab_in_dir(config, run_dir, timeout=timeout)\n        return _parse_expanded(run_dir)\n    finally:\n        shutil.rmtree(run_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/simulation/expand/#run_basement_preprocessor","title":"run_basement_preprocessor","text":""},{"location":"api/simulation/expand/#idfkit.simulation.expand.run_basement_preprocessor","title":"<code>idfkit.simulation.expand.run_basement_preprocessor(model, *, energyplus=None, weather=None, timeout=120.0)</code>","text":"<p>Run the Basement ground heat-transfer preprocessor and return the expanded document.</p> <p>The workflow is:</p> <ol> <li><code>ExpandObjects</code> extracts <code>GroundHeatTransfer:Basement:*</code> objects    from the model into <code>BasementGHTIn.idf</code>.</li> <li>The Basement preprocessor reads <code>BasementGHTIn.idf</code> and computes    ground temperatures, writing <code>EPObjects.TXT</code>.</li> <li>The resulting boundary conditions are appended to the expanded IDF.</li> </ol> <p>The original model is not mutated.</p> <p>If the document contains no <code>GroundHeatTransfer:Basement:*</code> objects a copy is returned immediately.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model containing <code>GroundHeatTransfer:Basement:*</code> objects.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, auto-discovery is used.</p> <code>None</code> <code>weather</code> <code>str | Path | None</code> <p>Path to a weather file (<code>.epw</code>).  The Basement preprocessor requires weather data to compute ground temperatures.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds for each subprocess invocation (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument with basement ground</p> <code>IDFDocument</code> <p>temperatures appended.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation is found.</p> <code>ExpandObjectsError</code> <p>If any preprocessor step fails.</p> Source code in <code>src/idfkit/simulation/expand.py</code> <pre><code>def run_basement_preprocessor(\n    model: IDFDocument,\n    *,\n    energyplus: EnergyPlusConfig | None = None,\n    weather: str | Path | None = None,\n    timeout: float = 120.0,\n) -&gt; IDFDocument:\n    \"\"\"Run the *Basement* ground heat-transfer preprocessor and return the expanded document.\n\n    The workflow is:\n\n    1. ``ExpandObjects`` extracts ``GroundHeatTransfer:Basement:*`` objects\n       from the model into ``BasementGHTIn.idf``.\n    2. The **Basement** preprocessor reads ``BasementGHTIn.idf`` and computes\n       ground temperatures, writing ``EPObjects.TXT``.\n    3. The resulting boundary conditions are appended to the expanded IDF.\n\n    The original *model* is **not** mutated.\n\n    If the document contains no ``GroundHeatTransfer:Basement:*`` objects a\n    [copy][idfkit.document.IDFDocument.copy] is returned immediately.\n\n    Args:\n        model: The EnergyPlus model containing\n            ``GroundHeatTransfer:Basement:*`` objects.\n        energyplus: Pre-configured EnergyPlus installation.  If ``None``,\n            auto-discovery is used.\n        weather: Path to a weather file (``.epw``).  The Basement preprocessor\n            requires weather data to compute ground temperatures.\n        timeout: Maximum time in seconds for each subprocess invocation\n            (default 120).\n\n    Returns:\n        A new [IDFDocument][idfkit.document.IDFDocument] with basement ground\n        temperatures appended.\n\n    Raises:\n        EnergyPlusNotFoundError: If no EnergyPlus installation is found.\n        ExpandObjectsError: If any preprocessor step fails.\n    \"\"\"\n    if not _has_basement_objects(model):\n        return model.copy()\n\n    config = energyplus if energyplus is not None else find_energyplus()\n    run_dir = _prepare_run_dir(model, weather=weather)\n    try:\n        _run_expand_objects(config, run_dir, timeout=timeout)\n\n        ght_input = run_dir / \"BasementGHTIn.idf\"\n        if not ght_input.is_file():\n            msg = (\n                \"ExpandObjects did not produce BasementGHTIn.idf.  \"\n                \"Ensure the model contains GroundHeatTransfer:Basement:* objects and \"\n                \"GroundHeatTransfer:Control has run_basement_preprocessor set to Yes.\"\n            )\n            raise ExpandObjectsError(msg, preprocessor=\"ExpandObjects\")\n\n        _run_basement_in_dir(config, run_dir, timeout=timeout)\n        return _parse_expanded(run_dir)\n    finally:\n        shutil.rmtree(run_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/simulation/expand/#run_preprocessing","title":"run_preprocessing","text":""},{"location":"api/simulation/expand/#idfkit.simulation.expand.run_preprocessing","title":"<code>idfkit.simulation.expand.run_preprocessing(model, *, energyplus=None, weather=None, timeout=120.0)</code>","text":"<p>Run ExpandObjects and any required ground heat-transfer preprocessors.</p> <p>This is a convenience function that runs all needed preprocessors in a single call.  It runs ExpandObjects once, then checks which preprocessor input files were produced (<code>GHTIn.idf</code> and/or <code>BasementGHTIn.idf</code>) and runs the corresponding Fortran solvers.</p> <p>simulate calls this automatically when the model contains ground heat-transfer objects and expand_objects is <code>True</code>.  Call it directly only when you need to inspect or modify the preprocessed model before simulation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model to preprocess.</p> required <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation.  If <code>None</code>, auto-discovery is used.</p> <code>None</code> <code>weather</code> <code>str | Path | None</code> <p>Path to a weather file (<code>.epw</code>).  Required by the Slab and Basement solvers.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Maximum time in seconds for each subprocess invocation (default 120).</p> <code>120.0</code> <p>Returns:</p> Type Description <code>IDFDocument</code> <p>A new IDFDocument with all</p> <code>IDFDocument</code> <p>preprocessing applied.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no EnergyPlus installation is found.</p> <code>ExpandObjectsError</code> <p>If any preprocessor step fails.</p> Source code in <code>src/idfkit/simulation/expand.py</code> <pre><code>def run_preprocessing(\n    model: IDFDocument,\n    *,\n    energyplus: EnergyPlusConfig | None = None,\n    weather: str | Path | None = None,\n    timeout: float = 120.0,\n) -&gt; IDFDocument:\n    \"\"\"Run ExpandObjects and any required ground heat-transfer preprocessors.\n\n    This is a convenience function that runs all needed preprocessors in\n    a single call.  It runs ExpandObjects once, then checks which\n    preprocessor input files were produced (``GHTIn.idf`` and/or\n    ``BasementGHTIn.idf``) and runs the corresponding Fortran solvers.\n\n    [simulate][idfkit.simulation.runner.simulate] calls this automatically\n    when the model contains ground heat-transfer objects and\n    *expand_objects* is ``True``.  Call it directly only when you need to\n    inspect or modify the preprocessed model before simulation.\n\n    Args:\n        model: The EnergyPlus model to preprocess.\n        energyplus: Pre-configured EnergyPlus installation.  If ``None``,\n            auto-discovery is used.\n        weather: Path to a weather file (``.epw``).  Required by the\n            Slab and Basement solvers.\n        timeout: Maximum time in seconds for each subprocess invocation\n            (default 120).\n\n    Returns:\n        A new [IDFDocument][idfkit.document.IDFDocument] with all\n        preprocessing applied.\n\n    Raises:\n        EnergyPlusNotFoundError: If no EnergyPlus installation is found.\n        ExpandObjectsError: If any preprocessor step fails.\n    \"\"\"\n    config = energyplus if energyplus is not None else find_energyplus()\n    run_dir = _prepare_run_dir(model, weather=weather)\n    try:\n        _run_expand_objects(config, run_dir, timeout=timeout)\n\n        if (run_dir / \"GHTIn.idf\").is_file():\n            _run_slab_in_dir(config, run_dir, timeout=timeout)\n\n        if (run_dir / \"BasementGHTIn.idf\").is_file():\n            _run_basement_in_dir(config, run_dir, timeout=timeout)\n\n        return _parse_expanded(run_dir)\n    finally:\n        shutil.rmtree(run_dir, ignore_errors=True)\n</code></pre>"},{"location":"api/simulation/expand/#needs_ground_heat_preprocessing","title":"needs_ground_heat_preprocessing","text":""},{"location":"api/simulation/expand/#idfkit.simulation.expand.needs_ground_heat_preprocessing","title":"<code>idfkit.simulation.expand.needs_ground_heat_preprocessing(model)</code>","text":"<p>Return <code>True</code> if model contains ground heat-transfer objects.</p> <p>Checks for <code>GroundHeatTransfer:Slab:*</code> and <code>GroundHeatTransfer:Basement:*</code> objects that require the Slab or Basement preprocessor before simulation.</p> <p>This is used by simulate to decide whether to auto-run the preprocessing pipeline.</p> Source code in <code>src/idfkit/simulation/expand.py</code> <pre><code>def needs_ground_heat_preprocessing(model: IDFDocument) -&gt; bool:\n    \"\"\"Return ``True`` if *model* contains ground heat-transfer objects.\n\n    Checks for ``GroundHeatTransfer:Slab:*`` and\n    ``GroundHeatTransfer:Basement:*`` objects that require the Slab or\n    Basement preprocessor before simulation.\n\n    This is used by [simulate][idfkit.simulation.runner.simulate] to decide\n    whether to auto-run the preprocessing pipeline.\n    \"\"\"\n    return _has_slab_objects(model) or _has_basement_objects(model)\n</code></pre>"},{"location":"api/simulation/expand/#error-handling","title":"Error Handling","text":"<p>All three functions raise <code>ExpandObjectsError</code> on failure.  The exception carries structured fields for programmatic access:</p> <pre><code>from idfkit.exceptions import ExpandObjectsError\n\ntry:\n    expanded = expand_objects(model)\nexcept ExpandObjectsError as e:\n    print(e.preprocessor)  # \"ExpandObjects\", \"Slab\", or \"Basement\"\n    print(e.exit_code)     # Process exit code, or None for timeout/OS error\n    print(e.stderr)        # Captured stderr (truncated to 500 chars)\n</code></pre>"},{"location":"api/simulation/expand/#error-modes","title":"Error modes","text":"Failure <code>preprocessor</code> <code>exit_code</code> <code>stderr</code> Executable not found Name <code>None</code> <code>None</code> Process timeout Name <code>None</code> Partial output OS error (permissions) Name <code>None</code> <code>None</code> Process crash (SIGSEGV) Name <code>139</code> Signal info Empty output (solver failure) Name <code>0</code> Solver output Fatal PreprocessorMessage Name <code>0</code> Error message Missing output file Name Exit code Process stderr"},{"location":"api/simulation/fs/","title":"File Systems API","text":"<p>Pluggable storage backends for simulation I/O.</p>"},{"location":"api/simulation/fs/#filesystem-protocol","title":"FileSystem Protocol","text":""},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem","title":"<code>idfkit.simulation.fs.FileSystem</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for file system operations used by the simulation module.</p> <p>All methods accept <code>str | Path</code> for path arguments.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>@runtime_checkable\nclass FileSystem(Protocol):\n    \"\"\"Protocol for file system operations used by the simulation module.\n\n    All methods accept ``str | Path`` for path arguments.\n    \"\"\"\n\n    def read_bytes(self, path: str | Path) -&gt; bytes:\n        \"\"\"Read a file as raw bytes.\n\n        Args:\n            path: Path to the file.\n\n        Returns:\n            The file contents as bytes.\n        \"\"\"\n        ...\n\n    def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n        \"\"\"Write raw bytes to a file.\n\n        Args:\n            path: Path to the file.\n            data: Bytes to write.\n        \"\"\"\n        ...\n\n    def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n        \"\"\"Read a file as text.\n\n        Args:\n            path: Path to the file.\n            encoding: Text encoding (default ``\"utf-8\"``).\n\n        Returns:\n            The file contents as a string.\n        \"\"\"\n        ...\n\n    def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n        \"\"\"Write text to a file.\n\n        Args:\n            path: Path to the file.\n            text: Text to write.\n            encoding: Text encoding (default ``\"utf-8\"``).\n        \"\"\"\n        ...\n\n    def exists(self, path: str | Path) -&gt; bool:\n        \"\"\"Check whether a file exists.\n\n        Args:\n            path: Path to check.\n\n        Returns:\n            True if the file exists.\n        \"\"\"\n        ...\n\n    def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n        \"\"\"Create directories recursively.\n\n        Args:\n            path: Directory path to create.\n            exist_ok: If True, do not raise if the directory already exists.\n        \"\"\"\n        ...\n\n    def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n        \"\"\"Copy a file from *src* to *dst*.\n\n        Args:\n            src: Source file path.\n            dst: Destination file path.\n        \"\"\"\n        ...\n\n    def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n        \"\"\"List files matching a glob pattern under *path*.\n\n        Args:\n            path: Base directory.\n            pattern: Glob pattern (e.g. ``\"*.sql\"``).\n\n        Returns:\n            List of matching file paths as strings.\n        \"\"\"\n        ...\n\n    def remove(self, path: str | Path) -&gt; None:\n        \"\"\"Remove a file.\n\n        Args:\n            path: Path to the file to remove.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.read_bytes","title":"<code>read_bytes(path)</code>","text":"<p>Read a file as raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The file contents as bytes.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def read_bytes(self, path: str | Path) -&gt; bytes:\n    \"\"\"Read a file as raw bytes.\n\n    Args:\n        path: Path to the file.\n\n    Returns:\n        The file contents as bytes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>","text":"<p>Write raw bytes to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>data</code> <code>bytes</code> <p>Bytes to write.</p> required Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n    \"\"\"Write raw bytes to a file.\n\n    Args:\n        path: Path to the file.\n        data: Bytes to write.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>","text":"<p>Read a file as text.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>encoding</code> <code>str</code> <p>Text encoding (default <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>str</code> <p>The file contents as a string.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n    \"\"\"Read a file as text.\n\n    Args:\n        path: Path to the file.\n        encoding: Text encoding (default ``\"utf-8\"``).\n\n    Returns:\n        The file contents as a string.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>","text":"<p>Write text to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>text</code> <code>str</code> <p>Text to write.</p> required <code>encoding</code> <code>str</code> <p>Text encoding (default <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"Write text to a file.\n\n    Args:\n        path: Path to the file.\n        text: Text to write.\n        encoding: Text encoding (default ``\"utf-8\"``).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.exists","title":"<code>exists(path)</code>","text":"<p>Check whether a file exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def exists(self, path: str | Path) -&gt; bool:\n    \"\"\"Check whether a file exists.\n\n    Args:\n        path: Path to check.\n\n    Returns:\n        True if the file exists.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>","text":"<p>Create directories recursively.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Directory path to create.</p> required <code>exist_ok</code> <code>bool</code> <p>If True, do not raise if the directory already exists.</p> <code>False</code> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n    \"\"\"Create directories recursively.\n\n    Args:\n        path: Directory path to create.\n        exist_ok: If True, do not raise if the directory already exists.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.copy","title":"<code>copy(src, dst)</code>","text":"<p>Copy a file from src to dst.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str | Path</code> <p>Source file path.</p> required <code>dst</code> <code>str | Path</code> <p>Destination file path.</p> required Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n    \"\"\"Copy a file from *src* to *dst*.\n\n    Args:\n        src: Source file path.\n        dst: Destination file path.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.glob","title":"<code>glob(path, pattern)</code>","text":"<p>List files matching a glob pattern under path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Base directory.</p> required <code>pattern</code> <code>str</code> <p>Glob pattern (e.g. <code>\"*.sql\"</code>).</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of matching file paths as strings.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n    \"\"\"List files matching a glob pattern under *path*.\n\n    Args:\n        path: Base directory.\n        pattern: Glob pattern (e.g. ``\"*.sql\"``).\n\n    Returns:\n        List of matching file paths as strings.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.FileSystem.remove","title":"<code>remove(path)</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file to remove.</p> required Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def remove(self, path: str | Path) -&gt; None:\n    \"\"\"Remove a file.\n\n    Args:\n        path: Path to the file to remove.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#asyncfilesystem-protocol","title":"AsyncFileSystem Protocol","text":""},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem","title":"<code>idfkit.simulation.fs.AsyncFileSystem</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for async file system operations used by the async simulation module.</p> <p>This is the async counterpart to FileSystem.  Use this with async_simulate and the async batch functions to avoid blocking the event loop during file I/O \u2014 especially important for network-backed storage like S3.</p> <p>All methods accept <code>str | Path</code> for path arguments.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>@runtime_checkable\nclass AsyncFileSystem(Protocol):\n    \"\"\"Protocol for async file system operations used by the async simulation module.\n\n    This is the async counterpart to [FileSystem][idfkit.simulation.fs.FileSystem].  Use this with\n    [async_simulate][idfkit.simulation.async_runner.async_simulate] and the async batch\n    functions to avoid blocking the event loop during file I/O \u2014 especially\n    important for network-backed storage like S3.\n\n    All methods accept ``str | Path`` for path arguments.\n    \"\"\"\n\n    async def read_bytes(self, path: str | Path) -&gt; bytes:\n        \"\"\"Read a file as raw bytes.\n\n        Args:\n            path: Path to the file.\n\n        Returns:\n            The file contents as bytes.\n        \"\"\"\n        ...\n\n    async def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n        \"\"\"Write raw bytes to a file.\n\n        Args:\n            path: Path to the file.\n            data: Bytes to write.\n        \"\"\"\n        ...\n\n    async def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n        \"\"\"Read a file as text.\n\n        Args:\n            path: Path to the file.\n            encoding: Text encoding (default ``\"utf-8\"``).\n\n        Returns:\n            The file contents as a string.\n        \"\"\"\n        ...\n\n    async def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n        \"\"\"Write text to a file.\n\n        Args:\n            path: Path to the file.\n            text: Text to write.\n            encoding: Text encoding (default ``\"utf-8\"``).\n        \"\"\"\n        ...\n\n    async def exists(self, path: str | Path) -&gt; bool:\n        \"\"\"Check whether a file exists.\n\n        Args:\n            path: Path to check.\n\n        Returns:\n            True if the file exists.\n        \"\"\"\n        ...\n\n    async def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n        \"\"\"Create directories recursively.\n\n        Args:\n            path: Directory path to create.\n            exist_ok: If True, do not raise if the directory already exists.\n        \"\"\"\n        ...\n\n    async def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n        \"\"\"Copy a file from *src* to *dst*.\n\n        Args:\n            src: Source file path.\n            dst: Destination file path.\n        \"\"\"\n        ...\n\n    async def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n        \"\"\"List files matching a glob pattern under *path*.\n\n        Args:\n            path: Base directory.\n            pattern: Glob pattern (e.g. ``\"*.sql\"``).\n\n        Returns:\n            List of matching file paths as strings.\n        \"\"\"\n        ...\n\n    async def remove(self, path: str | Path) -&gt; None:\n        \"\"\"Remove a file.\n\n        Args:\n            path: Path to the file to remove.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.read_bytes","title":"<code>read_bytes(path)</code>  <code>async</code>","text":"<p>Read a file as raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The file contents as bytes.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def read_bytes(self, path: str | Path) -&gt; bytes:\n    \"\"\"Read a file as raw bytes.\n\n    Args:\n        path: Path to the file.\n\n    Returns:\n        The file contents as bytes.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>  <code>async</code>","text":"<p>Write raw bytes to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>data</code> <code>bytes</code> <p>Bytes to write.</p> required Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n    \"\"\"Write raw bytes to a file.\n\n    Args:\n        path: Path to the file.\n        data: Bytes to write.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>  <code>async</code>","text":"<p>Read a file as text.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>encoding</code> <code>str</code> <p>Text encoding (default <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> <p>Returns:</p> Type Description <code>str</code> <p>The file contents as a string.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n    \"\"\"Read a file as text.\n\n    Args:\n        path: Path to the file.\n        encoding: Text encoding (default ``\"utf-8\"``).\n\n    Returns:\n        The file contents as a string.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>  <code>async</code>","text":"<p>Write text to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file.</p> required <code>text</code> <code>str</code> <p>Text to write.</p> required <code>encoding</code> <code>str</code> <p>Text encoding (default <code>\"utf-8\"</code>).</p> <code>'utf-8'</code> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"Write text to a file.\n\n    Args:\n        path: Path to the file.\n        text: Text to write.\n        encoding: Text encoding (default ``\"utf-8\"``).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.exists","title":"<code>exists(path)</code>  <code>async</code>","text":"<p>Check whether a file exists.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file exists.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def exists(self, path: str | Path) -&gt; bool:\n    \"\"\"Check whether a file exists.\n\n    Args:\n        path: Path to check.\n\n    Returns:\n        True if the file exists.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>  <code>async</code>","text":"<p>Create directories recursively.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Directory path to create.</p> required <code>exist_ok</code> <code>bool</code> <p>If True, do not raise if the directory already exists.</p> <code>False</code> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n    \"\"\"Create directories recursively.\n\n    Args:\n        path: Directory path to create.\n        exist_ok: If True, do not raise if the directory already exists.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.copy","title":"<code>copy(src, dst)</code>  <code>async</code>","text":"<p>Copy a file from src to dst.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str | Path</code> <p>Source file path.</p> required <code>dst</code> <code>str | Path</code> <p>Destination file path.</p> required Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n    \"\"\"Copy a file from *src* to *dst*.\n\n    Args:\n        src: Source file path.\n        dst: Destination file path.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.glob","title":"<code>glob(path, pattern)</code>  <code>async</code>","text":"<p>List files matching a glob pattern under path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Base directory.</p> required <code>pattern</code> <code>str</code> <p>Glob pattern (e.g. <code>\"*.sql\"</code>).</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of matching file paths as strings.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n    \"\"\"List files matching a glob pattern under *path*.\n\n    Args:\n        path: Base directory.\n        pattern: Glob pattern (e.g. ``\"*.sql\"``).\n\n    Returns:\n        List of matching file paths as strings.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncFileSystem.remove","title":"<code>remove(path)</code>  <code>async</code>","text":"<p>Remove a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the file to remove.</p> required Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def remove(self, path: str | Path) -&gt; None:\n    \"\"\"Remove a file.\n\n    Args:\n        path: Path to the file to remove.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/fs/#localfilesystem","title":"LocalFileSystem","text":""},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem","title":"<code>idfkit.simulation.fs.LocalFileSystem</code>","text":"<p>File system implementation backed by pathlib and shutil.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>class LocalFileSystem:\n    \"\"\"File system implementation backed by [pathlib][] and [shutil][].\"\"\"\n\n    def read_bytes(self, path: str | Path) -&gt; bytes:\n        \"\"\"Read a file as raw bytes.\"\"\"\n        return Path(path).read_bytes()\n\n    def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n        \"\"\"Write raw bytes to a file.\"\"\"\n        Path(path).write_bytes(data)\n\n    def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n        \"\"\"Read a file as text.\"\"\"\n        return Path(path).read_text(encoding=encoding)\n\n    def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n        \"\"\"Write text to a file.\"\"\"\n        Path(path).write_text(text, encoding=encoding)\n\n    def exists(self, path: str | Path) -&gt; bool:\n        \"\"\"Check whether a file exists.\"\"\"\n        return Path(path).exists()\n\n    def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n        \"\"\"Create directories recursively.\"\"\"\n        Path(path).mkdir(parents=True, exist_ok=exist_ok)\n\n    def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n        \"\"\"Copy a file from *src* to *dst*.\"\"\"\n        shutil.copy2(str(src), str(dst))\n\n    def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n        \"\"\"List files matching a glob pattern under *path*.\"\"\"\n        return [str(p) for p in Path(path).glob(pattern)]\n\n    def remove(self, path: str | Path) -&gt; None:\n        \"\"\"Remove a file.\"\"\"\n        Path(path).unlink()\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.read_bytes","title":"<code>read_bytes(path)</code>","text":"<p>Read a file as raw bytes.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def read_bytes(self, path: str | Path) -&gt; bytes:\n    \"\"\"Read a file as raw bytes.\"\"\"\n    return Path(path).read_bytes()\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>","text":"<p>Write raw bytes to a file.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n    \"\"\"Write raw bytes to a file.\"\"\"\n    Path(path).write_bytes(data)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>","text":"<p>Read a file as text.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n    \"\"\"Read a file as text.\"\"\"\n    return Path(path).read_text(encoding=encoding)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>","text":"<p>Write text to a file.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"Write text to a file.\"\"\"\n    Path(path).write_text(text, encoding=encoding)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.exists","title":"<code>exists(path)</code>","text":"<p>Check whether a file exists.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def exists(self, path: str | Path) -&gt; bool:\n    \"\"\"Check whether a file exists.\"\"\"\n    return Path(path).exists()\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>","text":"<p>Create directories recursively.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n    \"\"\"Create directories recursively.\"\"\"\n    Path(path).mkdir(parents=True, exist_ok=exist_ok)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.copy","title":"<code>copy(src, dst)</code>","text":"<p>Copy a file from src to dst.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n    \"\"\"Copy a file from *src* to *dst*.\"\"\"\n    shutil.copy2(str(src), str(dst))\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.glob","title":"<code>glob(path, pattern)</code>","text":"<p>List files matching a glob pattern under path.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n    \"\"\"List files matching a glob pattern under *path*.\"\"\"\n    return [str(p) for p in Path(path).glob(pattern)]\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.LocalFileSystem.remove","title":"<code>remove(path)</code>","text":"<p>Remove a file.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def remove(self, path: str | Path) -&gt; None:\n    \"\"\"Remove a file.\"\"\"\n    Path(path).unlink()\n</code></pre>"},{"location":"api/simulation/fs/#asynclocalfilesystem","title":"AsyncLocalFileSystem","text":""},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem","title":"<code>idfkit.simulation.fs.AsyncLocalFileSystem</code>","text":"<p>Non-blocking local file system using asyncio.to_thread.</p> <p>Wraps LocalFileSystem so that each blocking I/O call runs in the default executor, keeping the event loop free.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>class AsyncLocalFileSystem:\n    \"\"\"Non-blocking local file system using [asyncio.to_thread][].\n\n    Wraps [LocalFileSystem][idfkit.simulation.fs.LocalFileSystem] so that each blocking I/O call runs in\n    the default executor, keeping the event loop free.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._sync = LocalFileSystem()\n\n    async def read_bytes(self, path: str | Path) -&gt; bytes:\n        \"\"\"Read a file as raw bytes without blocking the event loop.\"\"\"\n        return await asyncio.to_thread(self._sync.read_bytes, path)\n\n    async def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n        \"\"\"Write raw bytes to a file without blocking the event loop.\"\"\"\n        await asyncio.to_thread(self._sync.write_bytes, path, data)\n\n    async def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n        \"\"\"Read a file as text without blocking the event loop.\"\"\"\n        return await asyncio.to_thread(self._sync.read_text, path, encoding)\n\n    async def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n        \"\"\"Write text to a file without blocking the event loop.\"\"\"\n        await asyncio.to_thread(self._sync.write_text, path, text, encoding)\n\n    async def exists(self, path: str | Path) -&gt; bool:\n        \"\"\"Check whether a file exists without blocking the event loop.\"\"\"\n        return await asyncio.to_thread(self._sync.exists, path)\n\n    async def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n        \"\"\"Create directories recursively without blocking the event loop.\"\"\"\n        await asyncio.to_thread(self._sync.makedirs, path, exist_ok=exist_ok)\n\n    async def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n        \"\"\"Copy a file without blocking the event loop.\"\"\"\n        await asyncio.to_thread(self._sync.copy, src, dst)\n\n    async def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n        \"\"\"List files matching a glob pattern without blocking the event loop.\"\"\"\n        return await asyncio.to_thread(self._sync.glob, path, pattern)\n\n    async def remove(self, path: str | Path) -&gt; None:\n        \"\"\"Remove a file without blocking the event loop.\"\"\"\n        await asyncio.to_thread(self._sync.remove, path)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.read_bytes","title":"<code>read_bytes(path)</code>  <code>async</code>","text":"<p>Read a file as raw bytes without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def read_bytes(self, path: str | Path) -&gt; bytes:\n    \"\"\"Read a file as raw bytes without blocking the event loop.\"\"\"\n    return await asyncio.to_thread(self._sync.read_bytes, path)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>  <code>async</code>","text":"<p>Write raw bytes to a file without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n    \"\"\"Write raw bytes to a file without blocking the event loop.\"\"\"\n    await asyncio.to_thread(self._sync.write_bytes, path, data)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>  <code>async</code>","text":"<p>Read a file as text without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n    \"\"\"Read a file as text without blocking the event loop.\"\"\"\n    return await asyncio.to_thread(self._sync.read_text, path, encoding)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>  <code>async</code>","text":"<p>Write text to a file without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"Write text to a file without blocking the event loop.\"\"\"\n    await asyncio.to_thread(self._sync.write_text, path, text, encoding)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.exists","title":"<code>exists(path)</code>  <code>async</code>","text":"<p>Check whether a file exists without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def exists(self, path: str | Path) -&gt; bool:\n    \"\"\"Check whether a file exists without blocking the event loop.\"\"\"\n    return await asyncio.to_thread(self._sync.exists, path)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>  <code>async</code>","text":"<p>Create directories recursively without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n    \"\"\"Create directories recursively without blocking the event loop.\"\"\"\n    await asyncio.to_thread(self._sync.makedirs, path, exist_ok=exist_ok)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.copy","title":"<code>copy(src, dst)</code>  <code>async</code>","text":"<p>Copy a file without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n    \"\"\"Copy a file without blocking the event loop.\"\"\"\n    await asyncio.to_thread(self._sync.copy, src, dst)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.glob","title":"<code>glob(path, pattern)</code>  <code>async</code>","text":"<p>List files matching a glob pattern without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n    \"\"\"List files matching a glob pattern without blocking the event loop.\"\"\"\n    return await asyncio.to_thread(self._sync.glob, path, pattern)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncLocalFileSystem.remove","title":"<code>remove(path)</code>  <code>async</code>","text":"<p>Remove a file without blocking the event loop.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def remove(self, path: str | Path) -&gt; None:\n    \"\"\"Remove a file without blocking the event loop.\"\"\"\n    await asyncio.to_thread(self._sync.remove, path)\n</code></pre>"},{"location":"api/simulation/fs/#s3filesystem","title":"S3FileSystem","text":""},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem","title":"<code>idfkit.simulation.fs.S3FileSystem</code>","text":"<p>File system implementation backed by Amazon S3.</p> <p>Requires the <code>boto3</code> package (install via <code>pip install idfkit[s3]</code>).</p> <p>This backend enables cloud-native simulation workflows where results are stored directly in S3 for later retrieval. EnergyPlus runs locally in a temporary directory, then results are uploaded to S3 after completion.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>prefix</code> <code>str</code> <p>Optional key prefix prepended to all paths. Use this to namespace simulations (e.g., <code>\"project-x/batch-42/\"</code>).</p> <code>''</code> <code>**boto_kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>boto3.client(\"s3\", ...)</code>. Common options include:</p> <ul> <li><code>region_name</code>: AWS region (e.g., <code>\"us-east-1\"</code>)</li> <li><code>endpoint_url</code>: Custom endpoint for S3-compatible services   (MinIO, LocalStack, etc.)</li> <li><code>aws_access_key_id</code>, <code>aws_secret_access_key</code>: Explicit   credentials (normally use IAM roles or environment variables)</li> </ul> <code>{}</code> <p>Examples:</p> <pre><code># Basic usage\nfs = S3FileSystem(bucket=\"my-bucket\", prefix=\"simulations/\")\n\n# With MinIO (S3-compatible)\nfs = S3FileSystem(\n    bucket=\"local-bucket\",\n    endpoint_url=\"http://localhost:9000\",\n    aws_access_key_id=\"minioadmin\",\n    aws_secret_access_key=\"minioadmin\",\n)\n\n# Use with simulate()\nresult = simulate(model, weather, output_dir=\"run-001\", fs=fs)\n</code></pre> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>class S3FileSystem:\n    \"\"\"File system implementation backed by Amazon S3.\n\n    Requires the ``boto3`` package (install via ``pip install idfkit[s3]``).\n\n    This backend enables cloud-native simulation workflows where results are\n    stored directly in S3 for later retrieval. EnergyPlus runs locally in a\n    temporary directory, then results are uploaded to S3 after completion.\n\n    Args:\n        bucket: S3 bucket name.\n        prefix: Optional key prefix prepended to all paths. Use this to\n            namespace simulations (e.g., ``\"project-x/batch-42/\"``).\n        **boto_kwargs: Additional keyword arguments passed to\n            ``boto3.client(\"s3\", ...)``. Common options include:\n\n            - ``region_name``: AWS region (e.g., ``\"us-east-1\"``)\n            - ``endpoint_url``: Custom endpoint for S3-compatible services\n              (MinIO, LocalStack, etc.)\n            - ``aws_access_key_id``, ``aws_secret_access_key``: Explicit\n              credentials (normally use IAM roles or environment variables)\n\n    Examples:\n        ```python\n        # Basic usage\n        fs = S3FileSystem(bucket=\"my-bucket\", prefix=\"simulations/\")\n\n        # With MinIO (S3-compatible)\n        fs = S3FileSystem(\n            bucket=\"local-bucket\",\n            endpoint_url=\"http://localhost:9000\",\n            aws_access_key_id=\"minioadmin\",\n            aws_secret_access_key=\"minioadmin\",\n        )\n\n        # Use with simulate()\n        result = simulate(model, weather, output_dir=\"run-001\", fs=fs)\n        ```\n    \"\"\"\n\n    def __init__(self, bucket: str, prefix: str = \"\", **boto_kwargs: Any) -&gt; None:\n        try:\n            import boto3  # type: ignore[import-not-found]\n        except ImportError:\n            msg = \"boto3 is required for S3FileSystem. Install it with: pip install idfkit[s3]\"\n            raise ImportError(msg) from None\n        _boto3: Any = boto3\n        self._bucket = bucket\n        self._prefix = prefix.strip(\"/\")\n        self._client: S3Client = _boto3.client(\"s3\", **boto_kwargs)\n\n    def _key(self, path: str | Path) -&gt; str:\n        \"\"\"Build an S3 key by prepending the configured prefix.\n\n        Args:\n            path: Logical file path.\n\n        Returns:\n            The full S3 object key.\n        \"\"\"\n        raw = str(path).lstrip(\"/\")\n        if self._prefix:\n            return f\"{self._prefix}/{raw}\"\n        return raw\n\n    def read_bytes(self, path: str | Path) -&gt; bytes:\n        \"\"\"Read a file as raw bytes from S3.\"\"\"\n        resp = self._client.get_object(Bucket=self._bucket, Key=self._key(path))\n        return resp[\"Body\"].read()  # type: ignore[no-any-return]\n\n    def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n        \"\"\"Write raw bytes to S3.\"\"\"\n        self._client.put_object(Bucket=self._bucket, Key=self._key(path), Body=data)\n\n    def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n        \"\"\"Read a file as text from S3.\"\"\"\n        return self.read_bytes(path).decode(encoding)\n\n    def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n        \"\"\"Write text to S3.\"\"\"\n        self.write_bytes(path, text.encode(encoding))\n\n    def exists(self, path: str | Path) -&gt; bool:\n        \"\"\"Check whether an object exists in S3.\"\"\"\n        try:\n            self._client.head_object(Bucket=self._bucket, Key=self._key(path))\n        except Exception:\n            return False\n        return True\n\n    def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n        \"\"\"No-op \u2014 S3 has no directory concept.\"\"\"\n\n    def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n        \"\"\"Copy an object within the same bucket.\"\"\"\n        self._client.copy_object(\n            Bucket=self._bucket,\n            CopySource={\"Bucket\": self._bucket, \"Key\": self._key(src)},\n            Key=self._key(dst),\n        )\n\n    def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n        \"\"\"List objects matching a glob pattern under *path*.\n\n        Returns logical paths (without the configured S3 prefix) so that\n        they can be passed back to other ``S3FileSystem`` methods which\n        prepend the prefix automatically via ``_key()``.\n        \"\"\"\n        prefix = self._key(path).rstrip(\"/\") + \"/\"\n        paginator = self._client.get_paginator(\"list_objects_v2\")\n        matches: list[str] = []\n        # Compute how much of the key is the S3 prefix, so we can strip it.\n        logical_base = str(path).strip(\"/\")\n        for page in paginator.paginate(Bucket=self._bucket, Prefix=prefix):\n            for obj in page.get(\"Contents\", []):\n                key = obj.get(\"Key\", \"\")\n                if not key:\n                    continue\n                # Match only the filename portion against the pattern\n                name = key[len(prefix) :]\n                if fnmatch.fnmatch(name, pattern):\n                    # Return logical path (base/name) that _key() can prefix\n                    matches.append(f\"{logical_base}/{name}\" if logical_base else name)\n        return matches\n\n    def remove(self, path: str | Path) -&gt; None:\n        \"\"\"Delete an object from S3.\"\"\"\n        self._client.delete_object(Bucket=self._bucket, Key=self._key(path))\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.read_bytes","title":"<code>read_bytes(path)</code>","text":"<p>Read a file as raw bytes from S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def read_bytes(self, path: str | Path) -&gt; bytes:\n    \"\"\"Read a file as raw bytes from S3.\"\"\"\n    resp = self._client.get_object(Bucket=self._bucket, Key=self._key(path))\n    return resp[\"Body\"].read()  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>","text":"<p>Write raw bytes to S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n    \"\"\"Write raw bytes to S3.\"\"\"\n    self._client.put_object(Bucket=self._bucket, Key=self._key(path), Body=data)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>","text":"<p>Read a file as text from S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n    \"\"\"Read a file as text from S3.\"\"\"\n    return self.read_bytes(path).decode(encoding)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>","text":"<p>Write text to S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"Write text to S3.\"\"\"\n    self.write_bytes(path, text.encode(encoding))\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.exists","title":"<code>exists(path)</code>","text":"<p>Check whether an object exists in S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def exists(self, path: str | Path) -&gt; bool:\n    \"\"\"Check whether an object exists in S3.\"\"\"\n    try:\n        self._client.head_object(Bucket=self._bucket, Key=self._key(path))\n    except Exception:\n        return False\n    return True\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>","text":"<p>No-op \u2014 S3 has no directory concept.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n    \"\"\"No-op \u2014 S3 has no directory concept.\"\"\"\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.copy","title":"<code>copy(src, dst)</code>","text":"<p>Copy an object within the same bucket.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n    \"\"\"Copy an object within the same bucket.\"\"\"\n    self._client.copy_object(\n        Bucket=self._bucket,\n        CopySource={\"Bucket\": self._bucket, \"Key\": self._key(src)},\n        Key=self._key(dst),\n    )\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.glob","title":"<code>glob(path, pattern)</code>","text":"<p>List objects matching a glob pattern under path.</p> <p>Returns logical paths (without the configured S3 prefix) so that they can be passed back to other <code>S3FileSystem</code> methods which prepend the prefix automatically via <code>_key()</code>.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n    \"\"\"List objects matching a glob pattern under *path*.\n\n    Returns logical paths (without the configured S3 prefix) so that\n    they can be passed back to other ``S3FileSystem`` methods which\n    prepend the prefix automatically via ``_key()``.\n    \"\"\"\n    prefix = self._key(path).rstrip(\"/\") + \"/\"\n    paginator = self._client.get_paginator(\"list_objects_v2\")\n    matches: list[str] = []\n    # Compute how much of the key is the S3 prefix, so we can strip it.\n    logical_base = str(path).strip(\"/\")\n    for page in paginator.paginate(Bucket=self._bucket, Prefix=prefix):\n        for obj in page.get(\"Contents\", []):\n            key = obj.get(\"Key\", \"\")\n            if not key:\n                continue\n            # Match only the filename portion against the pattern\n            name = key[len(prefix) :]\n            if fnmatch.fnmatch(name, pattern):\n                # Return logical path (base/name) that _key() can prefix\n                matches.append(f\"{logical_base}/{name}\" if logical_base else name)\n    return matches\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.S3FileSystem.remove","title":"<code>remove(path)</code>","text":"<p>Delete an object from S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>def remove(self, path: str | Path) -&gt; None:\n    \"\"\"Delete an object from S3.\"\"\"\n    self._client.delete_object(Bucket=self._bucket, Key=self._key(path))\n</code></pre>"},{"location":"api/simulation/fs/#asyncs3filesystem","title":"AsyncS3FileSystem","text":""},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem","title":"<code>idfkit.simulation.fs.AsyncS3FileSystem</code>","text":"<p>Async file system implementation backed by Amazon S3 via <code>aiobotocore</code>.</p> <p>Requires the <code>aiobotocore</code> package (install via <code>pip install idfkit[async-s3]</code>).</p> <p>This is the non-blocking counterpart to S3FileSystem.  Use it with async_simulate and the async batch functions to avoid blocking the event loop during S3 I/O.</p> <p>The client must be initialised via the async context manager protocol:</p> <pre><code>```python\nasync with AsyncS3FileSystem(bucket=\"my-bucket\") as fs:\n    result = await async_simulate(model, weather, output_dir=\"run-001\", fs=fs)\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>S3 bucket name.</p> required <code>prefix</code> <code>str</code> <p>Optional key prefix prepended to all paths. Use this to namespace simulations (e.g., <code>\"project-x/batch-42/\"</code>).</p> <code>''</code> <code>**boto_kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to <code>session.create_client(\"s3\", ...)</code>. Common options include:</p> <ul> <li><code>region_name</code>: AWS region (e.g., <code>\"us-east-1\"</code>)</li> <li><code>endpoint_url</code>: Custom endpoint for S3-compatible services   (MinIO, LocalStack, etc.)</li> <li><code>aws_access_key_id</code>, <code>aws_secret_access_key</code>: Explicit   credentials (normally use IAM roles or environment variables)</li> </ul> <code>{}</code> <p>Examples:</p> <pre><code>from idfkit.simulation import AsyncS3FileSystem, async_simulate\n\nasync with AsyncS3FileSystem(bucket=\"my-bucket\", prefix=\"sims/\") as fs:\n    result = await async_simulate(model, weather, output_dir=\"run-001\", fs=fs)\n    errors = await result.async_errors()\n</code></pre> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>class AsyncS3FileSystem:\n    \"\"\"Async file system implementation backed by Amazon S3 via ``aiobotocore``.\n\n    Requires the ``aiobotocore`` package\n    (install via ``pip install idfkit[async-s3]``).\n\n    This is the non-blocking counterpart to [S3FileSystem][idfkit.simulation.fs.S3FileSystem].  Use it\n    with [async_simulate][idfkit.simulation.async_runner.async_simulate] and the\n    async batch functions to avoid blocking the event loop during S3 I/O.\n\n    The client must be initialised via the async context manager protocol:\n\n        ```python\n        async with AsyncS3FileSystem(bucket=\"my-bucket\") as fs:\n            result = await async_simulate(model, weather, output_dir=\"run-001\", fs=fs)\n        ```\n\n    Args:\n        bucket: S3 bucket name.\n        prefix: Optional key prefix prepended to all paths. Use this to\n            namespace simulations (e.g., ``\"project-x/batch-42/\"``).\n        **boto_kwargs: Additional keyword arguments passed to\n            ``session.create_client(\"s3\", ...)``. Common options include:\n\n            - ``region_name``: AWS region (e.g., ``\"us-east-1\"``)\n            - ``endpoint_url``: Custom endpoint for S3-compatible services\n              (MinIO, LocalStack, etc.)\n            - ``aws_access_key_id``, ``aws_secret_access_key``: Explicit\n              credentials (normally use IAM roles or environment variables)\n\n    Examples:\n        ```python\n        from idfkit.simulation import AsyncS3FileSystem, async_simulate\n\n        async with AsyncS3FileSystem(bucket=\"my-bucket\", prefix=\"sims/\") as fs:\n            result = await async_simulate(model, weather, output_dir=\"run-001\", fs=fs)\n            errors = await result.async_errors()\n        ```\n    \"\"\"\n\n    def __init__(self, bucket: str, prefix: str = \"\", **boto_kwargs: Any) -&gt; None:\n        try:\n            from aiobotocore.session import get_session  # type: ignore[import-not-found]\n        except ImportError:\n            msg = \"aiobotocore is required for AsyncS3FileSystem. Install it with: pip install idfkit[async-s3]\"\n            raise ImportError(msg) from None\n        self._bucket = bucket\n        self._prefix = prefix.strip(\"/\")\n        self._session: Any = get_session()\n        self._boto_kwargs = boto_kwargs\n        self._client: AsyncS3Client | None = None\n        self._client_ctx: Any = None\n\n    async def __aenter__(self) -&gt; AsyncS3FileSystem:\n        \"\"\"Create the aiobotocore S3 client.\"\"\"\n        self._client_ctx = self._session.create_client(\"s3\", **self._boto_kwargs)\n        self._client = await self._client_ctx.__aenter__()\n        return self\n\n    async def __aexit__(self, *args: Any) -&gt; None:\n        \"\"\"Close the aiobotocore S3 client.\"\"\"\n        if self._client_ctx is not None:\n            await self._client_ctx.__aexit__(*args)\n            self._client = None\n            self._client_ctx = None\n\n    def _ensure_client(self) -&gt; AsyncS3Client:\n        if self._client is None:\n            msg = (\n                \"AsyncS3FileSystem client is not initialised. \"\n                \"Use 'async with AsyncS3FileSystem(...) as fs:' to create the client.\"\n            )\n            raise RuntimeError(msg)\n        return self._client\n\n    def _key(self, path: str | Path) -&gt; str:\n        \"\"\"Build an S3 key by prepending the configured prefix.\n\n        Args:\n            path: Logical file path.\n\n        Returns:\n            The full S3 object key.\n        \"\"\"\n        raw = str(path).lstrip(\"/\")\n        if self._prefix:\n            return f\"{self._prefix}/{raw}\"\n        return raw\n\n    async def read_bytes(self, path: str | Path) -&gt; bytes:\n        \"\"\"Read a file as raw bytes from S3.\"\"\"\n        client = self._ensure_client()\n        resp = await client.get_object(Bucket=self._bucket, Key=self._key(path))\n        async with resp[\"Body\"] as stream:\n            return await stream.read()  # type: ignore[no-any-return]\n\n    async def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n        \"\"\"Write raw bytes to S3.\"\"\"\n        client = self._ensure_client()\n        await client.put_object(Bucket=self._bucket, Key=self._key(path), Body=data)\n\n    async def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n        \"\"\"Read a file as text from S3.\"\"\"\n        return (await self.read_bytes(path)).decode(encoding)\n\n    async def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n        \"\"\"Write text to S3.\"\"\"\n        await self.write_bytes(path, text.encode(encoding))\n\n    async def exists(self, path: str | Path) -&gt; bool:\n        \"\"\"Check whether an object exists in S3.\"\"\"\n        client = self._ensure_client()\n        try:\n            await client.head_object(Bucket=self._bucket, Key=self._key(path))\n        except Exception:\n            return False\n        return True\n\n    async def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n        \"\"\"No-op \u2014 S3 has no directory concept.\"\"\"\n\n    async def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n        \"\"\"Copy an object within the same bucket.\"\"\"\n        client = self._ensure_client()\n        await client.copy_object(\n            Bucket=self._bucket,\n            CopySource={\"Bucket\": self._bucket, \"Key\": self._key(src)},\n            Key=self._key(dst),\n        )\n\n    async def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n        \"\"\"List objects matching a glob pattern under *path*.\n\n        Returns logical paths (without the configured S3 prefix) so that\n        they can be passed back to other ``AsyncS3FileSystem`` methods which\n        prepend the prefix automatically via ``_key()``.\n        \"\"\"\n        client = self._ensure_client()\n        prefix = self._key(path).rstrip(\"/\") + \"/\"\n        paginator = client.get_paginator(\"list_objects_v2\")\n        logical_base = str(path).strip(\"/\")\n        return await self._collect_glob_matches(paginator, prefix, pattern, logical_base)\n\n    async def _collect_glob_matches(\n        self,\n        paginator: Any,\n        prefix: str,\n        pattern: str,\n        logical_base: str,\n    ) -&gt; list[str]:\n        \"\"\"Iterate the paginator and collect matching keys.\n\n        Separated to keep the ``glob`` method's return type fully known\n        to the type checker (aiobotocore's paginator yields untyped pages).\n        \"\"\"\n        matches: list[str] = []\n        async for raw_page in paginator.paginate(Bucket=self._bucket, Prefix=prefix):\n            page: dict[str, Any] = raw_page\n            contents: list[dict[str, Any]] = page.get(\"Contents\", [])\n            for obj in contents:\n                key: str = obj.get(\"Key\", \"\")\n                if not key:\n                    continue\n                name: str = key[len(prefix) :]\n                if fnmatch.fnmatch(name, pattern):\n                    matches.append(f\"{logical_base}/{name}\" if logical_base else name)\n        return matches\n\n    async def remove(self, path: str | Path) -&gt; None:\n        \"\"\"Delete an object from S3.\"\"\"\n        client = self._ensure_client()\n        await client.delete_object(Bucket=self._bucket, Key=self._key(path))\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.read_bytes","title":"<code>read_bytes(path)</code>  <code>async</code>","text":"<p>Read a file as raw bytes from S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def read_bytes(self, path: str | Path) -&gt; bytes:\n    \"\"\"Read a file as raw bytes from S3.\"\"\"\n    client = self._ensure_client()\n    resp = await client.get_object(Bucket=self._bucket, Key=self._key(path))\n    async with resp[\"Body\"] as stream:\n        return await stream.read()  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.write_bytes","title":"<code>write_bytes(path, data)</code>  <code>async</code>","text":"<p>Write raw bytes to S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n    \"\"\"Write raw bytes to S3.\"\"\"\n    client = self._ensure_client()\n    await client.put_object(Bucket=self._bucket, Key=self._key(path), Body=data)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.read_text","title":"<code>read_text(path, encoding='utf-8')</code>  <code>async</code>","text":"<p>Read a file as text from S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n    \"\"\"Read a file as text from S3.\"\"\"\n    return (await self.read_bytes(path)).decode(encoding)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.write_text","title":"<code>write_text(path, text, encoding='utf-8')</code>  <code>async</code>","text":"<p>Write text to S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n    \"\"\"Write text to S3.\"\"\"\n    await self.write_bytes(path, text.encode(encoding))\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.exists","title":"<code>exists(path)</code>  <code>async</code>","text":"<p>Check whether an object exists in S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def exists(self, path: str | Path) -&gt; bool:\n    \"\"\"Check whether an object exists in S3.\"\"\"\n    client = self._ensure_client()\n    try:\n        await client.head_object(Bucket=self._bucket, Key=self._key(path))\n    except Exception:\n        return False\n    return True\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.makedirs","title":"<code>makedirs(path, *, exist_ok=False)</code>  <code>async</code>","text":"<p>No-op \u2014 S3 has no directory concept.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n    \"\"\"No-op \u2014 S3 has no directory concept.\"\"\"\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.copy","title":"<code>copy(src, dst)</code>  <code>async</code>","text":"<p>Copy an object within the same bucket.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n    \"\"\"Copy an object within the same bucket.\"\"\"\n    client = self._ensure_client()\n    await client.copy_object(\n        Bucket=self._bucket,\n        CopySource={\"Bucket\": self._bucket, \"Key\": self._key(src)},\n        Key=self._key(dst),\n    )\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.glob","title":"<code>glob(path, pattern)</code>  <code>async</code>","text":"<p>List objects matching a glob pattern under path.</p> <p>Returns logical paths (without the configured S3 prefix) so that they can be passed back to other <code>AsyncS3FileSystem</code> methods which prepend the prefix automatically via <code>_key()</code>.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n    \"\"\"List objects matching a glob pattern under *path*.\n\n    Returns logical paths (without the configured S3 prefix) so that\n    they can be passed back to other ``AsyncS3FileSystem`` methods which\n    prepend the prefix automatically via ``_key()``.\n    \"\"\"\n    client = self._ensure_client()\n    prefix = self._key(path).rstrip(\"/\") + \"/\"\n    paginator = client.get_paginator(\"list_objects_v2\")\n    logical_base = str(path).strip(\"/\")\n    return await self._collect_glob_matches(paginator, prefix, pattern, logical_base)\n</code></pre>"},{"location":"api/simulation/fs/#idfkit.simulation.fs.AsyncS3FileSystem.remove","title":"<code>remove(path)</code>  <code>async</code>","text":"<p>Delete an object from S3.</p> Source code in <code>src/idfkit/simulation/fs.py</code> <pre><code>async def remove(self, path: str | Path) -&gt; None:\n    \"\"\"Delete an object from S3.\"\"\"\n    client = self._ensure_client()\n    await client.delete_object(Bucket=self._bucket, Key=self._key(path))\n</code></pre>"},{"location":"api/simulation/plotting/","title":"Plotting API","text":"<p>Pluggable plotting backends for result visualization.</p>"},{"location":"api/simulation/plotting/#plotbackend-protocol","title":"PlotBackend Protocol","text":""},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.PlotBackend","title":"<code>idfkit.simulation.plotting.PlotBackend</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for plotting backends used by the simulation module.</p> <p>Implementations must provide methods for common chart types. Each method returns a figure object native to the backend (e.g. matplotlib <code>Figure</code> or plotly <code>Figure</code>).</p> Source code in <code>src/idfkit/simulation/plotting/__init__.py</code> <pre><code>@runtime_checkable\nclass PlotBackend(Protocol):\n    \"\"\"Protocol for plotting backends used by the simulation module.\n\n    Implementations must provide methods for common chart types. Each method\n    returns a figure object native to the backend (e.g. matplotlib ``Figure``\n    or plotly ``Figure``).\n    \"\"\"\n\n    def line(\n        self,\n        x: Sequence[Any],\n        y: Sequence[float],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n        label: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a single line plot.\n\n        Args:\n            x: X-axis values (e.g. timestamps).\n            y: Y-axis values.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n            label: Optional line label for legend.\n\n        Returns:\n            A figure object native to the backend.\n        \"\"\"\n        ...\n\n    def multi_line(\n        self,\n        x: Sequence[Any],\n        y_series: dict[str, Sequence[float]],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a multi-line plot with legend.\n\n        Args:\n            x: Shared X-axis values.\n            y_series: Mapping of label to Y values for each line.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A figure object native to the backend.\n        \"\"\"\n        ...\n\n    def heatmap(\n        self,\n        data: Sequence[Sequence[float]],\n        *,\n        x_labels: Sequence[str] | None = None,\n        y_labels: Sequence[str] | None = None,\n        title: str | None = None,\n        colorbar_label: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a 2D heatmap.\n\n        Args:\n            data: 2D array of values (rows, columns).\n            x_labels: Optional labels for columns.\n            y_labels: Optional labels for rows.\n            title: Optional plot title.\n            colorbar_label: Optional label for the colorbar.\n\n        Returns:\n            A figure object native to the backend.\n        \"\"\"\n        ...\n\n    def bar(\n        self,\n        categories: Sequence[str],\n        values: Sequence[float],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a bar chart.\n\n        Args:\n            categories: Category labels for each bar.\n            values: Values for each bar.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A figure object native to the backend.\n        \"\"\"\n        ...\n\n    def stacked_bar(\n        self,\n        categories: Sequence[str],\n        series: dict[str, Sequence[float]],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a stacked bar chart.\n\n        Args:\n            categories: Category labels for each bar group.\n            series: Mapping of series label to values.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A figure object native to the backend.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.PlotBackend.bar","title":"<code>bar(categories, values, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>Sequence[str]</code> <p>Category labels for each bar.</p> required <code>values</code> <code>Sequence[float]</code> <p>Values for each bar.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p> Source code in <code>src/idfkit/simulation/plotting/__init__.py</code> <pre><code>def bar(\n    self,\n    categories: Sequence[str],\n    values: Sequence[float],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a bar chart.\n\n    Args:\n        categories: Category labels for each bar.\n        values: Values for each bar.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A figure object native to the backend.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.PlotBackend.heatmap","title":"<code>heatmap(data, *, x_labels=None, y_labels=None, title=None, colorbar_label=None)</code>","text":"<p>Create a 2D heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[Sequence[float]]</code> <p>2D array of values (rows, columns).</p> required <code>x_labels</code> <code>Sequence[str] | None</code> <p>Optional labels for columns.</p> <code>None</code> <code>y_labels</code> <code>Sequence[str] | None</code> <p>Optional labels for rows.</p> <code>None</code> <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>colorbar_label</code> <code>str | None</code> <p>Optional label for the colorbar.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p> Source code in <code>src/idfkit/simulation/plotting/__init__.py</code> <pre><code>def heatmap(\n    self,\n    data: Sequence[Sequence[float]],\n    *,\n    x_labels: Sequence[str] | None = None,\n    y_labels: Sequence[str] | None = None,\n    title: str | None = None,\n    colorbar_label: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a 2D heatmap.\n\n    Args:\n        data: 2D array of values (rows, columns).\n        x_labels: Optional labels for columns.\n        y_labels: Optional labels for rows.\n        title: Optional plot title.\n        colorbar_label: Optional label for the colorbar.\n\n    Returns:\n        A figure object native to the backend.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.PlotBackend.line","title":"<code>line(x, y, *, title=None, xlabel=None, ylabel=None, label=None)</code>","text":"<p>Create a single line plot.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[Any]</code> <p>X-axis values (e.g. timestamps).</p> required <code>y</code> <code>Sequence[float]</code> <p>Y-axis values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <code>label</code> <code>str | None</code> <p>Optional line label for legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p> Source code in <code>src/idfkit/simulation/plotting/__init__.py</code> <pre><code>def line(\n    self,\n    x: Sequence[Any],\n    y: Sequence[float],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n    label: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a single line plot.\n\n    Args:\n        x: X-axis values (e.g. timestamps).\n        y: Y-axis values.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n        label: Optional line label for legend.\n\n    Returns:\n        A figure object native to the backend.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.PlotBackend.multi_line","title":"<code>multi_line(x, y_series, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a multi-line plot with legend.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[Any]</code> <p>Shared X-axis values.</p> required <code>y_series</code> <code>dict[str, Sequence[float]]</code> <p>Mapping of label to Y values for each line.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p> Source code in <code>src/idfkit/simulation/plotting/__init__.py</code> <pre><code>def multi_line(\n    self,\n    x: Sequence[Any],\n    y_series: dict[str, Sequence[float]],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a multi-line plot with legend.\n\n    Args:\n        x: Shared X-axis values.\n        y_series: Mapping of label to Y values for each line.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A figure object native to the backend.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.PlotBackend.stacked_bar","title":"<code>stacked_bar(categories, series, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a stacked bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>Sequence[str]</code> <p>Category labels for each bar group.</p> required <code>series</code> <code>dict[str, Sequence[float]]</code> <p>Mapping of series label to values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object native to the backend.</p> Source code in <code>src/idfkit/simulation/plotting/__init__.py</code> <pre><code>def stacked_bar(\n    self,\n    categories: Sequence[str],\n    series: dict[str, Sequence[float]],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a stacked bar chart.\n\n    Args:\n        categories: Category labels for each bar group.\n        series: Mapping of series label to values.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A figure object native to the backend.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/simulation/plotting/#get_default_backend","title":"get_default_backend","text":""},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.get_default_backend","title":"<code>idfkit.simulation.plotting.get_default_backend()</code>","text":"<p>Auto-detect and return an available plotting backend.</p> <p>Tries matplotlib first, then plotly. Raises ImportError if neither is available.</p> <p>Returns:</p> Type Description <code>PlotBackend</code> <p>A PlotBackend instance.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If neither matplotlib nor plotly is installed.</p> Source code in <code>src/idfkit/simulation/plotting/__init__.py</code> <pre><code>def get_default_backend() -&gt; PlotBackend:\n    \"\"\"Auto-detect and return an available plotting backend.\n\n    Tries matplotlib first, then plotly. Raises ImportError if neither\n    is available.\n\n    Returns:\n        A PlotBackend instance.\n\n    Raises:\n        ImportError: If neither matplotlib nor plotly is installed.\n    \"\"\"\n    # Try matplotlib first (more common)\n    try:\n        from .matplotlib import MatplotlibBackend\n\n        return MatplotlibBackend()\n    except ImportError:\n        pass\n\n    # Fall back to plotly\n    try:\n        from .plotly import PlotlyBackend\n\n        return PlotlyBackend()\n    except ImportError:\n        pass\n\n    msg = (\n        \"No plotting backend available. Install matplotlib or plotly: \"\n        \"pip install idfkit[plot] or pip install idfkit[plotly]\"\n    )\n    raise ImportError(msg)\n</code></pre>"},{"location":"api/simulation/plotting/#built-in-visualizations","title":"Built-in Visualizations","text":""},{"location":"api/simulation/plotting/#plot_temperature_profile","title":"plot_temperature_profile","text":""},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.visualizations.plot_temperature_profile","title":"<code>idfkit.simulation.plotting.visualizations.plot_temperature_profile(sql, zones, *, backend=None, title='Zone Air Temperatures', frequency=None)</code>","text":"<p>Create a multi-line plot of zone air temperatures.</p> <p>Queries <code>Zone Mean Air Temperature</code> for each specified zone and plots them on a shared time axis.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>SQLResult</code> <p>An open SQLResult database.</p> required <code>zones</code> <code>Sequence[str]</code> <p>Zone names to plot.</p> required <code>backend</code> <code>PlotBackend | None</code> <p>Plotting backend to use. Auto-detects if not provided.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Zone Air Temperatures'</code> <code>frequency</code> <code>str | None</code> <p>Optional frequency filter (e.g. <code>\"Hourly\"</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object from the backend.</p> Source code in <code>src/idfkit/simulation/plotting/visualizations.py</code> <pre><code>def plot_temperature_profile(\n    sql: SQLResult,\n    zones: Sequence[str],\n    *,\n    backend: PlotBackend | None = None,\n    title: str = \"Zone Air Temperatures\",\n    frequency: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a multi-line plot of zone air temperatures.\n\n    Queries ``Zone Mean Air Temperature`` for each specified zone and\n    plots them on a shared time axis.\n\n    Args:\n        sql: An open SQLResult database.\n        zones: Zone names to plot.\n        backend: Plotting backend to use. Auto-detects if not provided.\n        title: Plot title.\n        frequency: Optional frequency filter (e.g. ``\"Hourly\"``).\n\n    Returns:\n        A figure object from the backend.\n    \"\"\"\n    if backend is None:\n        from . import get_default_backend\n\n        backend = get_default_backend()\n\n    timestamps: Sequence[Any] = ()\n    y_series: dict[str, Sequence[float]] = {}\n\n    for zone in zones:\n        ts = sql.get_timeseries(\"Zone Mean Air Temperature\", zone, frequency=frequency)\n        if not timestamps:\n            timestamps = ts.timestamps\n        y_series[zone] = ts.values\n\n    return backend.multi_line(\n        timestamps,\n        y_series,\n        title=title,\n        xlabel=\"Time\",\n        ylabel=\"Temperature (C)\",\n    )\n</code></pre>"},{"location":"api/simulation/plotting/#plot_energy_balance","title":"plot_energy_balance","text":""},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.visualizations.plot_energy_balance","title":"<code>idfkit.simulation.plotting.visualizations.plot_energy_balance(sql, *, backend=None, title='End-Use Energy by Category')</code>","text":"<p>Create a bar chart of end-use energy consumption.</p> <p>Extracts data from the <code>AnnualBuildingUtilityPerformanceSummary</code> report and plots energy consumption by end-use category.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>SQLResult</code> <p>An open SQLResult database.</p> required <code>backend</code> <code>PlotBackend | None</code> <p>Plotting backend to use. Auto-detects if not provided.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'End-Use Energy by Category'</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object from the backend.</p> Source code in <code>src/idfkit/simulation/plotting/visualizations.py</code> <pre><code>def plot_energy_balance(\n    sql: SQLResult,\n    *,\n    backend: PlotBackend | None = None,\n    title: str = \"End-Use Energy by Category\",\n) -&gt; Any:\n    \"\"\"Create a bar chart of end-use energy consumption.\n\n    Extracts data from the ``AnnualBuildingUtilityPerformanceSummary`` report\n    and plots energy consumption by end-use category.\n\n    Args:\n        sql: An open SQLResult database.\n        backend: Plotting backend to use. Auto-detects if not provided.\n        title: Plot title.\n\n    Returns:\n        A figure object from the backend.\n    \"\"\"\n    if backend is None:\n        from . import get_default_backend\n\n        backend = get_default_backend()\n\n    # Query end-use breakdown from the tabular report\n    rows = sql.get_tabular_data(\n        report_name=\"AnnualBuildingUtilityPerformanceSummary\",\n        table_name=\"End Uses\",\n    )\n\n    # Aggregate by row_name (end-use category), summing across fuels\n    # We want column_name \"Total Energy\" or sum numeric values from fuel columns\n    energy_by_use: dict[str, float] = {}\n    for row in rows:\n        # Skip header rows and non-numeric values\n        if row.row_name in (\"\", \"Total End Uses\"):\n            continue\n        try:\n            value = float(row.value)\n        except ValueError:\n            continue\n        # Skip zero or empty values\n        if value == 0:\n            continue\n        # Accumulate by end-use category\n        if row.row_name not in energy_by_use:\n            energy_by_use[row.row_name] = 0.0\n        energy_by_use[row.row_name] += value\n\n    # Sort by value descending\n    sorted_items = sorted(energy_by_use.items(), key=lambda x: x[1], reverse=True)\n    categories = [item[0] for item in sorted_items]\n    values = [item[1] for item in sorted_items]\n\n    return backend.bar(\n        categories,\n        values,\n        title=title,\n        xlabel=\"End Use\",\n        ylabel=\"Energy (GJ)\",\n    )\n</code></pre>"},{"location":"api/simulation/plotting/#plot_comfort_hours","title":"plot_comfort_hours","text":""},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.visualizations.plot_comfort_hours","title":"<code>idfkit.simulation.plotting.visualizations.plot_comfort_hours(sql, zones, *, comfort_min=20.0, comfort_max=26.0, backend=None, title='Comfort Hours by Zone and Month')</code>","text":"<p>Create a heatmap of comfort hours by zone and month.</p> <p>For each zone, calculates the percentage of hours within the comfort range for each month and displays as a heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>SQLResult</code> <p>An open SQLResult database.</p> required <code>zones</code> <code>Sequence[str]</code> <p>Zone names to analyze.</p> required <code>comfort_min</code> <code>float</code> <p>Minimum comfort temperature (default 20C).</p> <code>20.0</code> <code>comfort_max</code> <code>float</code> <p>Maximum comfort temperature (default 26C).</p> <code>26.0</code> <code>backend</code> <code>PlotBackend | None</code> <p>Plotting backend to use. Auto-detects if not provided.</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Comfort Hours by Zone and Month'</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object from the backend.</p> Source code in <code>src/idfkit/simulation/plotting/visualizations.py</code> <pre><code>def plot_comfort_hours(\n    sql: SQLResult,\n    zones: Sequence[str],\n    *,\n    comfort_min: float = 20.0,\n    comfort_max: float = 26.0,\n    backend: PlotBackend | None = None,\n    title: str = \"Comfort Hours by Zone and Month\",\n) -&gt; Any:\n    \"\"\"Create a heatmap of comfort hours by zone and month.\n\n    For each zone, calculates the percentage of hours within the comfort\n    range for each month and displays as a heatmap.\n\n    Args:\n        sql: An open SQLResult database.\n        zones: Zone names to analyze.\n        comfort_min: Minimum comfort temperature (default 20C).\n        comfort_max: Maximum comfort temperature (default 26C).\n        backend: Plotting backend to use. Auto-detects if not provided.\n        title: Plot title.\n\n    Returns:\n        A figure object from the backend.\n    \"\"\"\n    if backend is None:\n        from . import get_default_backend\n\n        backend = get_default_backend()\n\n    months = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n\n    # data[zone_idx][month_idx] = comfort percentage\n    data: list[list[float]] = []\n\n    for zone in zones:\n        ts = sql.get_timeseries(\"Zone Mean Air Temperature\", zone)\n\n        # Count hours per month\n        month_hours: dict[int, int] = dict.fromkeys(range(1, 13), 0)\n        month_comfort: dict[int, int] = dict.fromkeys(range(1, 13), 0)\n\n        for timestamp, value in zip(ts.timestamps, ts.values, strict=True):\n            month = timestamp.month\n            month_hours[month] += 1\n            if comfort_min &lt;= value &lt;= comfort_max:\n                month_comfort[month] += 1\n\n        # Calculate percentage for each month\n        row: list[float] = []\n        for m in range(1, 13):\n            pct = 100.0 * month_comfort[m] / month_hours[m] if month_hours[m] &gt; 0 else 0.0\n            row.append(pct)\n        data.append(row)\n\n    return backend.heatmap(\n        data,\n        x_labels=months,\n        y_labels=list(zones),\n        title=title,\n        colorbar_label=\"Comfort Hours (%)\",\n    )\n</code></pre>"},{"location":"api/simulation/plotting/#backend-implementations","title":"Backend Implementations","text":""},{"location":"api/simulation/plotting/#matplotlibbackend","title":"MatplotlibBackend","text":""},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.matplotlib.MatplotlibBackend","title":"<code>idfkit.simulation.plotting.matplotlib.MatplotlibBackend</code>","text":"<p>Plotting backend using matplotlib.</p> <p>Lazily imports matplotlib when methods are called. Each method returns a matplotlib <code>Figure</code> object.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If matplotlib is not installed.</p> Source code in <code>src/idfkit/simulation/plotting/matplotlib.py</code> <pre><code>class MatplotlibBackend:\n    \"\"\"Plotting backend using matplotlib.\n\n    Lazily imports matplotlib when methods are called. Each method returns\n    a matplotlib ``Figure`` object.\n\n    Raises:\n        ImportError: If matplotlib is not installed.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the backend, verifying matplotlib is available.\"\"\"\n        try:\n            import matplotlib.pyplot  # type: ignore[import-not-found]  # noqa: F401\n        except ImportError:\n            msg = \"matplotlib is required for MatplotlibBackend. Install it with: pip install idfkit[plot]\"\n            raise ImportError(msg) from None\n\n    def _get_pyplot(self) -&gt; Any:\n        \"\"\"Get matplotlib.pyplot module.\"\"\"\n        import matplotlib.pyplot as plt  # type: ignore[import-not-found]\n\n        return plt\n\n    def line(\n        self,\n        x: Sequence[Any],\n        y: Sequence[float],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n        label: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a single line plot.\n\n        Args:\n            x: X-axis values.\n            y: Y-axis values.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n            label: Optional line label for legend.\n\n        Returns:\n            A matplotlib Figure.\n        \"\"\"\n        plt = self._get_pyplot()\n        fig, ax = plt.subplots()\n        ax.plot(x, y, label=label)\n        if title:\n            ax.set_title(title)\n        if xlabel:\n            ax.set_xlabel(xlabel)\n        if ylabel:\n            ax.set_ylabel(ylabel)\n        if label:\n            ax.legend()\n        fig.tight_layout()\n        return fig\n\n    def multi_line(\n        self,\n        x: Sequence[Any],\n        y_series: dict[str, Sequence[float]],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a multi-line plot with legend.\n\n        Args:\n            x: Shared X-axis values.\n            y_series: Mapping of label to Y values.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A matplotlib Figure.\n        \"\"\"\n        plt = self._get_pyplot()\n        fig, ax = plt.subplots()\n        for line_label, y in y_series.items():\n            ax.plot(x, y, label=line_label)\n        if title:\n            ax.set_title(title)\n        if xlabel:\n            ax.set_xlabel(xlabel)\n        if ylabel:\n            ax.set_ylabel(ylabel)\n        if y_series:\n            ax.legend()\n        fig.tight_layout()\n        return fig\n\n    def heatmap(\n        self,\n        data: Sequence[Sequence[float]],\n        *,\n        x_labels: Sequence[str] | None = None,\n        y_labels: Sequence[str] | None = None,\n        title: str | None = None,\n        colorbar_label: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a 2D heatmap.\n\n        Args:\n            data: 2D array of values (rows, columns).\n            x_labels: Optional labels for columns.\n            y_labels: Optional labels for rows.\n            title: Optional plot title.\n            colorbar_label: Optional label for the colorbar.\n\n        Returns:\n            A matplotlib Figure.\n        \"\"\"\n        plt = self._get_pyplot()\n        fig, ax = plt.subplots()\n        im = ax.imshow(data, aspect=\"auto\")\n        cbar = fig.colorbar(im, ax=ax)\n        if colorbar_label:\n            cbar.set_label(colorbar_label)\n        if x_labels is not None:\n            ax.set_xticks(range(len(x_labels)))\n            ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n        if y_labels is not None:\n            ax.set_yticks(range(len(y_labels)))\n            ax.set_yticklabels(y_labels)\n        if title:\n            ax.set_title(title)\n        fig.tight_layout()\n        return fig\n\n    def bar(\n        self,\n        categories: Sequence[str],\n        values: Sequence[float],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a bar chart.\n\n        Args:\n            categories: Category labels for each bar.\n            values: Values for each bar.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A matplotlib Figure.\n        \"\"\"\n        plt = self._get_pyplot()\n        fig, ax = plt.subplots()\n        ax.bar(categories, values)\n        if title:\n            ax.set_title(title)\n        if xlabel:\n            ax.set_xlabel(xlabel)\n        if ylabel:\n            ax.set_ylabel(ylabel)\n        ax.tick_params(axis=\"x\", rotation=45)\n        fig.tight_layout()\n        return fig\n\n    def stacked_bar(\n        self,\n        categories: Sequence[str],\n        series: dict[str, Sequence[float]],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a stacked bar chart.\n\n        Args:\n            categories: Category labels for each bar group.\n            series: Mapping of series label to values.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A matplotlib Figure.\n        \"\"\"\n        plt = self._get_pyplot()\n        fig, ax = plt.subplots()\n        # Build x positions and bottom accumulator manually to avoid numpy\n        x_positions = list(range(len(categories)))\n        bottom: list[float] = [0.0] * len(categories)\n\n        for bar_label, bar_values in series.items():\n            ax.bar(x_positions, bar_values, bottom=bottom, label=bar_label)\n            for i, v in enumerate(bar_values):\n                bottom[i] += v\n\n        ax.set_xticks(x_positions)\n        ax.set_xticklabels(categories, rotation=45, ha=\"right\")\n        if title:\n            ax.set_title(title)\n        if xlabel:\n            ax.set_xlabel(xlabel)\n        if ylabel:\n            ax.set_ylabel(ylabel)\n        if series:\n            ax.legend()\n        fig.tight_layout()\n        return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.matplotlib.MatplotlibBackend.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the backend, verifying matplotlib is available.</p> Source code in <code>src/idfkit/simulation/plotting/matplotlib.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the backend, verifying matplotlib is available.\"\"\"\n    try:\n        import matplotlib.pyplot  # type: ignore[import-not-found]  # noqa: F401\n    except ImportError:\n        msg = \"matplotlib is required for MatplotlibBackend. Install it with: pip install idfkit[plot]\"\n        raise ImportError(msg) from None\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.matplotlib.MatplotlibBackend.bar","title":"<code>bar(categories, values, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>Sequence[str]</code> <p>Category labels for each bar.</p> required <code>values</code> <code>Sequence[float]</code> <p>Values for each bar.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib Figure.</p> Source code in <code>src/idfkit/simulation/plotting/matplotlib.py</code> <pre><code>def bar(\n    self,\n    categories: Sequence[str],\n    values: Sequence[float],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a bar chart.\n\n    Args:\n        categories: Category labels for each bar.\n        values: Values for each bar.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A matplotlib Figure.\n    \"\"\"\n    plt = self._get_pyplot()\n    fig, ax = plt.subplots()\n    ax.bar(categories, values)\n    if title:\n        ax.set_title(title)\n    if xlabel:\n        ax.set_xlabel(xlabel)\n    if ylabel:\n        ax.set_ylabel(ylabel)\n    ax.tick_params(axis=\"x\", rotation=45)\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.matplotlib.MatplotlibBackend.heatmap","title":"<code>heatmap(data, *, x_labels=None, y_labels=None, title=None, colorbar_label=None)</code>","text":"<p>Create a 2D heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[Sequence[float]]</code> <p>2D array of values (rows, columns).</p> required <code>x_labels</code> <code>Sequence[str] | None</code> <p>Optional labels for columns.</p> <code>None</code> <code>y_labels</code> <code>Sequence[str] | None</code> <p>Optional labels for rows.</p> <code>None</code> <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>colorbar_label</code> <code>str | None</code> <p>Optional label for the colorbar.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib Figure.</p> Source code in <code>src/idfkit/simulation/plotting/matplotlib.py</code> <pre><code>def heatmap(\n    self,\n    data: Sequence[Sequence[float]],\n    *,\n    x_labels: Sequence[str] | None = None,\n    y_labels: Sequence[str] | None = None,\n    title: str | None = None,\n    colorbar_label: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a 2D heatmap.\n\n    Args:\n        data: 2D array of values (rows, columns).\n        x_labels: Optional labels for columns.\n        y_labels: Optional labels for rows.\n        title: Optional plot title.\n        colorbar_label: Optional label for the colorbar.\n\n    Returns:\n        A matplotlib Figure.\n    \"\"\"\n    plt = self._get_pyplot()\n    fig, ax = plt.subplots()\n    im = ax.imshow(data, aspect=\"auto\")\n    cbar = fig.colorbar(im, ax=ax)\n    if colorbar_label:\n        cbar.set_label(colorbar_label)\n    if x_labels is not None:\n        ax.set_xticks(range(len(x_labels)))\n        ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n    if y_labels is not None:\n        ax.set_yticks(range(len(y_labels)))\n        ax.set_yticklabels(y_labels)\n    if title:\n        ax.set_title(title)\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.matplotlib.MatplotlibBackend.line","title":"<code>line(x, y, *, title=None, xlabel=None, ylabel=None, label=None)</code>","text":"<p>Create a single line plot.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[Any]</code> <p>X-axis values.</p> required <code>y</code> <code>Sequence[float]</code> <p>Y-axis values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <code>label</code> <code>str | None</code> <p>Optional line label for legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib Figure.</p> Source code in <code>src/idfkit/simulation/plotting/matplotlib.py</code> <pre><code>def line(\n    self,\n    x: Sequence[Any],\n    y: Sequence[float],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n    label: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a single line plot.\n\n    Args:\n        x: X-axis values.\n        y: Y-axis values.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n        label: Optional line label for legend.\n\n    Returns:\n        A matplotlib Figure.\n    \"\"\"\n    plt = self._get_pyplot()\n    fig, ax = plt.subplots()\n    ax.plot(x, y, label=label)\n    if title:\n        ax.set_title(title)\n    if xlabel:\n        ax.set_xlabel(xlabel)\n    if ylabel:\n        ax.set_ylabel(ylabel)\n    if label:\n        ax.legend()\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.matplotlib.MatplotlibBackend.multi_line","title":"<code>multi_line(x, y_series, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a multi-line plot with legend.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[Any]</code> <p>Shared X-axis values.</p> required <code>y_series</code> <code>dict[str, Sequence[float]]</code> <p>Mapping of label to Y values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib Figure.</p> Source code in <code>src/idfkit/simulation/plotting/matplotlib.py</code> <pre><code>def multi_line(\n    self,\n    x: Sequence[Any],\n    y_series: dict[str, Sequence[float]],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a multi-line plot with legend.\n\n    Args:\n        x: Shared X-axis values.\n        y_series: Mapping of label to Y values.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A matplotlib Figure.\n    \"\"\"\n    plt = self._get_pyplot()\n    fig, ax = plt.subplots()\n    for line_label, y in y_series.items():\n        ax.plot(x, y, label=line_label)\n    if title:\n        ax.set_title(title)\n    if xlabel:\n        ax.set_xlabel(xlabel)\n    if ylabel:\n        ax.set_ylabel(ylabel)\n    if y_series:\n        ax.legend()\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.matplotlib.MatplotlibBackend.stacked_bar","title":"<code>stacked_bar(categories, series, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a stacked bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>Sequence[str]</code> <p>Category labels for each bar group.</p> required <code>series</code> <code>dict[str, Sequence[float]]</code> <p>Mapping of series label to values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib Figure.</p> Source code in <code>src/idfkit/simulation/plotting/matplotlib.py</code> <pre><code>def stacked_bar(\n    self,\n    categories: Sequence[str],\n    series: dict[str, Sequence[float]],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a stacked bar chart.\n\n    Args:\n        categories: Category labels for each bar group.\n        series: Mapping of series label to values.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A matplotlib Figure.\n    \"\"\"\n    plt = self._get_pyplot()\n    fig, ax = plt.subplots()\n    # Build x positions and bottom accumulator manually to avoid numpy\n    x_positions = list(range(len(categories)))\n    bottom: list[float] = [0.0] * len(categories)\n\n    for bar_label, bar_values in series.items():\n        ax.bar(x_positions, bar_values, bottom=bottom, label=bar_label)\n        for i, v in enumerate(bar_values):\n            bottom[i] += v\n\n    ax.set_xticks(x_positions)\n    ax.set_xticklabels(categories, rotation=45, ha=\"right\")\n    if title:\n        ax.set_title(title)\n    if xlabel:\n        ax.set_xlabel(xlabel)\n    if ylabel:\n        ax.set_ylabel(ylabel)\n    if series:\n        ax.legend()\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#plotlybackend","title":"PlotlyBackend","text":""},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.plotly.PlotlyBackend","title":"<code>idfkit.simulation.plotting.plotly.PlotlyBackend</code>","text":"<p>Plotting backend using plotly.</p> <p>Lazily imports plotly when methods are called. Each method returns a plotly <code>Figure</code> object.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If plotly is not installed.</p> Source code in <code>src/idfkit/simulation/plotting/plotly.py</code> <pre><code>class PlotlyBackend:\n    \"\"\"Plotting backend using plotly.\n\n    Lazily imports plotly when methods are called. Each method returns\n    a plotly ``Figure`` object.\n\n    Raises:\n        ImportError: If plotly is not installed.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the backend, verifying plotly is available.\"\"\"\n        try:\n            import plotly.graph_objects  # type: ignore[import-not-found]  # noqa: F401\n        except ImportError:\n            msg = \"plotly is required for PlotlyBackend. Install it with: pip install idfkit[plotly]\"\n            raise ImportError(msg) from None\n\n    def _get_go(self) -&gt; Any:\n        \"\"\"Get plotly.graph_objects module.\"\"\"\n        import plotly.graph_objects as go  # type: ignore[import-not-found]\n\n        return go\n\n    def line(\n        self,\n        x: Sequence[Any],\n        y: Sequence[float],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n        label: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a single line plot.\n\n        Args:\n            x: X-axis values.\n            y: Y-axis values.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n            label: Optional line label for legend.\n\n        Returns:\n            A plotly Figure.\n        \"\"\"\n        go = self._get_go()\n        fig = go.Figure()\n        fig.add_trace(go.Scatter(x=list(x), y=list(y), mode=\"lines\", name=label or \"\"))\n        fig.update_layout(\n            title=title,\n            xaxis_title=xlabel,\n            yaxis_title=ylabel,\n            showlegend=label is not None,\n        )\n        return fig\n\n    def multi_line(\n        self,\n        x: Sequence[Any],\n        y_series: dict[str, Sequence[float]],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a multi-line plot with legend.\n\n        Args:\n            x: Shared X-axis values.\n            y_series: Mapping of label to Y values.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A plotly Figure.\n        \"\"\"\n        go = self._get_go()\n        fig = go.Figure()\n        for name, y in y_series.items():\n            fig.add_trace(go.Scatter(x=list(x), y=list(y), mode=\"lines\", name=name))\n        fig.update_layout(\n            title=title,\n            xaxis_title=xlabel,\n            yaxis_title=ylabel,\n            showlegend=bool(y_series),\n        )\n        return fig\n\n    def heatmap(\n        self,\n        data: Sequence[Sequence[float]],\n        *,\n        x_labels: Sequence[str] | None = None,\n        y_labels: Sequence[str] | None = None,\n        title: str | None = None,\n        colorbar_label: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a 2D heatmap.\n\n        Args:\n            data: 2D array of values (rows, columns).\n            x_labels: Optional labels for columns.\n            y_labels: Optional labels for rows.\n            title: Optional plot title.\n            colorbar_label: Optional label for the colorbar.\n\n        Returns:\n            A plotly Figure.\n        \"\"\"\n        go = self._get_go()\n        heatmap_args: dict[str, Any] = {\n            \"z\": [list(row) for row in data],\n        }\n        if x_labels is not None:\n            heatmap_args[\"x\"] = list(x_labels)\n        if y_labels is not None:\n            heatmap_args[\"y\"] = list(y_labels)\n        if colorbar_label:\n            heatmap_args[\"colorbar\"] = {\"title\": colorbar_label}\n\n        fig = go.Figure(data=go.Heatmap(**heatmap_args))\n        fig.update_layout(title=title)\n        return fig\n\n    def bar(\n        self,\n        categories: Sequence[str],\n        values: Sequence[float],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a bar chart.\n\n        Args:\n            categories: Category labels for each bar.\n            values: Values for each bar.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A plotly Figure.\n        \"\"\"\n        go = self._get_go()\n        fig = go.Figure(data=go.Bar(x=list(categories), y=list(values)))\n        fig.update_layout(\n            title=title,\n            xaxis_title=xlabel,\n            yaxis_title=ylabel,\n        )\n        return fig\n\n    def stacked_bar(\n        self,\n        categories: Sequence[str],\n        series: dict[str, Sequence[float]],\n        *,\n        title: str | None = None,\n        xlabel: str | None = None,\n        ylabel: str | None = None,\n    ) -&gt; Any:\n        \"\"\"Create a stacked bar chart.\n\n        Args:\n            categories: Category labels for each bar group.\n            series: Mapping of series label to values.\n            title: Optional plot title.\n            xlabel: Optional X-axis label.\n            ylabel: Optional Y-axis label.\n\n        Returns:\n            A plotly Figure.\n        \"\"\"\n        go = self._get_go()\n        fig = go.Figure()\n        for name, bar_values in series.items():\n            fig.add_trace(go.Bar(x=list(categories), y=list(bar_values), name=name))\n        fig.update_layout(\n            title=title,\n            xaxis_title=xlabel,\n            yaxis_title=ylabel,\n            barmode=\"stack\",\n        )\n        return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.plotly.PlotlyBackend.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the backend, verifying plotly is available.</p> Source code in <code>src/idfkit/simulation/plotting/plotly.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the backend, verifying plotly is available.\"\"\"\n    try:\n        import plotly.graph_objects  # type: ignore[import-not-found]  # noqa: F401\n    except ImportError:\n        msg = \"plotly is required for PlotlyBackend. Install it with: pip install idfkit[plotly]\"\n        raise ImportError(msg) from None\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.plotly.PlotlyBackend.bar","title":"<code>bar(categories, values, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>Sequence[str]</code> <p>Category labels for each bar.</p> required <code>values</code> <code>Sequence[float]</code> <p>Values for each bar.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A plotly Figure.</p> Source code in <code>src/idfkit/simulation/plotting/plotly.py</code> <pre><code>def bar(\n    self,\n    categories: Sequence[str],\n    values: Sequence[float],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a bar chart.\n\n    Args:\n        categories: Category labels for each bar.\n        values: Values for each bar.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A plotly Figure.\n    \"\"\"\n    go = self._get_go()\n    fig = go.Figure(data=go.Bar(x=list(categories), y=list(values)))\n    fig.update_layout(\n        title=title,\n        xaxis_title=xlabel,\n        yaxis_title=ylabel,\n    )\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.plotly.PlotlyBackend.heatmap","title":"<code>heatmap(data, *, x_labels=None, y_labels=None, title=None, colorbar_label=None)</code>","text":"<p>Create a 2D heatmap.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Sequence[Sequence[float]]</code> <p>2D array of values (rows, columns).</p> required <code>x_labels</code> <code>Sequence[str] | None</code> <p>Optional labels for columns.</p> <code>None</code> <code>y_labels</code> <code>Sequence[str] | None</code> <p>Optional labels for rows.</p> <code>None</code> <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>colorbar_label</code> <code>str | None</code> <p>Optional label for the colorbar.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A plotly Figure.</p> Source code in <code>src/idfkit/simulation/plotting/plotly.py</code> <pre><code>def heatmap(\n    self,\n    data: Sequence[Sequence[float]],\n    *,\n    x_labels: Sequence[str] | None = None,\n    y_labels: Sequence[str] | None = None,\n    title: str | None = None,\n    colorbar_label: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a 2D heatmap.\n\n    Args:\n        data: 2D array of values (rows, columns).\n        x_labels: Optional labels for columns.\n        y_labels: Optional labels for rows.\n        title: Optional plot title.\n        colorbar_label: Optional label for the colorbar.\n\n    Returns:\n        A plotly Figure.\n    \"\"\"\n    go = self._get_go()\n    heatmap_args: dict[str, Any] = {\n        \"z\": [list(row) for row in data],\n    }\n    if x_labels is not None:\n        heatmap_args[\"x\"] = list(x_labels)\n    if y_labels is not None:\n        heatmap_args[\"y\"] = list(y_labels)\n    if colorbar_label:\n        heatmap_args[\"colorbar\"] = {\"title\": colorbar_label}\n\n    fig = go.Figure(data=go.Heatmap(**heatmap_args))\n    fig.update_layout(title=title)\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.plotly.PlotlyBackend.line","title":"<code>line(x, y, *, title=None, xlabel=None, ylabel=None, label=None)</code>","text":"<p>Create a single line plot.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[Any]</code> <p>X-axis values.</p> required <code>y</code> <code>Sequence[float]</code> <p>Y-axis values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <code>label</code> <code>str | None</code> <p>Optional line label for legend.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A plotly Figure.</p> Source code in <code>src/idfkit/simulation/plotting/plotly.py</code> <pre><code>def line(\n    self,\n    x: Sequence[Any],\n    y: Sequence[float],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n    label: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a single line plot.\n\n    Args:\n        x: X-axis values.\n        y: Y-axis values.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n        label: Optional line label for legend.\n\n    Returns:\n        A plotly Figure.\n    \"\"\"\n    go = self._get_go()\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=list(x), y=list(y), mode=\"lines\", name=label or \"\"))\n    fig.update_layout(\n        title=title,\n        xaxis_title=xlabel,\n        yaxis_title=ylabel,\n        showlegend=label is not None,\n    )\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.plotly.PlotlyBackend.multi_line","title":"<code>multi_line(x, y_series, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a multi-line plot with legend.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Sequence[Any]</code> <p>Shared X-axis values.</p> required <code>y_series</code> <code>dict[str, Sequence[float]]</code> <p>Mapping of label to Y values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A plotly Figure.</p> Source code in <code>src/idfkit/simulation/plotting/plotly.py</code> <pre><code>def multi_line(\n    self,\n    x: Sequence[Any],\n    y_series: dict[str, Sequence[float]],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a multi-line plot with legend.\n\n    Args:\n        x: Shared X-axis values.\n        y_series: Mapping of label to Y values.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A plotly Figure.\n    \"\"\"\n    go = self._get_go()\n    fig = go.Figure()\n    for name, y in y_series.items():\n        fig.add_trace(go.Scatter(x=list(x), y=list(y), mode=\"lines\", name=name))\n    fig.update_layout(\n        title=title,\n        xaxis_title=xlabel,\n        yaxis_title=ylabel,\n        showlegend=bool(y_series),\n    )\n    return fig\n</code></pre>"},{"location":"api/simulation/plotting/#idfkit.simulation.plotting.plotly.PlotlyBackend.stacked_bar","title":"<code>stacked_bar(categories, series, *, title=None, xlabel=None, ylabel=None)</code>","text":"<p>Create a stacked bar chart.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>Sequence[str]</code> <p>Category labels for each bar group.</p> required <code>series</code> <code>dict[str, Sequence[float]]</code> <p>Mapping of series label to values.</p> required <code>title</code> <code>str | None</code> <p>Optional plot title.</p> <code>None</code> <code>xlabel</code> <code>str | None</code> <p>Optional X-axis label.</p> <code>None</code> <code>ylabel</code> <code>str | None</code> <p>Optional Y-axis label.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A plotly Figure.</p> Source code in <code>src/idfkit/simulation/plotting/plotly.py</code> <pre><code>def stacked_bar(\n    self,\n    categories: Sequence[str],\n    series: dict[str, Sequence[float]],\n    *,\n    title: str | None = None,\n    xlabel: str | None = None,\n    ylabel: str | None = None,\n) -&gt; Any:\n    \"\"\"Create a stacked bar chart.\n\n    Args:\n        categories: Category labels for each bar group.\n        series: Mapping of series label to values.\n        title: Optional plot title.\n        xlabel: Optional X-axis label.\n        ylabel: Optional Y-axis label.\n\n    Returns:\n        A plotly Figure.\n    \"\"\"\n    go = self._get_go()\n    fig = go.Figure()\n    for name, bar_values in series.items():\n        fig.add_trace(go.Bar(x=list(categories), y=list(bar_values), name=name))\n    fig.update_layout(\n        title=title,\n        xaxis_title=xlabel,\n        yaxis_title=ylabel,\n        barmode=\"stack\",\n    )\n    return fig\n</code></pre>"},{"location":"api/simulation/results/","title":"Results API","text":"<p>Simulation result container and output file access.</p>"},{"location":"api/simulation/results/#simulationresult","title":"SimulationResult","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult","title":"<code>idfkit.simulation.result.SimulationResult</code>  <code>dataclass</code>","text":"<p>Result of an EnergyPlus simulation run.</p> <p>Attributes:</p> Name Type Description <code>run_dir</code> <code>Path</code> <p>Directory containing all simulation output.</p> <code>success</code> <code>bool</code> <p>Whether the simulation exited successfully.</p> <code>exit_code</code> <code>int | None</code> <p>Process exit code (None if timed out).</p> <code>stdout</code> <code>str</code> <p>Captured standard output.</p> <code>stderr</code> <code>str</code> <p>Captured standard error.</p> <code>runtime_seconds</code> <code>float</code> <p>Wall-clock execution time in seconds.</p> <code>output_prefix</code> <code>str</code> <p>Output file prefix (default \"eplus\").</p> <code>fs</code> <code>FileSystem | None</code> <p>Optional sync file system backend for reading output files.</p> <code>async_fs</code> <code>AsyncFileSystem | None</code> <p>Optional async file system backend for non-blocking reads. Set automatically by async_simulate when an AsyncFileSystem is provided.</p> Source code in <code>src/idfkit/simulation/result.py</code> <pre><code>@dataclass(slots=True)\nclass SimulationResult:\n    \"\"\"Result of an EnergyPlus simulation run.\n\n    Attributes:\n        run_dir: Directory containing all simulation output.\n        success: Whether the simulation exited successfully.\n        exit_code: Process exit code (None if timed out).\n        stdout: Captured standard output.\n        stderr: Captured standard error.\n        runtime_seconds: Wall-clock execution time in seconds.\n        output_prefix: Output file prefix (default \"eplus\").\n        fs: Optional sync file system backend for reading output files.\n        async_fs: Optional async file system backend for non-blocking reads.\n            Set automatically by [async_simulate][idfkit.simulation.async_runner.async_simulate] when an\n            [AsyncFileSystem][idfkit.simulation.fs.AsyncFileSystem] is provided.\n    \"\"\"\n\n    run_dir: Path\n    success: bool\n    exit_code: int | None\n    stdout: str\n    stderr: str\n    runtime_seconds: float\n    output_prefix: str = \"eplus\"\n    fs: FileSystem | None = field(default=None, repr=False)\n    async_fs: AsyncFileSystem | None = field(default=None, repr=False)\n    _cached_errors: Any = field(default=_UNSET, init=False, repr=False)\n    _cached_sql: Any = field(default=_UNSET, init=False, repr=False)\n    _cached_variables: Any = field(default=_UNSET, init=False, repr=False)\n    _cached_csv: Any = field(default=_UNSET, init=False, repr=False)\n    _cached_html: Any = field(default=_UNSET, init=False, repr=False)\n\n    def __post_init__(self) -&gt; None:\n        if self.fs is not None and self.async_fs is not None:\n            msg = \"fs and async_fs are mutually exclusive \u2014 provide one or neither\"\n            raise ValueError(msg)\n\n    @property\n    def errors(self) -&gt; ErrorReport:\n        \"\"\"Parsed error report from the .err file (lazily cached).\n\n        Returns:\n            Parsed ErrorReport from the simulation's .err output.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_errors\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        err = self.err_path\n        if err is None:\n            report = ErrorReport.from_string(\"\")\n        elif self.fs is not None:\n            text = self.fs.read_text(str(err), encoding=\"latin-1\")\n            report = ErrorReport.from_string(text)\n        else:\n            report = ErrorReport.from_file(err)\n        object.__setattr__(self, \"_cached_errors\", report)\n        return report\n\n    @property\n    def sql(self) -&gt; SQLResult | None:\n        \"\"\"Parsed SQL output database (lazily cached).\n\n        Returns:\n            An SQLResult for querying time-series and tabular data,\n            or None if no .sql file was produced.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_sql\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        path = self.sql_path\n        if path is None:\n            object.__setattr__(self, \"_cached_sql\", None)\n            return None\n        from .parsers.sql import SQLResult as _SQLResult\n\n        if self.fs is not None:\n            # sqlite3 requires a local file \u2014 download to a temp file\n            data = self.fs.read_bytes(str(path))\n            with tempfile.NamedTemporaryFile(suffix=\".sql\", delete=False) as tmp_file:\n                tmp_file.write(data)\n            result: SQLResult = _SQLResult(Path(tmp_file.name))\n        else:\n            result = _SQLResult(path)\n        object.__setattr__(self, \"_cached_sql\", result)\n        return result\n\n    @property\n    def variables(self) -&gt; OutputVariableIndex | None:\n        \"\"\"Output variable/meter index from .rdd/.mdd files (lazily cached).\n\n        Returns:\n            An OutputVariableIndex for searching and injecting output\n            variables, or None if no .rdd file was produced.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_variables\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        rdd = self.rdd_path\n        if rdd is None:\n            object.__setattr__(self, \"_cached_variables\", None)\n            return None\n\n        if self.fs is not None:\n            from .parsers.rdd import parse_mdd, parse_rdd\n\n            rdd_text = self.fs.read_text(str(rdd), encoding=\"latin-1\")\n            variables = parse_rdd(rdd_text)\n            mdd = self.mdd_path\n            meters = parse_mdd(self.fs.read_text(str(mdd), encoding=\"latin-1\")) if mdd is not None else ()\n            from .outputs import OutputVariableIndex as _OutputVariableIndex\n\n            result: OutputVariableIndex = _OutputVariableIndex(variables=variables, meters=meters)\n        else:\n            from .outputs import OutputVariableIndex as _OutputVariableIndex\n\n            result = _OutputVariableIndex.from_files(rdd, self.mdd_path)\n        object.__setattr__(self, \"_cached_variables\", result)\n        return result\n\n    @property\n    def csv(self) -&gt; CSVResult | None:\n        \"\"\"Parsed CSV output (lazily cached).\n\n        Returns:\n            A CSVResult with extracted column metadata and values,\n            or None if no .csv file was produced.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_csv\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        path = self.csv_path\n        if path is None:\n            object.__setattr__(self, \"_cached_csv\", None)\n            return None\n        from .parsers.csv import CSVResult as _CSVResult\n\n        if self.fs is not None:\n            text = self.fs.read_text(str(path), encoding=\"latin-1\")\n            result: CSVResult = _CSVResult.from_string(text)\n        else:\n            result = _CSVResult.from_file(path)\n        object.__setattr__(self, \"_cached_csv\", result)\n        return result\n\n    @property\n    def html(self) -&gt; HTMLResult | None:\n        \"\"\"Parsed HTML tabular output (lazily cached).\n\n        Returns:\n            An HTMLResult with extracted tables and titles,\n            or None if no HTML file was produced.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_html\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        path = self.html_path\n        if path is None:\n            object.__setattr__(self, \"_cached_html\", None)\n            return None\n        from .parsers.html import HTMLResult as _HTMLResult\n\n        if self.fs is not None:\n            text = self.fs.read_text(str(path), encoding=\"latin-1\")\n            result: HTMLResult = _HTMLResult.from_string(text)\n        else:\n            result = _HTMLResult.from_file(path)\n        object.__setattr__(self, \"_cached_html\", result)\n        return result\n\n    @property\n    def sql_path(self) -&gt; Path | None:\n        \"\"\"Path to the SQLite output file, if present.\"\"\"\n        return self._find_output_file(\".sql\")\n\n    @property\n    def err_path(self) -&gt; Path | None:\n        \"\"\"Path to the .err output file, if present.\"\"\"\n        return self._find_output_file(\".err\")\n\n    @property\n    def eso_path(self) -&gt; Path | None:\n        \"\"\"Path to the .eso output file, if present.\"\"\"\n        return self._find_output_file(\".eso\")\n\n    @property\n    def csv_path(self) -&gt; Path | None:\n        \"\"\"Path to the .csv output file, if present.\"\"\"\n        return self._find_output_file(\".csv\")\n\n    @property\n    def html_path(self) -&gt; Path | None:\n        \"\"\"Path to the HTML tabular output file, if present.\"\"\"\n        return (\n            self._find_output_file(\"Table.htm\")\n            or self._find_output_file(\"Table.html\")\n            or self._find_output_file(\".htm\")\n            or self._find_output_file(\".html\")\n        )\n\n    @property\n    def rdd_path(self) -&gt; Path | None:\n        \"\"\"Path to the .rdd output file, if present.\"\"\"\n        return self._find_output_file(\".rdd\")\n\n    @property\n    def mdd_path(self) -&gt; Path | None:\n        \"\"\"Path to the .mdd output file, if present.\"\"\"\n        return self._find_output_file(\".mdd\")\n\n    def _find_output_file(self, suffix: str) -&gt; Path | None:\n        \"\"\"Find an output file by suffix.\n\n        Looks for ``{prefix}out{suffix}`` first, then falls back to\n        scanning the run directory for any file with the given suffix.\n\n        Args:\n            suffix: File suffix to look for (e.g. \".sql\", \".err\").\n\n        Returns:\n            Path to the file, or None if not found.\n\n        Raises:\n            RuntimeError: If only [async_fs][idfkit.simulation.result.SimulationResult.async_fs] is set (no sync access\n                available).  Use the ``async_*`` methods instead.\n        \"\"\"\n        if self.async_fs is not None and self.fs is None:\n            msg = (\n                \"This SimulationResult was created with an AsyncFileSystem. \"\n                \"Use the async accessors (e.g. async_errors(), async_sql()) \"\n                \"instead of the sync properties.\"\n            )\n            raise RuntimeError(msg)\n\n        primary = self.run_dir / f\"{self.output_prefix}out{suffix}\"\n\n        if self.fs is not None:\n            if self.fs.exists(str(primary)):\n                return primary\n            # Fallback: glob for matching files\n            matches = self.fs.glob(str(self.run_dir), f\"*{suffix}\")\n            if matches:\n                return Path(matches[0])\n            return None\n\n        # Local path-based lookup\n        if primary.is_file():\n            return primary\n\n        # Fallback: scan directory\n        for p in self.run_dir.iterdir():\n            if p.is_file() and p.name.endswith(suffix):\n                return p\n\n        return None\n\n    # ------------------------------------------------------------------\n    # Async accessors \u2014 non-blocking counterparts to the sync properties\n    # ------------------------------------------------------------------\n\n    async def async_errors(self) -&gt; ErrorReport:\n        \"\"\"Parsed error report from the .err file (async, lazily cached).\n\n        Non-blocking counterpart to [errors][idfkit.simulation.result.SimulationResult.errors] that uses\n        [async_fs][idfkit.simulation.result.SimulationResult.async_fs] for file reads.\n\n        Returns:\n            Parsed ErrorReport from the simulation's .err output.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_errors\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        err = await self._async_find_output_file(\".err\")\n        if err is None:\n            report = ErrorReport.from_string(\"\")\n        elif self.async_fs is not None:\n            text = await self.async_fs.read_text(str(err), encoding=\"latin-1\")\n            report = ErrorReport.from_string(text)\n        elif self.fs is not None:\n            text = self.fs.read_text(str(err), encoding=\"latin-1\")\n            report = ErrorReport.from_string(text)\n        else:\n            report = ErrorReport.from_file(err)\n        object.__setattr__(self, \"_cached_errors\", report)\n        return report\n\n    async def async_sql(self) -&gt; SQLResult | None:\n        \"\"\"Parsed SQL output database (async, lazily cached).\n\n        Non-blocking counterpart to [sql][idfkit.simulation.result.SimulationResult.sql] that uses\n        [async_fs][idfkit.simulation.result.SimulationResult.async_fs] for file reads.\n\n        Returns:\n            An SQLResult for querying time-series and tabular data,\n            or None if no .sql file was produced.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_sql\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        path = await self._async_find_output_file(\".sql\")\n        if path is None:\n            object.__setattr__(self, \"_cached_sql\", None)\n            return None\n        from .parsers.sql import SQLResult as _SQLResult\n\n        if self.async_fs is not None:\n            data = await self.async_fs.read_bytes(str(path))\n            with tempfile.NamedTemporaryFile(suffix=\".sql\", delete=False) as tmp_file:\n                tmp_file.write(data)\n            result: SQLResult = _SQLResult(Path(tmp_file.name))\n        elif self.fs is not None:\n            data = self.fs.read_bytes(str(path))\n            with tempfile.NamedTemporaryFile(suffix=\".sql\", delete=False) as tmp_file:\n                tmp_file.write(data)\n            result = _SQLResult(Path(tmp_file.name))\n        else:\n            result = _SQLResult(path)\n        object.__setattr__(self, \"_cached_sql\", result)\n        return result\n\n    async def async_variables(self) -&gt; OutputVariableIndex | None:\n        \"\"\"Output variable/meter index (async, lazily cached).\n\n        Non-blocking counterpart to [variables][idfkit.simulation.result.SimulationResult.variables] that uses\n        [async_fs][idfkit.simulation.result.SimulationResult.async_fs] for file reads.\n\n        Returns:\n            An OutputVariableIndex for searching and injecting output\n            variables, or None if no .rdd file was produced.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_variables\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        rdd = await self._async_find_output_file(\".rdd\")\n        if rdd is None:\n            object.__setattr__(self, \"_cached_variables\", None)\n            return None\n\n        if self.async_fs is not None:\n            from .parsers.rdd import parse_mdd, parse_rdd\n\n            rdd_text = await self.async_fs.read_text(str(rdd), encoding=\"latin-1\")\n            variables = parse_rdd(rdd_text)\n            mdd = await self._async_find_output_file(\".mdd\")\n            meters = parse_mdd(await self.async_fs.read_text(str(mdd), encoding=\"latin-1\")) if mdd is not None else ()\n            from .outputs import OutputVariableIndex as _OutputVariableIndex\n\n            result: OutputVariableIndex = _OutputVariableIndex(variables=variables, meters=meters)\n        elif self.fs is not None:\n            from .parsers.rdd import parse_mdd, parse_rdd\n\n            rdd_text = self.fs.read_text(str(rdd), encoding=\"latin-1\")\n            variables = parse_rdd(rdd_text)\n            mdd = self._find_output_file(\".mdd\")\n            meters = parse_mdd(self.fs.read_text(str(mdd), encoding=\"latin-1\")) if mdd is not None else ()\n            from .outputs import OutputVariableIndex as _OutputVariableIndex\n\n            result = _OutputVariableIndex(variables=variables, meters=meters)\n        else:\n            from .outputs import OutputVariableIndex as _OutputVariableIndex\n\n            result = _OutputVariableIndex.from_files(rdd, self.mdd_path)\n        object.__setattr__(self, \"_cached_variables\", result)\n        return result\n\n    async def async_csv(self) -&gt; CSVResult | None:\n        \"\"\"Parsed CSV output (async, lazily cached).\n\n        Non-blocking counterpart to [csv][idfkit.simulation.result.SimulationResult.csv] that uses\n        [async_fs][idfkit.simulation.result.SimulationResult.async_fs] for file reads.\n\n        Returns:\n            A CSVResult with extracted column metadata and values,\n            or None if no .csv file was produced.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_csv\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        path = await self._async_find_output_file(\".csv\")\n        if path is None:\n            object.__setattr__(self, \"_cached_csv\", None)\n            return None\n        from .parsers.csv import CSVResult as _CSVResult\n\n        if self.async_fs is not None:\n            text = await self.async_fs.read_text(str(path), encoding=\"latin-1\")\n            result: CSVResult = _CSVResult.from_string(text)\n        elif self.fs is not None:\n            text = self.fs.read_text(str(path), encoding=\"latin-1\")\n            result = _CSVResult.from_string(text)\n        else:\n            result = _CSVResult.from_file(path)\n        object.__setattr__(self, \"_cached_csv\", result)\n        return result\n\n    async def async_html(self) -&gt; HTMLResult | None:\n        \"\"\"Parsed HTML tabular output (async, lazily cached).\n\n        Non-blocking counterpart to [html][idfkit.simulation.result.SimulationResult.html] that uses\n        [async_fs][idfkit.simulation.result.SimulationResult.async_fs] for file reads.\n\n        Returns:\n            An HTMLResult with extracted tables and titles,\n            or None if no HTML file was produced.\n        \"\"\"\n        cached = object.__getattribute__(self, \"_cached_html\")\n        if cached is not _UNSET:\n            return cached  # type: ignore[no-any-return]\n        path = (\n            await self._async_find_output_file(\"Table.htm\")\n            or await self._async_find_output_file(\"Table.html\")\n            or await self._async_find_output_file(\".htm\")\n            or await self._async_find_output_file(\".html\")\n        )\n        if path is None:\n            object.__setattr__(self, \"_cached_html\", None)\n            return None\n        from .parsers.html import HTMLResult as _HTMLResult\n\n        if self.async_fs is not None:\n            text = await self.async_fs.read_text(str(path), encoding=\"latin-1\")\n            result: HTMLResult = _HTMLResult.from_string(text)\n        elif self.fs is not None:\n            text = self.fs.read_text(str(path), encoding=\"latin-1\")\n            result = _HTMLResult.from_string(text)\n        else:\n            result = _HTMLResult.from_file(path)\n        object.__setattr__(self, \"_cached_html\", result)\n        return result\n\n    async def _async_find_output_file(self, suffix: str) -&gt; Path | None:\n        \"\"\"Async counterpart to [_find_output_file][].\n\n        Uses [async_fs][idfkit.simulation.result.SimulationResult.async_fs] for non-blocking file lookups, falling back\n        to [fs][] or local path checks.\n\n        Args:\n            suffix: File suffix to look for (e.g. \".sql\", \".err\").\n\n        Returns:\n            Path to the file, or None if not found.\n        \"\"\"\n        primary = self.run_dir / f\"{self.output_prefix}out{suffix}\"\n\n        if self.async_fs is not None:\n            if await self.async_fs.exists(str(primary)):\n                return primary\n            matches = await self.async_fs.glob(str(self.run_dir), f\"*{suffix}\")\n            if matches:\n                return Path(matches[0])\n            return None\n\n        if self.fs is not None:\n            if self.fs.exists(str(primary)):\n                return primary\n            matches = self.fs.glob(str(self.run_dir), f\"*{suffix}\")\n            if matches:\n                return Path(matches[0])\n            return None\n\n        # Local path-based lookup\n        if primary.is_file():\n            return primary\n\n        for p in self.run_dir.iterdir():\n            if p.is_file() and p.name.endswith(suffix):\n                return p\n\n        return None\n\n    @classmethod\n    def from_directory(\n        cls,\n        path: str | Path,\n        *,\n        output_prefix: str = \"eplus\",\n        fs: FileSystem | None = None,\n        async_fs: AsyncFileSystem | None = None,\n    ) -&gt; SimulationResult:\n        \"\"\"Reconstruct a SimulationResult from an existing output directory.\n\n        Useful for inspecting results from a previous simulation run.\n\n        Args:\n            path: Path to the simulation output directory.\n            output_prefix: Output file prefix used during the run.\n            fs: Optional sync file system backend for reading output files.\n            async_fs: Optional async file system backend for non-blocking reads.\n\n        Returns:\n            SimulationResult pointing to the existing output.\n        \"\"\"\n        run_dir = Path(path) if (fs is not None or async_fs is not None) else Path(path).resolve()\n        return cls(\n            run_dir=run_dir,\n            success=True,\n            exit_code=0,\n            stdout=\"\",\n            stderr=\"\",\n            runtime_seconds=0.0,\n            output_prefix=output_prefix,\n            fs=fs,\n            async_fs=async_fs,\n        )\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.run_dir","title":"<code>run_dir</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.success","title":"<code>success</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.exit_code","title":"<code>exit_code</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.stdout","title":"<code>stdout</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.stderr","title":"<code>stderr</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.runtime_seconds","title":"<code>runtime_seconds</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.output_prefix","title":"<code>output_prefix = 'eplus'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.fs","title":"<code>fs = field(default=None, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.async_fs","title":"<code>async_fs = field(default=None, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.errors","title":"<code>errors</code>  <code>property</code>","text":"<p>Parsed error report from the .err file (lazily cached).</p> <p>Returns:</p> Type Description <code>ErrorReport</code> <p>Parsed ErrorReport from the simulation's .err output.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.sql","title":"<code>sql</code>  <code>property</code>","text":"<p>Parsed SQL output database (lazily cached).</p> <p>Returns:</p> Type Description <code>SQLResult | None</code> <p>An SQLResult for querying time-series and tabular data,</p> <code>SQLResult | None</code> <p>or None if no .sql file was produced.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.variables","title":"<code>variables</code>  <code>property</code>","text":"<p>Output variable/meter index from .rdd/.mdd files (lazily cached).</p> <p>Returns:</p> Type Description <code>OutputVariableIndex | None</code> <p>An OutputVariableIndex for searching and injecting output</p> <code>OutputVariableIndex | None</code> <p>variables, or None if no .rdd file was produced.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.csv","title":"<code>csv</code>  <code>property</code>","text":"<p>Parsed CSV output (lazily cached).</p> <p>Returns:</p> Type Description <code>CSVResult | None</code> <p>A CSVResult with extracted column metadata and values,</p> <code>CSVResult | None</code> <p>or None if no .csv file was produced.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.html","title":"<code>html</code>  <code>property</code>","text":"<p>Parsed HTML tabular output (lazily cached).</p> <p>Returns:</p> Type Description <code>HTMLResult | None</code> <p>An HTMLResult with extracted tables and titles,</p> <code>HTMLResult | None</code> <p>or None if no HTML file was produced.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.sql_path","title":"<code>sql_path</code>  <code>property</code>","text":"<p>Path to the SQLite output file, if present.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.err_path","title":"<code>err_path</code>  <code>property</code>","text":"<p>Path to the .err output file, if present.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.eso_path","title":"<code>eso_path</code>  <code>property</code>","text":"<p>Path to the .eso output file, if present.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.csv_path","title":"<code>csv_path</code>  <code>property</code>","text":"<p>Path to the .csv output file, if present.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.html_path","title":"<code>html_path</code>  <code>property</code>","text":"<p>Path to the HTML tabular output file, if present.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.rdd_path","title":"<code>rdd_path</code>  <code>property</code>","text":"<p>Path to the .rdd output file, if present.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.mdd_path","title":"<code>mdd_path</code>  <code>property</code>","text":"<p>Path to the .mdd output file, if present.</p>"},{"location":"api/simulation/results/#idfkit.simulation.result.SimulationResult.from_directory","title":"<code>from_directory(path, *, output_prefix='eplus', fs=None, async_fs=None)</code>  <code>classmethod</code>","text":"<p>Reconstruct a SimulationResult from an existing output directory.</p> <p>Useful for inspecting results from a previous simulation run.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the simulation output directory.</p> required <code>output_prefix</code> <code>str</code> <p>Output file prefix used during the run.</p> <code>'eplus'</code> <code>fs</code> <code>FileSystem | None</code> <p>Optional sync file system backend for reading output files.</p> <code>None</code> <code>async_fs</code> <code>AsyncFileSystem | None</code> <p>Optional async file system backend for non-blocking reads.</p> <code>None</code> <p>Returns:</p> Type Description <code>SimulationResult</code> <p>SimulationResult pointing to the existing output.</p> Source code in <code>src/idfkit/simulation/result.py</code> <pre><code>@classmethod\ndef from_directory(\n    cls,\n    path: str | Path,\n    *,\n    output_prefix: str = \"eplus\",\n    fs: FileSystem | None = None,\n    async_fs: AsyncFileSystem | None = None,\n) -&gt; SimulationResult:\n    \"\"\"Reconstruct a SimulationResult from an existing output directory.\n\n    Useful for inspecting results from a previous simulation run.\n\n    Args:\n        path: Path to the simulation output directory.\n        output_prefix: Output file prefix used during the run.\n        fs: Optional sync file system backend for reading output files.\n        async_fs: Optional async file system backend for non-blocking reads.\n\n    Returns:\n        SimulationResult pointing to the existing output.\n    \"\"\"\n    run_dir = Path(path) if (fs is not None or async_fs is not None) else Path(path).resolve()\n    return cls(\n        run_dir=run_dir,\n        success=True,\n        exit_code=0,\n        stdout=\"\",\n        stderr=\"\",\n        runtime_seconds=0.0,\n        output_prefix=output_prefix,\n        fs=fs,\n        async_fs=async_fs,\n    )\n</code></pre>"},{"location":"api/simulation/results/#errorreport","title":"ErrorReport","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport","title":"<code>idfkit.simulation.parsers.err.ErrorReport</code>  <code>dataclass</code>","text":"<p>Parsed contents of an EnergyPlus .err file.</p> <p>Attributes:</p> Name Type Description <code>fatal</code> <code>tuple[ErrorMessage, ...]</code> <p>Fatal error messages.</p> <code>severe</code> <code>tuple[ErrorMessage, ...]</code> <p>Severe error messages.</p> <code>warnings</code> <code>tuple[ErrorMessage, ...]</code> <p>Warning messages.</p> <code>info</code> <code>tuple[ErrorMessage, ...]</code> <p>Informational messages.</p> <code>warmup_converged</code> <code>bool</code> <p>Whether warmup convergence was reported.</p> <code>simulation_complete</code> <code>bool</code> <p>Whether the simulation completed successfully.</p> <code>raw_text</code> <code>str</code> <p>The original unparsed file text.</p> Source code in <code>src/idfkit/simulation/parsers/err.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass ErrorReport:\n    \"\"\"Parsed contents of an EnergyPlus .err file.\n\n    Attributes:\n        fatal: Fatal error messages.\n        severe: Severe error messages.\n        warnings: Warning messages.\n        info: Informational messages.\n        warmup_converged: Whether warmup convergence was reported.\n        simulation_complete: Whether the simulation completed successfully.\n        raw_text: The original unparsed file text.\n    \"\"\"\n\n    fatal: tuple[ErrorMessage, ...]\n    severe: tuple[ErrorMessage, ...]\n    warnings: tuple[ErrorMessage, ...]\n    info: tuple[ErrorMessage, ...]\n    warmup_converged: bool\n    simulation_complete: bool\n    raw_text: str\n\n    @property\n    def has_fatal(self) -&gt; bool:\n        \"\"\"Whether any fatal errors were found.\"\"\"\n        return len(self.fatal) &gt; 0\n\n    @property\n    def has_severe(self) -&gt; bool:\n        \"\"\"Whether any severe errors were found.\"\"\"\n        return len(self.severe) &gt; 0\n\n    @property\n    def fatal_count(self) -&gt; int:\n        \"\"\"Number of fatal errors.\"\"\"\n        return len(self.fatal)\n\n    @property\n    def severe_count(self) -&gt; int:\n        \"\"\"Number of severe errors.\"\"\"\n        return len(self.severe)\n\n    @property\n    def error_count(self) -&gt; int:\n        \"\"\"Total number of fatal + severe errors.\"\"\"\n        return len(self.fatal) + len(self.severe)\n\n    @property\n    def warning_count(self) -&gt; int:\n        \"\"\"Total number of warnings.\"\"\"\n        return len(self.warnings)\n\n    def summary(self) -&gt; str:\n        \"\"\"Return a human-readable summary of the error report.\n\n        Returns:\n            A multi-line summary string.\n        \"\"\"\n        lines: list[str] = []\n        lines.append(\n            f\"Fatal: {len(self.fatal)}, Severe: {len(self.severe)}, \"\n            f\"Warnings: {len(self.warnings)}, Info: {len(self.info)}\"\n        )\n        if self.warmup_converged:\n            lines.append(\"Warmup: converged\")\n        if self.simulation_complete:\n            lines.append(\"Simulation: completed successfully\")\n        elif self.has_fatal:\n            lines.append(\"Simulation: terminated with fatal error(s)\")\n        return \"\\n\".join(lines)\n\n    @classmethod\n    def from_file(cls, path: str | Path) -&gt; ErrorReport:\n        \"\"\"Parse an .err file from disk.\n\n        Args:\n            path: Path to the .err file.\n\n        Returns:\n            Parsed ErrorReport.\n        \"\"\"\n        text = Path(path).read_text(encoding=\"latin-1\")\n        return _parse_err(text)\n\n    @classmethod\n    def from_string(cls, text: str) -&gt; ErrorReport:\n        \"\"\"Parse .err content from a string.\n\n        Args:\n            text: Raw .err file contents.\n\n        Returns:\n            Parsed ErrorReport.\n        \"\"\"\n        return _parse_err(text)\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.fatal","title":"<code>fatal</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.severe","title":"<code>severe</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.warnings","title":"<code>warnings</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.fatal_count","title":"<code>fatal_count</code>  <code>property</code>","text":"<p>Number of fatal errors.</p>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.severe_count","title":"<code>severe_count</code>  <code>property</code>","text":"<p>Number of severe errors.</p>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.warning_count","title":"<code>warning_count</code>  <code>property</code>","text":"<p>Total number of warnings.</p>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.has_fatal","title":"<code>has_fatal</code>  <code>property</code>","text":"<p>Whether any fatal errors were found.</p>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.has_severe","title":"<code>has_severe</code>  <code>property</code>","text":"<p>Whether any severe errors were found.</p>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.summary","title":"<code>summary()</code>","text":"<p>Return a human-readable summary of the error report.</p> <p>Returns:</p> Type Description <code>str</code> <p>A multi-line summary string.</p> Source code in <code>src/idfkit/simulation/parsers/err.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Return a human-readable summary of the error report.\n\n    Returns:\n        A multi-line summary string.\n    \"\"\"\n    lines: list[str] = []\n    lines.append(\n        f\"Fatal: {len(self.fatal)}, Severe: {len(self.severe)}, \"\n        f\"Warnings: {len(self.warnings)}, Info: {len(self.info)}\"\n    )\n    if self.warmup_converged:\n        lines.append(\"Warmup: converged\")\n    if self.simulation_complete:\n        lines.append(\"Simulation: completed successfully\")\n    elif self.has_fatal:\n        lines.append(\"Simulation: terminated with fatal error(s)\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.from_file","title":"<code>from_file(path)</code>  <code>classmethod</code>","text":"<p>Parse an .err file from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the .err file.</p> required <p>Returns:</p> Type Description <code>ErrorReport</code> <p>Parsed ErrorReport.</p> Source code in <code>src/idfkit/simulation/parsers/err.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str | Path) -&gt; ErrorReport:\n    \"\"\"Parse an .err file from disk.\n\n    Args:\n        path: Path to the .err file.\n\n    Returns:\n        Parsed ErrorReport.\n    \"\"\"\n    text = Path(path).read_text(encoding=\"latin-1\")\n    return _parse_err(text)\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorReport.from_string","title":"<code>from_string(text)</code>  <code>classmethod</code>","text":"<p>Parse .err content from a string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Raw .err file contents.</p> required <p>Returns:</p> Type Description <code>ErrorReport</code> <p>Parsed ErrorReport.</p> Source code in <code>src/idfkit/simulation/parsers/err.py</code> <pre><code>@classmethod\ndef from_string(cls, text: str) -&gt; ErrorReport:\n    \"\"\"Parse .err content from a string.\n\n    Args:\n        text: Raw .err file contents.\n\n    Returns:\n        Parsed ErrorReport.\n    \"\"\"\n    return _parse_err(text)\n</code></pre>"},{"location":"api/simulation/results/#errormessage","title":"ErrorMessage","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.err.ErrorMessage","title":"<code>idfkit.simulation.parsers.err.ErrorMessage</code>  <code>dataclass</code>","text":"<p>A single error/warning message from EnergyPlus.</p> <p>Attributes:</p> Name Type Description <code>severity</code> <code>str</code> <p>One of \"Fatal\", \"Severe\", \"Warning\", \"Info\".</p> <code>message</code> <code>str</code> <p>The primary message text.</p> <code>details</code> <code>tuple[str, ...]</code> <p>Additional continuation lines (<code>** ~~~   **</code> lines).</p> Source code in <code>src/idfkit/simulation/parsers/err.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass ErrorMessage:\n    \"\"\"A single error/warning message from EnergyPlus.\n\n    Attributes:\n        severity: One of \"Fatal\", \"Severe\", \"Warning\", \"Info\".\n        message: The primary message text.\n        details: Additional continuation lines (``** ~~~   **`` lines).\n    \"\"\"\n\n    severity: str\n    message: str\n    details: tuple[str, ...]\n</code></pre>"},{"location":"api/simulation/results/#htmlresult","title":"HTMLResult","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLResult","title":"<code>idfkit.simulation.parsers.html.HTMLResult</code>  <code>dataclass</code>","text":"<p>Parsed HTML tabular output from an EnergyPlus simulation.</p> <p>Attributes:</p> Name Type Description <code>tables</code> <code>list[HTMLTable]</code> <p>All tables extracted from the file, in document order.</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>@dataclass(slots=True)\nclass HTMLResult:\n    \"\"\"Parsed HTML tabular output from an EnergyPlus simulation.\n\n    Attributes:\n        tables: All tables extracted from the file, in document order.\n    \"\"\"\n\n    tables: list[HTMLTable] = field(default_factory=lambda: [])\n\n    @classmethod\n    def from_file(cls, path: Path | str, encoding: str = \"latin-1\") -&gt; HTMLResult:\n        \"\"\"Parse an EnergyPlus HTML output file.\n\n        Args:\n            path: Path to the HTML file (typically ``eplustblTable.html``\n                or ``eplusoutTable.html``).\n            encoding: File encoding (default ``latin-1``).\n\n        Returns:\n            Parsed [HTMLResult][idfkit.simulation.parsers.html.HTMLResult].\n        \"\"\"\n        with open(path, encoding=encoding, errors=\"replace\") as f:\n            return cls.from_string(f.read())\n\n    @classmethod\n    def from_string(cls, html: str) -&gt; HTMLResult:\n        \"\"\"Parse an HTML string.\n\n        Args:\n            html: The raw HTML content.\n\n        Returns:\n            Parsed [HTMLResult][idfkit.simulation.parsers.html.HTMLResult].\n        \"\"\"\n        parser = _EnergyPlusHTMLParser()\n        parser.feed(html)\n        return cls(tables=parser.tables)\n\n    def __len__(self) -&gt; int:\n        return len(self.tables)\n\n    def __getitem__(self, index: int) -&gt; HTMLTable:\n        return self.tables[index]\n\n    def __iter__(self) -&gt; Iterator[HTMLTable]:\n        return iter(self.tables)\n\n    # ------------------------------------------------------------------\n    # Convenience accessors (eppy-compatible patterns)\n    # ------------------------------------------------------------------\n\n    def titletable(self) -&gt; list[tuple[str, list[list[str]]]]:\n        \"\"\"Return ``(title, rows)`` pairs like eppy's ``readhtml.titletable``.\n\n        Each entry is ``(bold_title, [header_row, *data_rows])``.\n        \"\"\"\n        result: list[tuple[str, list[list[str]]]] = []\n        for t in self.tables:\n            combined = [t.header, *t.rows] if t.header else list(t.rows)\n            result.append((t.title, combined))\n        return result\n\n    def tablebyname(self, name: str) -&gt; HTMLTable | None:\n        \"\"\"Find first table whose title contains *name* (case-insensitive).\"\"\"\n        lower = name.lower()\n        for t in self.tables:\n            if lower in t.title.lower():\n                return t\n        return None\n\n    def tablebyindex(self, index: int) -&gt; HTMLTable | None:\n        \"\"\"Get a table by its zero-based position.\"\"\"\n        if 0 &lt;= index &lt; len(self.tables):\n            return self.tables[index]\n        return None\n\n    def tablesbyreport(self, report_name: str) -&gt; list[HTMLTable]:\n        \"\"\"Get all tables belonging to a specific report.\"\"\"\n        lower = report_name.lower()\n        return [t for t in self.tables if lower in t.report_name.lower()]\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLResult.tables","title":"<code>tables = field(default_factory=(lambda: []))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLResult.from_file","title":"<code>from_file(path, encoding='latin-1')</code>  <code>classmethod</code>","text":"<p>Parse an EnergyPlus HTML output file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the HTML file (typically <code>eplustblTable.html</code> or <code>eplusoutTable.html</code>).</p> required <code>encoding</code> <code>str</code> <p>File encoding (default <code>latin-1</code>).</p> <code>'latin-1'</code> <p>Returns:</p> Type Description <code>HTMLResult</code> <p>Parsed HTMLResult.</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>@classmethod\ndef from_file(cls, path: Path | str, encoding: str = \"latin-1\") -&gt; HTMLResult:\n    \"\"\"Parse an EnergyPlus HTML output file.\n\n    Args:\n        path: Path to the HTML file (typically ``eplustblTable.html``\n            or ``eplusoutTable.html``).\n        encoding: File encoding (default ``latin-1``).\n\n    Returns:\n        Parsed [HTMLResult][idfkit.simulation.parsers.html.HTMLResult].\n    \"\"\"\n    with open(path, encoding=encoding, errors=\"replace\") as f:\n        return cls.from_string(f.read())\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLResult.from_string","title":"<code>from_string(html)</code>  <code>classmethod</code>","text":"<p>Parse an HTML string.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>The raw HTML content.</p> required <p>Returns:</p> Type Description <code>HTMLResult</code> <p>Parsed HTMLResult.</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>@classmethod\ndef from_string(cls, html: str) -&gt; HTMLResult:\n    \"\"\"Parse an HTML string.\n\n    Args:\n        html: The raw HTML content.\n\n    Returns:\n        Parsed [HTMLResult][idfkit.simulation.parsers.html.HTMLResult].\n    \"\"\"\n    parser = _EnergyPlusHTMLParser()\n    parser.feed(html)\n    return cls(tables=parser.tables)\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLResult.titletable","title":"<code>titletable()</code>","text":"<p>Return <code>(title, rows)</code> pairs like eppy's <code>readhtml.titletable</code>.</p> <p>Each entry is <code>(bold_title, [header_row, *data_rows])</code>.</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>def titletable(self) -&gt; list[tuple[str, list[list[str]]]]:\n    \"\"\"Return ``(title, rows)`` pairs like eppy's ``readhtml.titletable``.\n\n    Each entry is ``(bold_title, [header_row, *data_rows])``.\n    \"\"\"\n    result: list[tuple[str, list[list[str]]]] = []\n    for t in self.tables:\n        combined = [t.header, *t.rows] if t.header else list(t.rows)\n        result.append((t.title, combined))\n    return result\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLResult.tablebyname","title":"<code>tablebyname(name)</code>","text":"<p>Find first table whose title contains name (case-insensitive).</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>def tablebyname(self, name: str) -&gt; HTMLTable | None:\n    \"\"\"Find first table whose title contains *name* (case-insensitive).\"\"\"\n    lower = name.lower()\n    for t in self.tables:\n        if lower in t.title.lower():\n            return t\n    return None\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLResult.tablebyindex","title":"<code>tablebyindex(index)</code>","text":"<p>Get a table by its zero-based position.</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>def tablebyindex(self, index: int) -&gt; HTMLTable | None:\n    \"\"\"Get a table by its zero-based position.\"\"\"\n    if 0 &lt;= index &lt; len(self.tables):\n        return self.tables[index]\n    return None\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLResult.tablesbyreport","title":"<code>tablesbyreport(report_name)</code>","text":"<p>Get all tables belonging to a specific report.</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>def tablesbyreport(self, report_name: str) -&gt; list[HTMLTable]:\n    \"\"\"Get all tables belonging to a specific report.\"\"\"\n    lower = report_name.lower()\n    return [t for t in self.tables if lower in t.report_name.lower()]\n</code></pre>"},{"location":"api/simulation/results/#htmltable","title":"HTMLTable","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLTable","title":"<code>idfkit.simulation.parsers.html.HTMLTable</code>  <code>dataclass</code>","text":"<p>A single table extracted from the EnergyPlus HTML output.</p> <p>Attributes:</p> Name Type Description <code>title</code> <code>str</code> <p>The bold title preceding the table (e.g. <code>\"Site and Source Energy\"</code>).</p> <code>header</code> <code>list[str]</code> <p>Column headers (first <code>&lt;tr&gt;</code> with <code>&lt;th&gt;</code> cells).</p> <code>rows</code> <code>list[list[str]]</code> <p>Data rows as lists of strings.</p> <code>report_name</code> <code>str</code> <p>The top-level report name (e.g. <code>\"Annual Building Utility Performance Summary\"</code>).</p> <code>for_string</code> <code>str</code> <p>The <code>\"For:\"</code> qualifier (e.g. <code>\"Entire Facility\"</code>).</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>@dataclass(slots=True)\nclass HTMLTable:\n    \"\"\"A single table extracted from the EnergyPlus HTML output.\n\n    Attributes:\n        title: The bold title preceding the table (e.g.\n            ``\"Site and Source Energy\"``).\n        header: Column headers (first ``&lt;tr&gt;`` with ``&lt;th&gt;`` cells).\n        rows: Data rows as lists of strings.\n        report_name: The top-level report name (e.g.\n            ``\"Annual Building Utility Performance Summary\"``).\n        for_string: The ``\"For:\"`` qualifier (e.g. ``\"Entire Facility\"``).\n    \"\"\"\n\n    title: str\n    header: list[str]\n    rows: list[list[str]]\n    report_name: str = \"\"\n    for_string: str = \"\"\n\n    def to_dict(self) -&gt; dict[str, dict[str, str]]:\n        \"\"\"Convert to a dict mapping row headers to {col_header: value}.\n\n        The first column is treated as the row key.  Remaining columns\n        are keyed by their respective column headers (starting at\n        index 1).  Duplicate row keys are silently overwritten by the\n        last occurrence.\n\n        This gives convenient dict-style access similar to eppy's\n        ``readhtml.named_grid_h``.\n        \"\"\"\n        result: dict[str, dict[str, str]] = {}\n        for row in self.rows:\n            if not row:\n                continue\n            row_key = row[0]\n            entry: dict[str, str] = {}\n            for i in range(1, len(self.header)):\n                if i &lt; len(row):\n                    entry[self.header[i]] = row[i]\n            result[row_key] = entry\n        return result\n</code></pre>"},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLTable.title","title":"<code>title</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLTable.header","title":"<code>header</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLTable.rows","title":"<code>rows</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLTable.report_name","title":"<code>report_name = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLTable.for_string","title":"<code>for_string = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/simulation/results/#idfkit.simulation.parsers.html.HTMLTable.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to a dict mapping row headers to {col_header: value}.</p> <p>The first column is treated as the row key.  Remaining columns are keyed by their respective column headers (starting at index 1).  Duplicate row keys are silently overwritten by the last occurrence.</p> <p>This gives convenient dict-style access similar to eppy's <code>readhtml.named_grid_h</code>.</p> Source code in <code>src/idfkit/simulation/parsers/html.py</code> <pre><code>def to_dict(self) -&gt; dict[str, dict[str, str]]:\n    \"\"\"Convert to a dict mapping row headers to {col_header: value}.\n\n    The first column is treated as the row key.  Remaining columns\n    are keyed by their respective column headers (starting at\n    index 1).  Duplicate row keys are silently overwritten by the\n    last occurrence.\n\n    This gives convenient dict-style access similar to eppy's\n    ``readhtml.named_grid_h``.\n    \"\"\"\n    result: dict[str, dict[str, str]] = {}\n    for row in self.rows:\n        if not row:\n            continue\n        row_key = row[0]\n        entry: dict[str, str] = {}\n        for i in range(1, len(self.header)):\n            if i &lt; len(row):\n                entry[self.header[i]] = row[i]\n        result[row_key] = entry\n    return result\n</code></pre>"},{"location":"api/simulation/runner/","title":"Runner API","text":"<p>Core simulation execution functions.</p>"},{"location":"api/simulation/runner/#simulate","title":"simulate","text":""},{"location":"api/simulation/runner/#idfkit.simulation.runner.simulate","title":"<code>idfkit.simulation.runner.simulate(model, weather, *, output_dir=None, energyplus=None, expand_objects=True, annual=False, design_day=False, output_prefix='eplus', output_suffix='C', readvars=False, timeout=3600.0, extra_args=None, cache=None, fs=None, on_progress=None)</code>","text":"<p>Run an EnergyPlus simulation.</p> <p>Creates an isolated run directory, writes the model, and executes EnergyPlus as a subprocess. The caller's model is not mutated.</p> <p>When expand_objects is <code>True</code> (the default) and the model contains <code>GroundHeatTransfer:Slab:*</code> or <code>GroundHeatTransfer:Basement:*</code> objects, the Slab and/or Basement ground heat-transfer preprocessors are run automatically before simulation.  This is equivalent to calling run_slab_preprocessor or run_basement_preprocessor individually, but happens transparently.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The EnergyPlus model to simulate.</p> required <code>weather</code> <code>str | Path</code> <p>Path to the weather file (.epw).</p> required <code>output_dir</code> <code>str | Path | None</code> <p>Directory for output files (default: auto temp dir).</p> <code>None</code> <code>energyplus</code> <code>EnergyPlusConfig | None</code> <p>Pre-configured EnergyPlus installation. If None, uses find_energyplus for auto-discovery.</p> <code>None</code> <code>expand_objects</code> <code>bool</code> <p>Run ExpandObjects before simulation.  When <code>True</code>, also runs the Slab and Basement ground heat-transfer preprocessors if the model contains the corresponding objects.</p> <code>True</code> <code>annual</code> <code>bool</code> <p>Run annual simulation (<code>-a</code> flag).</p> <code>False</code> <code>design_day</code> <code>bool</code> <p>Run design-day-only simulation (<code>-D</code> flag).</p> <code>False</code> <code>output_prefix</code> <code>str</code> <p>Prefix for output files (default \"eplus\").</p> <code>'eplus'</code> <code>output_suffix</code> <code>Literal['C', 'L', 'D']</code> <p>Output file naming suffix: <code>\"C\"</code> for combined table files (default), <code>\"L\"</code> for legacy separate table files, or <code>\"D\"</code> for timestamped separate files.</p> <code>'C'</code> <code>readvars</code> <code>bool</code> <p>Run ReadVarsESO after simulation (<code>-r</code> flag).</p> <code>False</code> <code>timeout</code> <code>float</code> <p>Maximum runtime in seconds (default 3600).</p> <code>3600.0</code> <code>extra_args</code> <code>list[str] | None</code> <p>Additional command-line arguments.</p> <code>None</code> <code>cache</code> <code>SimulationCache | None</code> <p>Optional simulation cache for content-hash lookups.</p> <code>None</code> <code>fs</code> <code>FileSystem | None</code> <p>Optional file system backend for storing results on remote storage (e.g., S3). When provided, <code>output_dir</code> is required and specifies the remote destination path. EnergyPlus runs locally in a temp directory; results are then uploaded to <code>output_dir</code> via fs after execution.</p> <p>Note</p> <p>The <code>fs</code> parameter handles output storage only. The <code>weather</code> file must be a local path \u2014 remote weather files are not automatically downloaded. For cloud workflows, download weather files first using WeatherDownloader or pre-stage them locally before calling <code>simulate()</code>.</p> <code>None</code> <code>on_progress</code> <code>Callable[[SimulationProgress], Any] | Literal['tqdm'] | None</code> <p>Optional callback invoked with a SimulationProgress event each time EnergyPlus emits a progress line (warmup iterations, simulation day changes, post-processing steps, etc.).  Pass <code>\"tqdm\"</code> to use a built-in tqdm progress bar (requires <code>pip install idfkit[progress]</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>SimulationResult</code> <p>SimulationResult with paths to output files.</p> <p>Raises:</p> Type Description <code>SimulationError</code> <p>On timeout, OS error, or missing weather file.</p> <code>ExpandObjectsError</code> <p>If a preprocessing step (ExpandObjects, Slab, or Basement) fails during automatic preprocessing.</p> <code>EnergyPlusNotFoundError</code> <p>If EnergyPlus cannot be found.</p> Source code in <code>src/idfkit/simulation/runner.py</code> <pre><code>def simulate(\n    model: IDFDocument,\n    weather: str | Path,\n    *,\n    output_dir: str | Path | None = None,\n    energyplus: EnergyPlusConfig | None = None,\n    expand_objects: bool = True,\n    annual: bool = False,\n    design_day: bool = False,\n    output_prefix: str = \"eplus\",\n    output_suffix: Literal[\"C\", \"L\", \"D\"] = \"C\",\n    readvars: bool = False,\n    timeout: float = 3600.0,\n    extra_args: list[str] | None = None,\n    cache: SimulationCache | None = None,\n    fs: FileSystem | None = None,\n    on_progress: Callable[[SimulationProgress], Any] | Literal[\"tqdm\"] | None = None,\n) -&gt; SimulationResult:\n    \"\"\"Run an EnergyPlus simulation.\n\n    Creates an isolated run directory, writes the model, and executes\n    EnergyPlus as a subprocess. The caller's model is not mutated.\n\n    When *expand_objects* is ``True`` (the default) and the model contains\n    ``GroundHeatTransfer:Slab:*`` or ``GroundHeatTransfer:Basement:*``\n    objects, the Slab and/or Basement ground heat-transfer preprocessors\n    are run automatically before simulation.  This is equivalent to\n    calling [run_slab_preprocessor][idfkit.simulation.expand.run_slab_preprocessor] or\n    [run_basement_preprocessor][idfkit.simulation.expand.run_basement_preprocessor]\n    individually, but happens transparently.\n\n    Args:\n        model: The EnergyPlus model to simulate.\n        weather: Path to the weather file (.epw).\n        output_dir: Directory for output files (default: auto temp dir).\n        energyplus: Pre-configured EnergyPlus installation. If None,\n            uses [find_energyplus][idfkit.simulation.config.find_energyplus] for auto-discovery.\n        expand_objects: Run ExpandObjects before simulation.  When\n            ``True``, also runs the Slab and Basement ground heat-transfer\n            preprocessors if the model contains the corresponding objects.\n        annual: Run annual simulation (``-a`` flag).\n        design_day: Run design-day-only simulation (``-D`` flag).\n        output_prefix: Prefix for output files (default \"eplus\").\n        output_suffix: Output file naming suffix: ``\"C\"`` for combined table\n            files (default), ``\"L\"`` for legacy separate table files, or\n            ``\"D\"`` for timestamped separate files.\n        readvars: Run ReadVarsESO after simulation (``-r`` flag).\n        timeout: Maximum runtime in seconds (default 3600).\n        extra_args: Additional command-line arguments.\n        cache: Optional simulation cache for content-hash lookups.\n        fs: Optional file system backend for storing results on remote\n            storage (e.g., S3). When provided, ``output_dir`` is required\n            and specifies the remote destination path. EnergyPlus runs\n            locally in a temp directory; results are then uploaded to\n            ``output_dir`` via *fs* after execution.\n\n            !!! note\n                The ``fs`` parameter handles **output storage only**.\n                The ``weather`` file must be a local path \u2014 remote weather\n                files are not automatically downloaded. For cloud workflows,\n                download weather files first using [WeatherDownloader][idfkit.weather.WeatherDownloader]\n                or pre-stage them locally before calling ``simulate()``.\n        on_progress: Optional callback invoked with a\n            [SimulationProgress][idfkit.simulation.progress.SimulationProgress] event\n            each time EnergyPlus emits a progress line (warmup iterations,\n            simulation day changes, post-processing steps, etc.).  Pass\n            ``\"tqdm\"`` to use a built-in tqdm progress bar (requires\n            ``pip install idfkit[progress]``).\n\n    Returns:\n        SimulationResult with paths to output files.\n\n    Raises:\n        SimulationError: On timeout, OS error, or missing weather file.\n        ExpandObjectsError: If a preprocessing step (ExpandObjects, Slab,\n            or Basement) fails during automatic preprocessing.\n        EnergyPlusNotFoundError: If EnergyPlus cannot be found.\n    \"\"\"\n    if fs is not None and output_dir is None:\n        msg = \"output_dir is required when using a file system backend\"\n        raise ValueError(msg)\n\n    progress_cb, progress_cleanup = resolve_on_progress(on_progress)\n\n    try:\n        config = resolve_config(energyplus)\n        weather_path = Path(weather).resolve()\n\n        if not weather_path.is_file():\n            msg = f\"Weather file not found: {weather_path}\"\n            raise SimulationError(msg)\n\n        cache_key: CacheKey | None = None\n        if cache is not None:\n            cache_key = cache.compute_key(\n                model,\n                weather_path,\n                expand_objects=expand_objects,\n                annual=annual,\n                design_day=design_day,\n                output_suffix=output_suffix,\n                readvars=readvars,\n                extra_args=extra_args,\n            )\n            cached = cache.get(cache_key)\n            if cached is not None:\n                return cached\n\n        # Copy model to avoid mutation\n        sim_model = model.copy()\n        ensure_sql_output(sim_model)\n\n        # Auto-preprocess ground heat-transfer objects when needed.\n        sim_model, ep_expand = maybe_preprocess(model, sim_model, config, weather_path, expand_objects)\n\n        # When using a remote fs, always run locally in a temp dir\n        local_output_dir = None if fs is not None else output_dir\n        run_dir = prepare_run_directory(local_output_dir, weather_path)\n        idf_path = run_dir / \"model.idf\"\n\n        from ..writers import write_idf\n\n        write_idf(sim_model, idf_path)\n\n        cmd = build_command(\n            config=config,\n            idf_path=idf_path,\n            weather_path=run_dir / weather_path.name,\n            output_dir=run_dir,\n            output_prefix=output_prefix,\n            output_suffix=output_suffix,\n            expand_objects=ep_expand,\n            annual=annual,\n            design_day=design_day,\n            readvars=readvars,\n            extra_args=extra_args,\n        )\n\n        start = time.monotonic()\n\n        if progress_cb is not None:\n            stdout, stderr, returncode = _run_with_progress(cmd, run_dir, timeout, start, progress_cb)\n        else:\n            stdout, stderr, returncode = _run_simple(cmd, run_dir, timeout, start)\n    finally:\n        if progress_cleanup is not None:\n            progress_cleanup()\n\n    elapsed = time.monotonic() - start\n\n    if fs is not None:\n        remote_dir = Path(str(output_dir))\n        upload_results(run_dir, remote_dir, fs)\n        result = SimulationResult(\n            run_dir=remote_dir,\n            success=returncode == 0,\n            exit_code=returncode,\n            stdout=stdout,\n            stderr=stderr,\n            runtime_seconds=elapsed,\n            output_prefix=output_prefix,\n            fs=fs,\n        )\n    else:\n        result = SimulationResult(\n            run_dir=run_dir,\n            success=returncode == 0,\n            exit_code=returncode,\n            stdout=stdout,\n            stderr=stderr,\n            runtime_seconds=elapsed,\n            output_prefix=output_prefix,\n        )\n    if cache is not None and cache_key is not None and result.success:\n        cache.put(cache_key, result)\n    return result\n</code></pre>"},{"location":"api/simulation/runner/#find_energyplus","title":"find_energyplus","text":""},{"location":"api/simulation/runner/#idfkit.simulation.config.find_energyplus","title":"<code>idfkit.simulation.config.find_energyplus(*, version=None, path=None)</code>","text":"<p>Find an EnergyPlus installation.</p> Discovery order <ol> <li>Explicit path argument.</li> <li><code>ENERGYPLUS_DIR</code> environment variable.</li> <li><code>energyplus</code> on <code>PATH</code> (via shutil.which).</li> <li>Platform-specific default directories (newest version first).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>tuple[int, int, int] | str | None</code> <p>Optional version filter. Accepts <code>(major, minor, patch)</code> tuple or a string like <code>\"24.1.0\"</code> or <code>\"24.1\"</code>.</p> <code>None</code> <code>path</code> <code>str | Path | None</code> <p>Explicit path to EnergyPlus install directory or executable.</p> <code>None</code> <p>Returns:</p> Type Description <code>EnergyPlusConfig</code> <p>Validated EnergyPlusConfig.</p> <p>Raises:</p> Type Description <code>EnergyPlusNotFoundError</code> <p>If no matching installation is found.</p> Source code in <code>src/idfkit/simulation/config.py</code> <pre><code>def find_energyplus(\n    *,\n    version: tuple[int, int, int] | str | None = None,\n    path: str | Path | None = None,\n) -&gt; EnergyPlusConfig:\n    \"\"\"Find an EnergyPlus installation.\n\n    Discovery order:\n        1. Explicit *path* argument.\n        2. ``ENERGYPLUS_DIR`` environment variable.\n        3. ``energyplus`` on ``PATH`` (via [shutil.which][]).\n        4. Platform-specific default directories (newest version first).\n\n    Args:\n        version: Optional version filter. Accepts ``(major, minor, patch)``\n            tuple or a string like ``\"24.1.0\"`` or ``\"24.1\"``.\n        path: Explicit path to EnergyPlus install directory or executable.\n\n    Returns:\n        Validated EnergyPlusConfig.\n\n    Raises:\n        EnergyPlusNotFoundError: If no matching installation is found.\n    \"\"\"\n    target_version = _normalize_version(version) if version is not None else None\n    searched: list[str] = []\n\n    # 1. Explicit path\n    if path is not None:\n        p = Path(path).resolve()\n        searched.append(str(p))\n        config = EnergyPlusConfig.from_path(p)\n        if target_version is not None and config.version != target_version:\n            raise EnergyPlusNotFoundError(searched)\n        return config\n\n    # 2-4. Try candidates from env var, PATH, and platform dirs\n    for candidate in _discovery_candidates():\n        searched.append(str(candidate))\n        result = _try_candidate(candidate, target_version)\n        if result is not None:\n            return result\n\n    raise EnergyPlusNotFoundError(searched)\n</code></pre>"},{"location":"api/simulation/runner/#energyplusconfig","title":"EnergyPlusConfig","text":""},{"location":"api/simulation/runner/#idfkit.simulation.config.EnergyPlusConfig","title":"<code>idfkit.simulation.config.EnergyPlusConfig</code>  <code>dataclass</code>","text":"<p>Validated EnergyPlus installation configuration.</p> <p>Attributes:</p> Name Type Description <code>executable</code> <code>Path</code> <p>Path to the energyplus executable.</p> <code>version</code> <code>tuple[int, int, int]</code> <p>Parsed version as (major, minor, patch).</p> <code>install_dir</code> <code>Path</code> <p>Root installation directory.</p> <code>idd_path</code> <code>Path</code> <p>Path to the Energy+.idd file.</p> Source code in <code>src/idfkit/simulation/config.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass EnergyPlusConfig:\n    \"\"\"Validated EnergyPlus installation configuration.\n\n    Attributes:\n        executable: Path to the energyplus executable.\n        version: Parsed version as (major, minor, patch).\n        install_dir: Root installation directory.\n        idd_path: Path to the Energy+.idd file.\n    \"\"\"\n\n    executable: Path\n    version: tuple[int, int, int]\n    install_dir: Path\n    idd_path: Path\n\n    @property\n    def weather_dir(self) -&gt; Path | None:\n        \"\"\"Path to the bundled WeatherData directory, if present.\"\"\"\n        d = self.install_dir / \"WeatherData\"\n        return d if d.is_dir() else None\n\n    @property\n    def schema_path(self) -&gt; Path | None:\n        \"\"\"Path to Energy+.schema.epJSON, if present.\"\"\"\n        p = self.install_dir / \"Energy+.schema.epJSON\"\n        return p if p.is_file() else None\n\n    @property\n    def expand_objects_exe(self) -&gt; Path | None:\n        \"\"\"Path to ExpandObjects executable, if present.\"\"\"\n        name = \"ExpandObjects.exe\" if platform.system() == \"Windows\" else \"ExpandObjects\"\n        p = self.install_dir / name\n        return p if p.is_file() else None\n\n    @property\n    def _preprocess_dir(self) -&gt; Path:\n        \"\"\"Path to the PreProcess/GrndTempCalc directory.\"\"\"\n        return self.install_dir / \"PreProcess\" / \"GrndTempCalc\"\n\n    @property\n    def slab_exe(self) -&gt; Path | None:\n        \"\"\"Path to the Slab ground heat-transfer preprocessor, if present.\"\"\"\n        name = \"Slab.exe\" if platform.system() == \"Windows\" else \"Slab\"\n        p = self._preprocess_dir / name\n        return p if p.is_file() else None\n\n    @property\n    def slab_idd(self) -&gt; Path | None:\n        \"\"\"Path to SlabGHT.idd, if present.\"\"\"\n        p = self._preprocess_dir / \"SlabGHT.idd\"\n        return p if p.is_file() else None\n\n    @property\n    def basement_exe(self) -&gt; Path | None:\n        \"\"\"Path to the Basement ground heat-transfer preprocessor, if present.\"\"\"\n        name = \"Basement.exe\" if platform.system() == \"Windows\" else \"Basement\"\n        p = self._preprocess_dir / name\n        return p if p.is_file() else None\n\n    @property\n    def basement_idd(self) -&gt; Path | None:\n        \"\"\"Path to BasementGHT.idd, if present.\"\"\"\n        p = self._preprocess_dir / \"BasementGHT.idd\"\n        return p if p.is_file() else None\n\n    @classmethod\n    def from_path(cls, path: str | Path) -&gt; EnergyPlusConfig:\n        \"\"\"Create config from an explicit installation path.\n\n        The path can point to either the installation directory or the\n        energyplus executable directly.\n\n        Args:\n            path: Path to EnergyPlus install directory or executable.\n\n        Returns:\n            Validated EnergyPlusConfig.\n\n        Raises:\n            EnergyPlusNotFoundError: If the path is not a valid installation.\n        \"\"\"\n        path = Path(path).resolve()\n\n        # If path is an executable, derive install dir\n        if path.is_file():\n            install_dir = path.parent\n            exe = path\n        else:\n            install_dir = path\n            exe_name = \"energyplus.exe\" if platform.system() == \"Windows\" else \"energyplus\"\n            exe = install_dir / exe_name\n\n        if not exe.is_file():\n            raise EnergyPlusNotFoundError([str(install_dir)])\n\n        idd = install_dir / \"Energy+.idd\"\n        if not idd.is_file():\n            raise EnergyPlusNotFoundError([str(install_dir)])\n\n        version = _extract_version(install_dir)\n        if version is None:\n            version = _extract_version_from_idd(idd)\n        if version is None:\n            raise EnergyPlusNotFoundError([str(install_dir)])\n\n        return cls(\n            executable=exe,\n            version=version,\n            install_dir=install_dir,\n            idd_path=idd,\n        )\n</code></pre>"},{"location":"api/simulation/runner/#idfkit.simulation.config.EnergyPlusConfig.version","title":"<code>version</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/runner/#idfkit.simulation.config.EnergyPlusConfig.executable","title":"<code>executable</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/runner/#idfkit.simulation.config.EnergyPlusConfig.install_dir","title":"<code>install_dir</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/runner/#idfkit.simulation.config.EnergyPlusConfig.idd_path","title":"<code>idd_path</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/","title":"SQL API","text":"<p>SQLite output database parsing and query interface.</p>"},{"location":"api/simulation/sql/#sqlresult","title":"SQLResult","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.SQLResult","title":"<code>idfkit.simulation.parsers.sql.SQLResult</code>","text":"<p>Query interface for an EnergyPlus SQLite output database.</p> <p>Opens the database in read-only mode and provides methods for retrieving time-series data, tabular reports, and variable metadata.</p> <p>Can be used as a context manager:</p> <pre><code>```python\nwith SQLResult(\"eplusout.sql\") as sql:\n    ts = sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\n```\n</code></pre> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>class SQLResult:\n    \"\"\"Query interface for an EnergyPlus SQLite output database.\n\n    Opens the database in read-only mode and provides methods for retrieving\n    time-series data, tabular reports, and variable metadata.\n\n    Can be used as a context manager:\n\n        ```python\n        with SQLResult(\"eplusout.sql\") as sql:\n            ts = sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\n        ```\n    \"\"\"\n\n    def __init__(self, db_path: str | Path) -&gt; None:\n        \"\"\"Open the SQLite database in read-only mode.\n\n        Args:\n            db_path: Path to the EnergyPlus ``.sql`` output file.\n        \"\"\"\n        self._db_path = Path(db_path)\n        self._conn = sqlite3.connect(f\"file:{self._db_path}?mode=ro\", uri=True)\n\n    def close(self) -&gt; None:\n        \"\"\"Close the database connection.\"\"\"\n        self._conn.close()\n\n    def __enter__(self) -&gt; SQLResult:\n        return self\n\n    def __exit__(self, *exc: object) -&gt; None:\n        self.close()\n\n    def get_timeseries(\n        self,\n        variable_name: str,\n        key_value: str = \"*\",\n        frequency: str | None = None,\n        environment: Environment | None = None,\n    ) -&gt; TimeSeriesResult:\n        \"\"\"Retrieve a time series for a variable.\n\n        Args:\n            variable_name: The output variable name.\n            key_value: The key value (e.g. zone name). Use ``\"*\"`` for\n                environment-level variables. Case-insensitive matching.\n            frequency: Optional frequency filter (e.g. ``\"Hourly\"``).\n            environment: Filter by environment type. ``None`` (default)\n                returns all data, ``\"annual\"`` returns only weather-file\n                run period data, and ``\"sizing\"`` returns only design-day data.\n\n        Returns:\n            A TimeSeriesResult with timestamps and values.\n\n        Raises:\n            KeyError: If the variable is not found in the database.\n            ValueError: If *environment* is not a recognized value.\n        \"\"\"\n        cur = self._conn.cursor()\n\n        # Find the matching ReportDataDictionary entry\n        query = (\n            \"SELECT rdd.ReportDataDictionaryIndex, rdd.Name, rdd.KeyValue, rdd.Units \"\n            \"FROM ReportDataDictionary rdd \"\n            \"WHERE rdd.Name = ?\"\n        )\n        params: list[object] = [variable_name]\n\n        if key_value != \"*\":\n            query += \" AND UPPER(rdd.KeyValue) = UPPER(?)\"\n            params.append(key_value)\n\n        if frequency is not None:\n            query += \" AND rdd.ReportingFrequency = ?\"\n            params.append(frequency)\n\n        cur.execute(query, params)\n        row = cur.fetchone()\n        if row is None:\n            raise KeyError(f\"Variable not found: {variable_name!r} (key={key_value!r})\")  # noqa: TRY003\n\n        rdd_index, name, key_val, units = row\n\n        # Get the reporting frequency\n        cur.execute(\n            \"SELECT ReportingFrequency FROM ReportDataDictionary WHERE ReportDataDictionaryIndex = ?\",\n            (rdd_index,),\n        )\n        freq_row = cur.fetchone()\n        raw_freq: str = str(freq_row[0]) if freq_row else \"\"\n        freq: str = _FREQUENCY_MAP.get(raw_freq, raw_freq) if raw_freq else \"Unknown\"\n\n        # Retrieve time-series data, filtering out warmup periods.\n        # COALESCE handles EnergyPlus versions where WarmupFlag is NULL.\n        ts_query = (\n            \"SELECT t.Year, t.Month, t.Day, t.Hour, t.Minute, rd.Value \"\n            \"FROM ReportData rd \"\n            \"JOIN Time t ON rd.TimeIndex = t.TimeIndex \"\n        )\n        ts_params: list[object] = [rdd_index]\n        conditions = [\n            \"rd.ReportDataDictionaryIndex = ?\",\n            \"COALESCE(t.WarmupFlag, 0) = 0\",\n        ]\n\n        if environment is not None:\n            ts_query += \"JOIN EnvironmentPeriods ep ON t.EnvironmentPeriodIndex = ep.EnvironmentPeriodIndex \"\n            if environment == \"sizing\":\n                conditions.append(f\"ep.EnvironmentType IN ({', '.join('?' for _ in _SIZING_ENV_TYPES)})\")\n                ts_params.extend(_SIZING_ENV_TYPES)\n            elif environment == \"annual\":\n                conditions.append(\"ep.EnvironmentType = ?\")\n                ts_params.append(_ANNUAL_ENV_TYPE)\n            else:\n                msg = f\"environment must be 'sizing', 'annual', or None, got {environment!r}\"\n                raise ValueError(msg)\n\n        ts_query += \"WHERE \" + \" AND \".join(conditions) + \" ORDER BY t.TimeIndex\"\n        cur.execute(ts_query, ts_params)\n\n        timestamps: list[datetime] = []\n        values: list[float] = []\n        for year, month, day, hour, minute, value in cur.fetchall():\n            ref_year = year if year and year &gt; 0 else _REFERENCE_YEAR\n            timestamps.append(_make_timestamp(ref_year, month, day, hour, minute))\n            values.append(float(value))\n\n        return TimeSeriesResult(\n            variable_name=name,\n            key_value=key_val,\n            units=units,\n            frequency=freq,\n            timestamps=tuple(timestamps),\n            values=tuple(values),\n        )\n\n    def get_tabular_data(\n        self,\n        report_name: str | None = None,\n        table_name: str | None = None,\n        row_name: str | None = None,\n        column_name: str | None = None,\n        report_for: str | None = None,\n    ) -&gt; list[TabularRow]:\n        \"\"\"Retrieve tabular report data.\n\n        Args:\n            report_name: Optional filter by report name.\n            table_name: Optional filter by table name.\n            row_name: Optional filter by row label.\n            column_name: Optional filter by column label.\n            report_for: Optional filter by report scope (e.g. ``\"Entire Facility\"``).\n\n        Returns:\n            List of TabularRow entries matching the filters.\n        \"\"\"\n        query = (\n            \"SELECT ReportName, ReportForString, TableName, RowName, \"\n            \"ColumnName, Units, Value \"\n            \"FROM TabularDataWithStrings\"\n        )\n        conditions: list[str] = []\n        params: list[str] = []\n\n        if report_name is not None:\n            conditions.append(\"ReportName = ?\")\n            params.append(report_name)\n        if table_name is not None:\n            conditions.append(\"TableName = ?\")\n            params.append(table_name)\n        if row_name is not None:\n            conditions.append(\"RowName = ?\")\n            params.append(row_name)\n        if column_name is not None:\n            conditions.append(\"ColumnName = ?\")\n            params.append(column_name)\n        if report_for is not None:\n            conditions.append(\"ReportForString = ?\")\n            params.append(report_for)\n\n        if conditions:\n            query += \" WHERE \" + \" AND \".join(conditions)\n\n        cur = self._conn.cursor()\n        cur.execute(query, params)\n\n        return [\n            TabularRow(\n                report_name=row[0],\n                report_for=row[1],\n                table_name=row[2],\n                row_name=row[3],\n                column_name=row[4],\n                units=row[5],\n                value=row[6],\n            )\n            for row in cur.fetchall()\n        ]\n\n    def get_tabular_value(\n        self,\n        report_name: str,\n        table_name: str,\n        row_name: str,\n        column_name: str,\n        report_for: str = \"Entire Facility\",\n    ) -&gt; str:\n        \"\"\"Retrieve a single tabular cell value.\n\n        Convenience wrapper around [get_tabular_data][idfkit.simulation.parsers.sql.SQLResult.get_tabular_data] that returns\n        exactly one cell value.\n\n        Args:\n            report_name: Report name (e.g. ``\"AnnualBuildingUtilityPerformanceSummary\"``).\n            table_name: Table name (e.g. ``\"End Uses\"``).\n            row_name: Row label (e.g. ``\"Heating\"``).\n            column_name: Column label (e.g. ``\"Electricity\"``).\n            report_for: Report scope (default ``\"Entire Facility\"``).\n\n        Returns:\n            The cell value as a string.\n\n        Raises:\n            KeyError: If no matching row is found or if multiple rows match.\n        \"\"\"\n        rows = self.get_tabular_data(\n            report_name=report_name,\n            table_name=table_name,\n            row_name=row_name,\n            column_name=column_name,\n            report_for=report_for,\n        )\n        if len(rows) == 0:\n            msg = (\n                f\"No tabular data found: report={report_name!r}, table={table_name!r}, \"\n                f\"row={row_name!r}, column={column_name!r}\"\n            )\n            raise KeyError(msg)\n        if len(rows) &gt; 1:\n            msg = (\n                f\"Multiple rows found ({len(rows)}): report={report_name!r}, table={table_name!r}, \"\n                f\"row={row_name!r}, column={column_name!r}\"\n            )\n            raise KeyError(msg)\n        return rows[0].value\n\n    def list_variables(self) -&gt; list[VariableInfo]:\n        \"\"\"List all available variables in the database.\n\n        Returns:\n            List of VariableInfo entries describing each variable.\n        \"\"\"\n        cur = self._conn.cursor()\n        cur.execute(\"SELECT Name, KeyValue, ReportingFrequency, Units, IsMeter, Type FROM ReportDataDictionary\")\n\n        return [\n            VariableInfo(\n                name=str(row[0]),\n                key_value=str(row[1]),\n                frequency=_FREQUENCY_MAP.get(str(row[2]), str(row[2])),\n                units=str(row[3]),\n                is_meter=bool(row[4]),\n                variable_type=str(row[5]) if row[5] else \"\",\n            )\n            for row in cur.fetchall()\n        ]\n\n    def list_environments(self) -&gt; list[EnvironmentInfo]:\n        \"\"\"List all environment periods in the database.\n\n        Returns:\n            List of EnvironmentInfo entries describing each period (e.g.\n            design days and run periods).\n        \"\"\"\n        cur = self._conn.cursor()\n        cur.execute(\n            \"SELECT EnvironmentPeriodIndex, EnvironmentName, EnvironmentType \"\n            \"FROM EnvironmentPeriods ORDER BY EnvironmentPeriodIndex\"\n        )\n        return [EnvironmentInfo(index=row[0], name=str(row[1]), environment_type=row[2]) for row in cur.fetchall()]\n\n    def list_reports(self) -&gt; list[str]:\n        \"\"\"List all available tabular report names.\n\n        Returns:\n            Sorted list of unique report names.\n        \"\"\"\n        cur = self._conn.cursor()\n        cur.execute(\"SELECT DISTINCT ReportName FROM TabularDataWithStrings ORDER BY ReportName\")\n        return [row[0] for row in cur.fetchall()]\n\n    def to_dataframe(\n        self,\n        variable_name: str,\n        key_value: str = \"*\",\n        frequency: str | None = None,\n        environment: Environment | None = None,\n    ) -&gt; Any:\n        \"\"\"Retrieve a time series as a pandas DataFrame.\n\n        This is a convenience wrapper around [get_timeseries][idfkit.simulation.parsers.sql.SQLResult.get_timeseries] that\n        directly returns a DataFrame.\n\n        Args:\n            variable_name: The output variable name.\n            key_value: The key value. Use ``\"*\"`` for environment-level variables.\n            frequency: Optional frequency filter.\n            environment: Filter by environment type (``None`` by default\n                for all data, ``\"annual\"`` for run periods, ``\"sizing\"`` for\n                design days).\n\n        Returns:\n            A pandas DataFrame with a ``timestamp`` index.\n\n        Raises:\n            ImportError: If pandas is not installed.\n            KeyError: If the variable is not found.\n        \"\"\"\n        ts = self.get_timeseries(variable_name, key_value, frequency, environment)\n        return ts.to_dataframe()\n\n    def query(self, sql: str, parameters: tuple[object, ...] = ()) -&gt; list[tuple[object, ...]]:\n        \"\"\"Execute a raw SQL query.\n\n        Args:\n            sql: The SQL query string.\n            parameters: Query parameters for parameterized queries.\n\n        Returns:\n            List of result tuples.\n        \"\"\"\n        cur = self._conn.cursor()\n        cur.execute(sql, parameters)\n        return cur.fetchall()\n</code></pre>"},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.SQLResult.get_timeseries","title":"<code>get_timeseries(variable_name, key_value='*', frequency=None, environment=None)</code>","text":"<p>Retrieve a time series for a variable.</p> <p>Parameters:</p> Name Type Description Default <code>variable_name</code> <code>str</code> <p>The output variable name.</p> required <code>key_value</code> <code>str</code> <p>The key value (e.g. zone name). Use <code>\"*\"</code> for environment-level variables. Case-insensitive matching.</p> <code>'*'</code> <code>frequency</code> <code>str | None</code> <p>Optional frequency filter (e.g. <code>\"Hourly\"</code>).</p> <code>None</code> <code>environment</code> <code>Environment | None</code> <p>Filter by environment type. <code>None</code> (default) returns all data, <code>\"annual\"</code> returns only weather-file run period data, and <code>\"sizing\"</code> returns only design-day data.</p> <code>None</code> <p>Returns:</p> Type Description <code>TimeSeriesResult</code> <p>A TimeSeriesResult with timestamps and values.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the variable is not found in the database.</p> <code>ValueError</code> <p>If environment is not a recognized value.</p> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>def get_timeseries(\n    self,\n    variable_name: str,\n    key_value: str = \"*\",\n    frequency: str | None = None,\n    environment: Environment | None = None,\n) -&gt; TimeSeriesResult:\n    \"\"\"Retrieve a time series for a variable.\n\n    Args:\n        variable_name: The output variable name.\n        key_value: The key value (e.g. zone name). Use ``\"*\"`` for\n            environment-level variables. Case-insensitive matching.\n        frequency: Optional frequency filter (e.g. ``\"Hourly\"``).\n        environment: Filter by environment type. ``None`` (default)\n            returns all data, ``\"annual\"`` returns only weather-file\n            run period data, and ``\"sizing\"`` returns only design-day data.\n\n    Returns:\n        A TimeSeriesResult with timestamps and values.\n\n    Raises:\n        KeyError: If the variable is not found in the database.\n        ValueError: If *environment* is not a recognized value.\n    \"\"\"\n    cur = self._conn.cursor()\n\n    # Find the matching ReportDataDictionary entry\n    query = (\n        \"SELECT rdd.ReportDataDictionaryIndex, rdd.Name, rdd.KeyValue, rdd.Units \"\n        \"FROM ReportDataDictionary rdd \"\n        \"WHERE rdd.Name = ?\"\n    )\n    params: list[object] = [variable_name]\n\n    if key_value != \"*\":\n        query += \" AND UPPER(rdd.KeyValue) = UPPER(?)\"\n        params.append(key_value)\n\n    if frequency is not None:\n        query += \" AND rdd.ReportingFrequency = ?\"\n        params.append(frequency)\n\n    cur.execute(query, params)\n    row = cur.fetchone()\n    if row is None:\n        raise KeyError(f\"Variable not found: {variable_name!r} (key={key_value!r})\")  # noqa: TRY003\n\n    rdd_index, name, key_val, units = row\n\n    # Get the reporting frequency\n    cur.execute(\n        \"SELECT ReportingFrequency FROM ReportDataDictionary WHERE ReportDataDictionaryIndex = ?\",\n        (rdd_index,),\n    )\n    freq_row = cur.fetchone()\n    raw_freq: str = str(freq_row[0]) if freq_row else \"\"\n    freq: str = _FREQUENCY_MAP.get(raw_freq, raw_freq) if raw_freq else \"Unknown\"\n\n    # Retrieve time-series data, filtering out warmup periods.\n    # COALESCE handles EnergyPlus versions where WarmupFlag is NULL.\n    ts_query = (\n        \"SELECT t.Year, t.Month, t.Day, t.Hour, t.Minute, rd.Value \"\n        \"FROM ReportData rd \"\n        \"JOIN Time t ON rd.TimeIndex = t.TimeIndex \"\n    )\n    ts_params: list[object] = [rdd_index]\n    conditions = [\n        \"rd.ReportDataDictionaryIndex = ?\",\n        \"COALESCE(t.WarmupFlag, 0) = 0\",\n    ]\n\n    if environment is not None:\n        ts_query += \"JOIN EnvironmentPeriods ep ON t.EnvironmentPeriodIndex = ep.EnvironmentPeriodIndex \"\n        if environment == \"sizing\":\n            conditions.append(f\"ep.EnvironmentType IN ({', '.join('?' for _ in _SIZING_ENV_TYPES)})\")\n            ts_params.extend(_SIZING_ENV_TYPES)\n        elif environment == \"annual\":\n            conditions.append(\"ep.EnvironmentType = ?\")\n            ts_params.append(_ANNUAL_ENV_TYPE)\n        else:\n            msg = f\"environment must be 'sizing', 'annual', or None, got {environment!r}\"\n            raise ValueError(msg)\n\n    ts_query += \"WHERE \" + \" AND \".join(conditions) + \" ORDER BY t.TimeIndex\"\n    cur.execute(ts_query, ts_params)\n\n    timestamps: list[datetime] = []\n    values: list[float] = []\n    for year, month, day, hour, minute, value in cur.fetchall():\n        ref_year = year if year and year &gt; 0 else _REFERENCE_YEAR\n        timestamps.append(_make_timestamp(ref_year, month, day, hour, minute))\n        values.append(float(value))\n\n    return TimeSeriesResult(\n        variable_name=name,\n        key_value=key_val,\n        units=units,\n        frequency=freq,\n        timestamps=tuple(timestamps),\n        values=tuple(values),\n    )\n</code></pre>"},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.SQLResult.get_tabular_data","title":"<code>get_tabular_data(report_name=None, table_name=None, row_name=None, column_name=None, report_for=None)</code>","text":"<p>Retrieve tabular report data.</p> <p>Parameters:</p> Name Type Description Default <code>report_name</code> <code>str | None</code> <p>Optional filter by report name.</p> <code>None</code> <code>table_name</code> <code>str | None</code> <p>Optional filter by table name.</p> <code>None</code> <code>row_name</code> <code>str | None</code> <p>Optional filter by row label.</p> <code>None</code> <code>column_name</code> <code>str | None</code> <p>Optional filter by column label.</p> <code>None</code> <code>report_for</code> <code>str | None</code> <p>Optional filter by report scope (e.g. <code>\"Entire Facility\"</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[TabularRow]</code> <p>List of TabularRow entries matching the filters.</p> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>def get_tabular_data(\n    self,\n    report_name: str | None = None,\n    table_name: str | None = None,\n    row_name: str | None = None,\n    column_name: str | None = None,\n    report_for: str | None = None,\n) -&gt; list[TabularRow]:\n    \"\"\"Retrieve tabular report data.\n\n    Args:\n        report_name: Optional filter by report name.\n        table_name: Optional filter by table name.\n        row_name: Optional filter by row label.\n        column_name: Optional filter by column label.\n        report_for: Optional filter by report scope (e.g. ``\"Entire Facility\"``).\n\n    Returns:\n        List of TabularRow entries matching the filters.\n    \"\"\"\n    query = (\n        \"SELECT ReportName, ReportForString, TableName, RowName, \"\n        \"ColumnName, Units, Value \"\n        \"FROM TabularDataWithStrings\"\n    )\n    conditions: list[str] = []\n    params: list[str] = []\n\n    if report_name is not None:\n        conditions.append(\"ReportName = ?\")\n        params.append(report_name)\n    if table_name is not None:\n        conditions.append(\"TableName = ?\")\n        params.append(table_name)\n    if row_name is not None:\n        conditions.append(\"RowName = ?\")\n        params.append(row_name)\n    if column_name is not None:\n        conditions.append(\"ColumnName = ?\")\n        params.append(column_name)\n    if report_for is not None:\n        conditions.append(\"ReportForString = ?\")\n        params.append(report_for)\n\n    if conditions:\n        query += \" WHERE \" + \" AND \".join(conditions)\n\n    cur = self._conn.cursor()\n    cur.execute(query, params)\n\n    return [\n        TabularRow(\n            report_name=row[0],\n            report_for=row[1],\n            table_name=row[2],\n            row_name=row[3],\n            column_name=row[4],\n            units=row[5],\n            value=row[6],\n        )\n        for row in cur.fetchall()\n    ]\n</code></pre>"},{"location":"api/simulation/sql/#timeseriesresult","title":"TimeSeriesResult","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult","title":"<code>idfkit.simulation.parsers.sql.TimeSeriesResult</code>  <code>dataclass</code>","text":"<p>A single time series extracted from an EnergyPlus SQL database.</p> <p>Attributes:</p> Name Type Description <code>variable_name</code> <code>str</code> <p>The output variable name.</p> <code>key_value</code> <code>str</code> <p>The key value (e.g. zone or surface name).</p> <code>units</code> <code>str</code> <p>The variable units.</p> <code>frequency</code> <code>str</code> <p>The reporting frequency.</p> <code>timestamps</code> <code>tuple[datetime, ...]</code> <p>Timestamp for each data point.</p> <code>values</code> <code>tuple[float, ...]</code> <p>Numeric values for each data point.</p> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass TimeSeriesResult:\n    \"\"\"A single time series extracted from an EnergyPlus SQL database.\n\n    Attributes:\n        variable_name: The output variable name.\n        key_value: The key value (e.g. zone or surface name).\n        units: The variable units.\n        frequency: The reporting frequency.\n        timestamps: Timestamp for each data point.\n        values: Numeric values for each data point.\n    \"\"\"\n\n    variable_name: str\n    key_value: str\n    units: str\n    frequency: str\n    timestamps: tuple[datetime, ...]\n    values: tuple[float, ...]\n\n    def to_dataframe(self) -&gt; Any:\n        \"\"\"Convert to a pandas DataFrame.\n\n        Requires pandas to be installed.\n\n        Returns:\n            A DataFrame with a ``timestamp`` index and a column for the values.\n\n        Raises:\n            ImportError: If pandas is not installed.\n        \"\"\"\n        try:\n            import pandas  # type: ignore[import-not-found]\n        except ImportError:\n            msg = \"pandas is required for DataFrame conversion. Install it with: pip install idfkit[dataframes]\"\n            raise ImportError(msg) from None\n        pd: Any = pandas\n        return pd.DataFrame(  # type: ignore[no-any-return]\n            {\"timestamp\": list(self.timestamps), self.variable_name: list(self.values)}\n        ).set_index(\"timestamp\")\n\n    def plot(self, *, backend: Any = None, title: str | None = None) -&gt; Any:\n        \"\"\"Plot this time series as a line chart.\n\n        Auto-detects the plotting backend if not provided. Requires matplotlib\n        or plotly to be installed.\n\n        Args:\n            backend: A PlotBackend instance. If not provided, auto-detects.\n            title: Optional plot title. Defaults to ``\"key_value: variable_name\"``.\n\n        Returns:\n            A figure object from the backend.\n\n        Raises:\n            ImportError: If no plotting backend is available.\n        \"\"\"\n        if backend is None:\n            from ..plotting import get_default_backend\n\n            backend = get_default_backend()\n        plot_title = title or f\"{self.key_value}: {self.variable_name}\"\n        return backend.line(\n            list(self.timestamps),\n            list(self.values),\n            title=plot_title,\n            xlabel=\"Time\",\n            ylabel=f\"{self.variable_name} ({self.units})\",\n            label=self.key_value,\n        )\n</code></pre>"},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult.variable_name","title":"<code>variable_name</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult.key_value","title":"<code>key_value</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult.units","title":"<code>units</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult.frequency","title":"<code>frequency</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult.timestamps","title":"<code>timestamps</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult.values","title":"<code>values</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult.to_dataframe","title":"<code>to_dataframe()</code>","text":"<p>Convert to a pandas DataFrame.</p> <p>Requires pandas to be installed.</p> <p>Returns:</p> Type Description <code>Any</code> <p>A DataFrame with a <code>timestamp</code> index and a column for the values.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pandas is not installed.</p> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>def to_dataframe(self) -&gt; Any:\n    \"\"\"Convert to a pandas DataFrame.\n\n    Requires pandas to be installed.\n\n    Returns:\n        A DataFrame with a ``timestamp`` index and a column for the values.\n\n    Raises:\n        ImportError: If pandas is not installed.\n    \"\"\"\n    try:\n        import pandas  # type: ignore[import-not-found]\n    except ImportError:\n        msg = \"pandas is required for DataFrame conversion. Install it with: pip install idfkit[dataframes]\"\n        raise ImportError(msg) from None\n    pd: Any = pandas\n    return pd.DataFrame(  # type: ignore[no-any-return]\n        {\"timestamp\": list(self.timestamps), self.variable_name: list(self.values)}\n    ).set_index(\"timestamp\")\n</code></pre>"},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TimeSeriesResult.plot","title":"<code>plot(*, backend=None, title=None)</code>","text":"<p>Plot this time series as a line chart.</p> <p>Auto-detects the plotting backend if not provided. Requires matplotlib or plotly to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>Any</code> <p>A PlotBackend instance. If not provided, auto-detects.</p> <code>None</code> <code>title</code> <code>str | None</code> <p>Optional plot title. Defaults to <code>\"key_value: variable_name\"</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A figure object from the backend.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If no plotting backend is available.</p> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>def plot(self, *, backend: Any = None, title: str | None = None) -&gt; Any:\n    \"\"\"Plot this time series as a line chart.\n\n    Auto-detects the plotting backend if not provided. Requires matplotlib\n    or plotly to be installed.\n\n    Args:\n        backend: A PlotBackend instance. If not provided, auto-detects.\n        title: Optional plot title. Defaults to ``\"key_value: variable_name\"``.\n\n    Returns:\n        A figure object from the backend.\n\n    Raises:\n        ImportError: If no plotting backend is available.\n    \"\"\"\n    if backend is None:\n        from ..plotting import get_default_backend\n\n        backend = get_default_backend()\n    plot_title = title or f\"{self.key_value}: {self.variable_name}\"\n    return backend.line(\n        list(self.timestamps),\n        list(self.values),\n        title=plot_title,\n        xlabel=\"Time\",\n        ylabel=f\"{self.variable_name} ({self.units})\",\n        label=self.key_value,\n    )\n</code></pre>"},{"location":"api/simulation/sql/#tabularrow","title":"TabularRow","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TabularRow","title":"<code>idfkit.simulation.parsers.sql.TabularRow</code>  <code>dataclass</code>","text":"<p>A single row from an EnergyPlus tabular report.</p> <p>Attributes:</p> Name Type Description <code>report_name</code> <code>str</code> <p>The report name (e.g. <code>\"AnnualBuildingUtilityPerformanceSummary\"</code>).</p> <code>report_for</code> <code>str</code> <p>The report scope (e.g. <code>\"Entire Facility\"</code>).</p> <code>table_name</code> <code>str</code> <p>The table name within the report.</p> <code>row_name</code> <code>str</code> <p>The row label.</p> <code>column_name</code> <code>str</code> <p>The column label.</p> <code>units</code> <code>str</code> <p>The value units.</p> <code>value</code> <code>str</code> <p>The cell value as a string.</p> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass TabularRow:\n    \"\"\"A single row from an EnergyPlus tabular report.\n\n    Attributes:\n        report_name: The report name (e.g. ``\"AnnualBuildingUtilityPerformanceSummary\"``).\n        report_for: The report scope (e.g. ``\"Entire Facility\"``).\n        table_name: The table name within the report.\n        row_name: The row label.\n        column_name: The column label.\n        units: The value units.\n        value: The cell value as a string.\n    \"\"\"\n\n    report_name: str\n    report_for: str\n    table_name: str\n    row_name: str\n    column_name: str\n    units: str\n    value: str\n</code></pre>"},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TabularRow.report_name","title":"<code>report_name</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TabularRow.report_for","title":"<code>report_for</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TabularRow.table_name","title":"<code>table_name</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TabularRow.row_name","title":"<code>row_name</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TabularRow.column_name","title":"<code>column_name</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TabularRow.units","title":"<code>units</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.TabularRow.value","title":"<code>value</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#variableinfo","title":"VariableInfo","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.VariableInfo","title":"<code>idfkit.simulation.parsers.sql.VariableInfo</code>  <code>dataclass</code>","text":"<p>Metadata about an available variable or meter in the SQL database.</p> <p>This class represents both regular variables and meters because EnergyPlus stores them in a single <code>ReportDataDictionary</code> table, distinguished only by an <code>IsMeter</code> column.  Use the is_meter flag to tell them apart.</p> <p>For pre-simulation discovery from <code>.rdd</code> / <code>.mdd</code> files, see the separate OutputVariable and OutputMeter classes instead.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The variable name.</p> <code>key_value</code> <code>str</code> <p>The key value.  Empty for meters.</p> <code>frequency</code> <code>str</code> <p>The reporting frequency.</p> <code>units</code> <code>str</code> <p>The variable units.</p> <code>is_meter</code> <code>bool</code> <p>Whether this is a meter (vs. a regular variable).</p> <code>variable_type</code> <code>str</code> <p>The variable type string (e.g. <code>\"Zone\"</code>, <code>\"HVAC\"</code>).</p> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass VariableInfo:\n    \"\"\"Metadata about an available variable or meter in the SQL database.\n\n    This class represents both regular variables and meters because EnergyPlus\n    stores them in a single ``ReportDataDictionary`` table, distinguished only\n    by an ``IsMeter`` column.  Use the [is_meter][idfkit.simulation.parsers.sql.VariableInfo.is_meter] flag to tell them\n    apart.\n\n    For pre-simulation discovery from ``.rdd`` / ``.mdd`` files, see the\n    separate [OutputVariable][idfkit.simulation.parsers.rdd.OutputVariable] and\n    [OutputMeter][idfkit.simulation.parsers.rdd.OutputMeter] classes instead.\n\n    Attributes:\n        name: The variable name.\n        key_value: The key value.  Empty for meters.\n        frequency: The reporting frequency.\n        units: The variable units.\n        is_meter: Whether this is a meter (vs. a regular variable).\n        variable_type: The variable type string (e.g. ``\"Zone\"``, ``\"HVAC\"``).\n    \"\"\"\n\n    name: str\n    key_value: str\n    frequency: str\n    units: str\n    is_meter: bool\n    variable_type: str\n</code></pre>"},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.VariableInfo.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.VariableInfo.key_value","title":"<code>key_value</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.VariableInfo.frequency","title":"<code>frequency</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.VariableInfo.units","title":"<code>units</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.VariableInfo.is_meter","title":"<code>is_meter</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.VariableInfo.variable_type","title":"<code>variable_type</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#environmentinfo","title":"EnvironmentInfo","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.EnvironmentInfo","title":"<code>idfkit.simulation.parsers.sql.EnvironmentInfo</code>  <code>dataclass</code>","text":"<p>Metadata about a simulation environment period.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>The environment period index in the database.</p> <code>name</code> <code>str</code> <p>The environment name (e.g. <code>\"RUN PERIOD 1\"</code>).</p> <code>environment_type</code> <code>int</code> <p>The type integer (1 = DesignDay, 2 = DesignRunPeriod, 3 = WeatherFileRunPeriod).</p> Source code in <code>src/idfkit/simulation/parsers/sql.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass EnvironmentInfo:\n    \"\"\"Metadata about a simulation environment period.\n\n    Attributes:\n        index: The environment period index in the database.\n        name: The environment name (e.g. ``\"RUN PERIOD 1\"``).\n        environment_type: The type integer (1 = DesignDay, 2 = DesignRunPeriod,\n            3 = WeatherFileRunPeriod).\n    \"\"\"\n\n    index: int\n    name: str\n    environment_type: int\n</code></pre>"},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.EnvironmentInfo.index","title":"<code>index</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.EnvironmentInfo.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"api/simulation/sql/#idfkit.simulation.parsers.sql.EnvironmentInfo.environment_type","title":"<code>environment_type</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/","title":"Weather API Overview","text":"<p>The weather module provides station search, file downloads, and design day management.</p>"},{"location":"api/weather/#quick-reference","title":"Quick Reference","text":"Class/Function Description <code>StationIndex</code> Weather station search and filtering <code>WeatherStation</code> Station metadata container <code>WeatherDownloader</code> EPW/DDY file downloads <code>WeatherFiles</code> Downloaded file paths <code>DesignDayManager</code> Design day parsing and injection <code>DesignDayType</code> Design day type enumeration <code>apply_ashrae_sizing()</code> Quick design day application <code>geocode()</code> Address to coordinates"},{"location":"api/weather/#module-contents","title":"Module Contents","text":"<p>Weather station search, download, and design day injection.</p> <p>This sub-package provides tools for:</p> <ul> <li>Searching the climate.onebuilding.org TMYx station index by name,   coordinates, or metadata filters.</li> <li>Downloading EPW and DDY weather files with local caching.</li> <li>Parsing DDY files and injecting ASHRAE design day conditions into   EnergyPlus models.</li> <li>Geocoding addresses to coordinates via the free Nominatim API.</li> </ul>"},{"location":"api/weather/#idfkit.weather--station-index","title":"Station Index","text":"<p>The bundled index contains ~55,000 dataset entries representing ~17,300 unique physical weather stations worldwide. The difference is because each station may have multiple TMYx year-range variants (e.g., <code>TMYx.2007-2021</code>, <code>TMYx.2009-2023</code>), each stored as a separate entry with its own download URL.</p>"},{"location":"api/weather/#idfkit.weather--quick-start","title":"Quick Start","text":"<p>Search by name:</p> <pre><code>```python\nfrom idfkit.weather import StationIndex, WeatherDownloader\n\nindex = StationIndex.load()  # Instant, no network needed\nresults = index.search(\"chicago ohare\")\nstation = results[0].station\n\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\nprint(files.epw, files.ddy)\n```\n</code></pre>"},{"location":"api/weather/#idfkit.weather--search-by-address-splat-pattern","title":"Search by Address (Splat Pattern)","text":"<p>Combine geocode with nearest using the splat operator to find weather stations near any address:</p> <pre><code>```python\nfrom idfkit.weather import StationIndex, geocode\n\nindex = StationIndex.load()\n\n# Find stations near an address (one line!)\nresults = index.nearest(*geocode(\"350 Fifth Avenue, New York, NY\"))\n\nfor r in results[:3]:\n    print(f\"{r.station.display_name}: {r.distance_km:.0f} km\")\n\n# Output:\n# New York La Guardia AP, NY, USA: 10 km\n# New York J F Kennedy Intl AP, NY, USA: 18 km\n# Newark Liberty Intl AP, NJ, USA: 22 km\n```\n</code></pre> <p>The geocode function uses the free Nominatim (OpenStreetMap) service, which requires no API key. Requests are rate-limited to 1 per second.</p>"},{"location":"api/weather/#idfkit.weather--apply-design-days","title":"Apply Design Days","text":"<p>Inject ASHRAE sizing design days into your model:</p> <pre><code>```python\nfrom idfkit import load_idf\nfrom idfkit.weather import StationIndex, apply_ashrae_sizing\n\nmodel = load_idf(\"building.idf\")\nstation = StationIndex.load().search(\"chicago ohare\")[0].station\n\n# Apply ASHRAE 90.1 design conditions\nadded = apply_ashrae_sizing(model, station, standard=\"90.1\")\nprint(f\"Added {len(added)} design days\")\n```\n</code></pre>"},{"location":"api/weather/#idfkit.weather--index-freshness","title":"Index Freshness","text":"<p><code>StationIndex.load()</code> is purely local and requires no extra dependencies. Use <code>StationIndex.refresh()</code> (requires <code>openpyxl</code>) to re-download the upstream Excel indexes and rebuild the local cache:</p> <pre><code>```python\nif index.check_for_updates():\n    index = StationIndex.refresh()  # requires openpyxl\n```\n</code></pre>"},{"location":"api/weather/#idfkit.weather.NoDesignDaysError","title":"<code>NoDesignDaysError</code>","text":"<p>               Bases: <code>IdfKitError</code></p> <p>Raised when a DDY file contains no SizingPeriod:DesignDay objects.</p> <p>This typically occurs for weather stations that lack ASHRAE design conditions data in the climate.onebuilding.org database.</p> <p>Attributes:</p> Name Type Description <code>station_name</code> <p>Display name of the station (if available).</p> <code>ddy_path</code> <p>Path to the DDY file that was parsed.</p> <code>nearby_suggestions</code> <p>List of nearby stations that may have design days.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager","title":"<code>DesignDayManager</code>","text":"<p>Parse DDY files and inject design day conditions into IDF models.</p> <p>A DDY file is a valid IDF-syntax file containing <code>Site:Location</code> and <code>SizingPeriod:DesignDay</code> objects.  This class uses <code>[idfkit.load_idf][idfkit.load_idf]</code> to parse the file and classifies each design day by its ASHRAE condition type.</p> <p>Parameters:</p> Name Type Description Default <code>ddy_path</code> <code>Path | str</code> <p>Path to a <code>.ddy</code> file.</p> required <code>version</code> <code>tuple[int, int, int] | None</code> <p>EnergyPlus version to use for schema resolution.  Defaults to the latest supported version (design day fields are stable across versions).</p> <code>None</code>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.all_design_days","title":"<code>all_design_days</code>  <code>property</code>","text":"<p>All <code>SizingPeriod:DesignDay</code> objects from the DDY file.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.annual","title":"<code>annual</code>  <code>property</code>","text":"<p>All classified annual design day objects.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.monthly","title":"<code>monthly</code>  <code>property</code>","text":"<p>All monthly design day objects.</p> <p>Monthly design days follow the naming pattern <code>{Location} {Month} {pct}% Condns {type}</code> and are not classified into the DesignDayType enum.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.heating","title":"<code>heating</code>  <code>property</code>","text":"<p>All heating design days.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.cooling","title":"<code>cooling</code>  <code>property</code>","text":"<p>All cooling dry-bulb, wet-bulb, enthalpy, and dehumidification design days.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.location","title":"<code>location</code>  <code>property</code>","text":"<p>The <code>Site:Location</code> object from the DDY file, if present.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.from_station","title":"<code>from_station(station, *, dataset=None, version=None)</code>  <code>classmethod</code>","text":"<p>Download the DDY file for a station and parse it.</p> <p>Parameters:</p> Name Type Description Default <code>station</code> <code>WeatherStation</code> <p>The weather station.</p> required <code>dataset</code> <code>str | None</code> <p>TMYx variant to download.  Defaults to the most recent.</p> <code>None</code> <code>version</code> <code>tuple[int, int, int] | None</code> <p>EnergyPlus version for schema resolution.</p> <code>None</code>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.get","title":"<code>get(dd_type)</code>","text":"<p>Get a specific design day by type.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.raise_if_empty","title":"<code>raise_if_empty()</code>","text":"<p>Raise :exc:<code>NoDesignDaysError</code> if this DDY has no design days.</p> <p>When constructed via from_station, the error message includes suggestions for nearby stations that may have design day data.</p> <p>Raises:</p> Type Description <code>NoDesignDaysError</code> <p>If <code>all_design_days</code> is empty.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.apply_to_model","title":"<code>apply_to_model(model, *, heating='99.6%', cooling='1%', include_wet_bulb=False, include_enthalpy=False, include_dehumidification=False, include_wind=False, update_location=True, replace_existing=True)</code>","text":"<p>Inject design day objects into an IDF model.</p> <p>Selects the appropriate design days based on common ASHRAE sizing practices and adds them as <code>SizingPeriod:DesignDay</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The target IDFDocument.</p> required <code>heating</code> <code>Literal['99.6%', '99%', 'both']</code> <p>Which heating percentile to include.</p> <code>'99.6%'</code> <code>cooling</code> <code>Literal['0.4%', '1%', '2%', 'all']</code> <p>Which cooling dry-bulb percentile to include.</p> <code>'1%'</code> <code>include_wet_bulb</code> <code>bool</code> <p>Also include cooling wet-bulb design days.</p> <code>False</code> <code>include_enthalpy</code> <code>bool</code> <p>Also include cooling enthalpy design days.</p> <code>False</code> <code>include_dehumidification</code> <code>bool</code> <p>Also include dehumidification design days.</p> <code>False</code> <code>include_wind</code> <code>bool</code> <p>Also include heating wind-speed design days.</p> <code>False</code> <code>update_location</code> <code>bool</code> <p>Update the <code>Site:Location</code> object to match the DDY file metadata.</p> <code>True</code> <code>replace_existing</code> <code>bool</code> <p>Remove existing <code>SizingPeriod:DesignDay</code> objects before adding new ones.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of design day names that were added.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayManager.summary","title":"<code>summary()</code>","text":"<p>Human-readable summary of all design days in the DDY file.</p>"},{"location":"api/weather/#idfkit.weather.DesignDayType","title":"<code>DesignDayType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Classification of ASHRAE annual design day conditions.</p> <p>Values encode the condition type and annual percentile.</p>"},{"location":"api/weather/#idfkit.weather.WeatherDownloader","title":"<code>WeatherDownloader</code>","text":"<p>Download and cache weather files from climate.onebuilding.org.</p> <p>Downloaded ZIP archives are extracted and cached locally so that subsequent requests for the same station and dataset are served from disk without a network call.</p> <p>Examples:</p> <pre><code>from idfkit.weather import StationIndex, WeatherDownloader\n\nstation = StationIndex.load().search(\"chicago ohare\")[0].station\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\nprint(files.epw)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Path | None</code> <p>Override the default cache directory.</p> <code>None</code> <code>max_age</code> <code>timedelta | float | None</code> <p>Maximum age of cached files before re-downloading. Can be a timedelta or a number of seconds. If <code>None</code> (default), cached files never expire.</p> <code>None</code> Note <p>The cache has no size limit. For CI/CD environments with limited disk space, consider using clear_cache periodically or setting a <code>max_age</code> to force re-downloads of stale files.</p>"},{"location":"api/weather/#idfkit.weather.WeatherDownloader.download","title":"<code>download(station)</code>","text":"<p>Download and extract weather files for station.</p> <p>If the files are already cached and not stale, no network request is made.</p> <p>Parameters:</p> Name Type Description Default <code>station</code> <code>WeatherStation</code> <p>The weather station to download files for.</p> required <p>Returns:</p> Type Description <code>WeatherFiles</code> <p>A WeatherFiles with paths to the extracted files.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the download or extraction fails.</p>"},{"location":"api/weather/#idfkit.weather.WeatherDownloader.get_epw","title":"<code>get_epw(station)</code>","text":"<p>Download and return the path to the EPW file.</p>"},{"location":"api/weather/#idfkit.weather.WeatherDownloader.get_ddy","title":"<code>get_ddy(station)</code>","text":"<p>Download and return the path to the DDY file.</p>"},{"location":"api/weather/#idfkit.weather.WeatherDownloader.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Remove all cached weather files.</p> <p>This removes the entire <code>files/</code> subdirectory within the cache, which contains all downloaded ZIP archives and extracted files.</p>"},{"location":"api/weather/#idfkit.weather.WeatherFiles","title":"<code>WeatherFiles</code>  <code>dataclass</code>","text":"<p>Paths to downloaded and extracted weather files.</p> <p>Attributes:</p> Name Type Description <code>epw</code> <code>Path</code> <p>Path to the <code>.epw</code> file (always present after extraction).</p> <code>ddy</code> <code>Path</code> <p>Path to the <code>.ddy</code> file (always present after extraction).</p> <code>stat</code> <code>Path | None</code> <p>Path to the <code>.stat</code> file, or <code>None</code> if not included.</p> <code>zip_path</code> <code>Path</code> <p>Path to the original downloaded ZIP archive.</p> <code>station</code> <code>WeatherStation</code> <p>The station this download corresponds to.</p>"},{"location":"api/weather/#idfkit.weather.GeocodingError","title":"<code>GeocodingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an address cannot be geocoded.</p>"},{"location":"api/weather/#idfkit.weather.StationIndex","title":"<code>StationIndex</code>","text":"<p>Searchable index of weather stations from climate.onebuilding.org.</p> <p>Use load to load the bundled (or user-refreshed) station index. No network access or <code>openpyxl</code> is required for load.</p> <p>Use check_for_updates to see if upstream data has changed, and refresh to re-download and rebuild the index.</p> <p>Examples:</p> <pre><code>index = StationIndex.load()\nresults = index.search(\"chicago ohare\", limit=3)\nfor r in results:\n    print(r.station.display_name, r.score)\n</code></pre>"},{"location":"api/weather/#idfkit.weather.StationIndex.stations","title":"<code>stations</code>  <code>property</code>","text":"<p>All stations in the index.</p>"},{"location":"api/weather/#idfkit.weather.StationIndex.countries","title":"<code>countries</code>  <code>property</code>","text":"<p>Sorted list of unique country codes in the index.</p>"},{"location":"api/weather/#idfkit.weather.StationIndex.load","title":"<code>load(*, cache_dir=None)</code>  <code>classmethod</code>","text":"<p>Load the station index from a local compressed file.</p> <p>Checks for a user-refreshed cache first, then falls back to the bundled index shipped with the package.  No network access is required.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Path | None</code> <p>Override the default cache directory.</p> <code>None</code>"},{"location":"api/weather/#idfkit.weather.StationIndex.from_stations","title":"<code>from_stations(stations)</code>  <code>classmethod</code>","text":"<p>Create an index from an explicit list of stations (useful for tests).</p>"},{"location":"api/weather/#idfkit.weather.StationIndex.refresh","title":"<code>refresh(*, cache_dir=None)</code>  <code>classmethod</code>","text":"<p>Re-download Excel indexes from climate.onebuilding.org and rebuild the cache.</p> <p>Requires <code>openpyxl</code>.  Install with <code>pip install idfkit[weather]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Path | None</code> <p>Override the default cache directory.</p> <code>None</code>"},{"location":"api/weather/#idfkit.weather.StationIndex.check_for_updates","title":"<code>check_for_updates()</code>","text":"<p>Check if upstream Excel files have changed since this index was built.</p> <p>Sends lightweight HEAD requests to climate.onebuilding.org. Returns <code>True</code> if any file has a newer <code>Last-Modified</code> date. Returns <code>False</code> if all files match or if the check fails (offline, timeout, etc.).</p>"},{"location":"api/weather/#idfkit.weather.StationIndex.get_by_wmo","title":"<code>get_by_wmo(wmo)</code>","text":"<p>Look up stations by WMO number.</p> <p>Parameters:</p> Name Type Description Default <code>wmo</code> <code>str</code> <p>WMO station number as a string (e.g. <code>\"722950\"</code>).</p> required <p>Returns a list because a single WMO number can correspond to multiple stations or dataset variants.</p>"},{"location":"api/weather/#idfkit.weather.StationIndex.search","title":"<code>search(query, *, limit=10, country=None)</code>","text":"<p>Fuzzy-search stations by name, city, state, or WMO number.</p> <p>Matching is case-insensitive and uses substring / token-prefix heuristics (no external NLP dependencies).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Free-text search query.</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <code>country</code> <code>str | None</code> <p>If given, restrict to stations in this country code.</p> <code>None</code>"},{"location":"api/weather/#idfkit.weather.StationIndex.nearest","title":"<code>nearest(latitude, longitude, *, limit=5, max_distance_km=None, country=None)</code>","text":"<p>Find stations nearest to a geographic coordinate.</p> <p>Uses the Haversine formula for great-circle distance.  A bounding-box pre-filter is applied when max_distance_km is specified to avoid computing distances for stations that are obviously too far.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Decimal degrees, north positive.</p> required <code>longitude</code> <code>float</code> <p>Decimal degrees, east positive.</p> required <code>limit</code> <code>int</code> <p>Maximum results to return.</p> <code>5</code> <code>max_distance_km</code> <code>float | None</code> <p>Exclude stations farther than this.</p> <code>None</code> <code>country</code> <code>str | None</code> <p>If given, restrict to this country code.</p> <code>None</code>"},{"location":"api/weather/#idfkit.weather.StationIndex.filter","title":"<code>filter(*, country=None, state=None, wmo_region=None)</code>","text":"<p>Filter stations by metadata criteria.</p> <p>All specified criteria must match (logical AND).</p>"},{"location":"api/weather/#idfkit.weather.SearchResult","title":"<code>SearchResult</code>  <code>dataclass</code>","text":"<p>A text search result with relevance score.</p>"},{"location":"api/weather/#idfkit.weather.SearchResult.score","title":"<code>score</code>  <code>instance-attribute</code>","text":"<p>Relevance score from 0.0 to 1.0, higher is better.</p>"},{"location":"api/weather/#idfkit.weather.SearchResult.match_field","title":"<code>match_field</code>  <code>instance-attribute</code>","text":"<p>Which field matched: <code>\"wmo\"</code>, <code>\"name\"</code>, <code>\"state\"</code>, <code>\"country\"</code>.</p>"},{"location":"api/weather/#idfkit.weather.SpatialResult","title":"<code>SpatialResult</code>  <code>dataclass</code>","text":"<p>A spatial proximity result with great-circle distance.</p>"},{"location":"api/weather/#idfkit.weather.SpatialResult.distance_km","title":"<code>distance_km</code>  <code>instance-attribute</code>","text":"<p>Great-circle distance in kilometres.</p>"},{"location":"api/weather/#idfkit.weather.WeatherStation","title":"<code>WeatherStation</code>  <code>dataclass</code>","text":"<p>Metadata for a single weather file entry from climate.onebuilding.org.</p> <p>Each instance represents one downloadable weather dataset. The same physical station may appear multiple times with different <code>source</code> or year-range variants (e.g. <code>TMYx.2007-2021</code> vs <code>TMYx.2009-2023</code>).</p> <p>Attributes:</p> Name Type Description <code>country</code> <code>str</code> <p>ISO 3166 country code (e.g. <code>\"USA\"</code>).</p> <code>state</code> <code>str</code> <p>State or province abbreviation (e.g. <code>\"CA\"</code>).</p> <code>city</code> <code>str</code> <p>City or station name as it appears in the index (e.g. <code>\"Marina.Muni.AP\"</code>).</p> <code>wmo</code> <code>str</code> <p>WMO station number as a string to preserve leading zeros (e.g. <code>\"722950\"</code> or <code>\"012345\"</code>).</p> <code>source</code> <code>str</code> <p>Dataset source identifier (e.g. <code>\"SRC-TMYx\"</code>).</p> <code>latitude</code> <code>float</code> <p>Decimal degrees, north positive.</p> <code>longitude</code> <code>float</code> <p>Decimal degrees, east positive.</p> <code>timezone</code> <code>float</code> <p>Hours offset from GMT (e.g. <code>-8.0</code>).</p> <code>elevation</code> <code>float</code> <p>Meters above sea level.</p> <code>url</code> <code>str</code> <p>Full download URL for the ZIP archive.</p>"},{"location":"api/weather/#idfkit.weather.WeatherStation.display_name","title":"<code>display_name</code>  <code>property</code>","text":"<p>Human-readable station name with location context.</p> <p>Dots in the city name are replaced with spaces for readability.</p>"},{"location":"api/weather/#idfkit.weather.WeatherStation.dataset_variant","title":"<code>dataset_variant</code>  <code>property</code>","text":"<p>Extract the TMYx dataset variant from the download URL.</p> <p>Returns a string like <code>\"TMYx\"</code>, <code>\"TMYx.2007-2021\"</code>, or <code>\"TMYx.2009-2023\"</code>.</p>"},{"location":"api/weather/#idfkit.weather.WeatherStation.to_dict","title":"<code>to_dict()</code>","text":"<p>Serialize to a plain dictionary for JSON storage.</p>"},{"location":"api/weather/#idfkit.weather.WeatherStation.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Deserialize from a plain dictionary.</p>"},{"location":"api/weather/#idfkit.weather.apply_ashrae_sizing","title":"<code>apply_ashrae_sizing(model, station, *, standard='general', version=None)</code>","text":"<p>Apply standard ASHRAE sizing design days to a model.</p> <p>This is the one-line convenience function for the most common use case.</p> Presets <ul> <li><code>\"90.1\"</code>: Heating 99.6% + Cooling 1% DB + Cooling 1% WB   (per ASHRAE Standard 90.1 requirements).</li> <li><code>\"general\"</code>: Heating 99.6% + Cooling 0.4% DB   (conservative general practice).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The IDFDocument to modify.</p> required <code>station</code> <code>WeatherStation</code> <p>Weather station whose DDY file to use.</p> required <code>standard</code> <code>Literal['90.1', 'general']</code> <p>ASHRAE preset to apply.</p> <code>'general'</code> <code>version</code> <code>tuple[int, int, int] | None</code> <p>EnergyPlus version for schema resolution.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of design day names that were added.</p> <p>Raises:</p> Type Description <code>NoDesignDaysError</code> <p>If the station's DDY file contains no design days. The exception includes suggestions for nearby stations that may have design day data.</p>"},{"location":"api/weather/designday/","title":"Design Days API","text":"<p>Design day parsing, classification, and model injection.</p>"},{"location":"api/weather/designday/#designdaymanager","title":"DesignDayManager","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayManager","title":"<code>idfkit.weather.designday.DesignDayManager</code>","text":"<p>Parse DDY files and inject design day conditions into IDF models.</p> <p>A DDY file is a valid IDF-syntax file containing <code>Site:Location</code> and <code>SizingPeriod:DesignDay</code> objects.  This class uses <code>[idfkit.load_idf][idfkit.load_idf]</code> to parse the file and classifies each design day by its ASHRAE condition type.</p> <p>Parameters:</p> Name Type Description Default <code>ddy_path</code> <code>Path | str</code> <p>Path to a <code>.ddy</code> file.</p> required <code>version</code> <code>tuple[int, int, int] | None</code> <p>EnergyPlus version to use for schema resolution.  Defaults to the latest supported version (design day fields are stable across versions).</p> <code>None</code> Source code in <code>src/idfkit/weather/designday.py</code> <pre><code>class DesignDayManager:\n    \"\"\"Parse DDY files and inject design day conditions into IDF models.\n\n    A DDY file is a valid IDF-syntax file containing ``Site:Location`` and\n    ``SizingPeriod:DesignDay`` objects.  This class uses `[idfkit.load_idf][idfkit.load_idf]`\n    to parse the file and classifies each design day by its ASHRAE condition\n    type.\n\n    Args:\n        ddy_path: Path to a ``.ddy`` file.\n        version: EnergyPlus version to use for schema resolution.  Defaults\n            to the latest supported version (design day fields are stable\n            across versions).\n    \"\"\"\n\n    __slots__ = (\"_all_objects\", \"_design_days\", \"_doc\", \"_location\", \"_path\", \"_station\")\n\n    def __init__(\n        self,\n        ddy_path: Path | str,\n        version: tuple[int, int, int] | None = None,\n        *,\n        station: WeatherStation | None = None,\n    ) -&gt; None:\n        from ..idf_parser import parse_idf\n\n        self._path = Path(ddy_path)\n        self._doc = parse_idf(self._path, version=version or LATEST_VERSION)\n        self._design_days: dict[DesignDayType, IDFObject] = {}\n        self._all_objects: list[IDFObject] = []\n        self._location: IDFObject | None = None\n        self._station: WeatherStation | None = station\n        self._parse()\n\n    def _parse(self) -&gt; None:\n        \"\"\"Parse and classify the design days in the document.\"\"\"\n        # Extract Site:Location\n        if \"Site:Location\" in self._doc:\n            collection = self._doc[\"Site:Location\"]\n            if len(collection) &gt; 0:\n                self._location = next(iter(collection))\n\n        # Store all SizingPeriod:DesignDay objects and classify annual ones\n        if \"SizingPeriod:DesignDay\" in self._doc:\n            for dd in self._doc[\"SizingPeriod:DesignDay\"]:\n                self._all_objects.append(dd)\n                dd_type = _classify_design_day(dd.name)\n                if dd_type is not None:\n                    self._design_days[dd_type] = dd\n\n    @classmethod\n    def from_station(\n        cls,\n        station: WeatherStation,\n        *,\n        dataset: str | None = None,\n        version: tuple[int, int, int] | None = None,\n    ) -&gt; DesignDayManager:\n        \"\"\"Download the DDY file for a station and parse it.\n\n        Args:\n            station: The weather station.\n            dataset: TMYx variant to download.  Defaults to the most recent.\n            version: EnergyPlus version for schema resolution.\n        \"\"\"\n        from .download import WeatherDownloader\n\n        # If a specific dataset is requested we need to find the matching\n        # station entry (same WMO, matching URL).  For now, download whatever\n        # the station's URL points to.\n        _ = dataset  # reserved for future dataset selection\n        downloader = WeatherDownloader()\n        ddy_path = downloader.get_ddy(station)\n        return cls(ddy_path, version=version, station=station)\n\n    # --- Accessors ----------------------------------------------------------\n\n    @property\n    def all_design_days(self) -&gt; list[IDFObject]:\n        \"\"\"All ``SizingPeriod:DesignDay`` objects from the DDY file.\"\"\"\n        return list(self._all_objects)\n\n    @property\n    def annual(self) -&gt; list[IDFObject]:\n        \"\"\"All classified annual design day objects.\"\"\"\n        return list(self._design_days.values())\n\n    @property\n    def monthly(self) -&gt; list[IDFObject]:\n        \"\"\"All monthly design day objects.\n\n        Monthly design days follow the naming pattern\n        ``{Location} {Month} {pct}% Condns {type}`` and are not classified\n        into the [DesignDayType][idfkit.weather.designday.DesignDayType] enum.\n        \"\"\"\n        return [dd for dd in self._all_objects if _MONTH_PATTERN.search(dd.name)]\n\n    def get(self, dd_type: DesignDayType) -&gt; IDFObject | None:\n        \"\"\"Get a specific design day by type.\"\"\"\n        return self._design_days.get(dd_type)\n\n    @property\n    def heating(self) -&gt; list[IDFObject]:\n        \"\"\"All heating design days.\"\"\"\n        return [dd for t, dd in self._design_days.items() if t.value.startswith(\"heating\")]\n\n    @property\n    def cooling(self) -&gt; list[IDFObject]:\n        \"\"\"All cooling dry-bulb, wet-bulb, enthalpy, and dehumidification design days.\"\"\"\n        return [dd for t, dd in self._design_days.items() if t.value.startswith((\"cooling\", \"dehumid\"))]\n\n    @property\n    def location(self) -&gt; IDFObject | None:\n        \"\"\"The ``Site:Location`` object from the DDY file, if present.\"\"\"\n        return self._location\n\n    def raise_if_empty(self) -&gt; None:\n        \"\"\"Raise :exc:`NoDesignDaysError` if this DDY has no design days.\n\n        When constructed via [from_station][idfkit.weather.designday.DesignDayManager.from_station], the error message includes\n        suggestions for nearby stations that may have design day data.\n\n        Raises:\n            NoDesignDaysError: If ``all_design_days`` is empty.\n        \"\"\"\n        if self._all_objects:\n            return\n\n        station_name: str | None = None\n        nearby: list[str] = []\n\n        # Extract station name from Site:Location if available\n        if self._location is not None:\n            station_name = self._location.name\n        elif self._station is not None:\n            station_name = self._station.display_name\n\n        # Find nearby stations if we have coordinates\n        if self._station is not None:\n            from .index import StationIndex\n\n            try:\n                index = StationIndex.load()\n                results = index.nearest(\n                    self._station.latitude,\n                    self._station.longitude,\n                    limit=6,\n                )\n                for r in results:\n                    # Skip the current station\n                    if r.station.wmo == self._station.wmo:\n                        continue\n                    nearby.append(f\"{r.station.display_name} (WMO {r.station.wmo}, {r.distance_km:.0f} km)\")\n                    if len(nearby) &gt;= 5:\n                        break\n            except Exception:  # noqa: S110 - Don't let suggestion lookup break the error\n                pass\n\n        raise NoDesignDaysError(\n            station_name=station_name,\n            ddy_path=str(self._path),\n            nearby_suggestions=nearby,\n        )\n\n    # --- Injection ----------------------------------------------------------\n\n    @staticmethod\n    def _select_types(\n        *,\n        heating: str,\n        cooling: str,\n        include_wet_bulb: bool,\n        include_enthalpy: bool,\n        include_dehumidification: bool,\n        include_wind: bool,\n    ) -&gt; list[DesignDayType]:\n        \"\"\"Build the list of design day types to inject.\"\"\"\n        _P = DesignDayType\n        selected: list[DesignDayType] = []\n\n        # Heating\n        if heating in (\"99.6%\", \"both\"):\n            selected.append(_P.HEATING_99_6)\n        if heating in (\"99%\", \"both\"):\n            selected.append(_P.HEATING_99)\n\n        # Cooling dry-bulb\n        _cooling_db = {\"0.4%\": [_P.COOLING_DB_0_4], \"1%\": [_P.COOLING_DB_1], \"2%\": [_P.COOLING_DB_2]}\n        _cooling_db[\"all\"] = [_P.COOLING_DB_0_4, _P.COOLING_DB_1, _P.COOLING_DB_2]\n        selected.extend(_cooling_db.get(cooling, []))\n\n        # Cooling wet-bulb\n        if include_wet_bulb:\n            _wb = {\"0.4%\": [_P.COOLING_WB_0_4], \"1%\": [_P.COOLING_WB_1], \"2%\": [_P.COOLING_WB_2]}\n            _wb[\"all\"] = [_P.COOLING_WB_0_4, _P.COOLING_WB_1, _P.COOLING_WB_2]\n            selected.extend(_wb.get(cooling, []))\n\n        # Cooling enthalpy\n        if include_enthalpy:\n            _e = {\"0.4%\": [_P.COOLING_ENTH_0_4], \"1%\": [_P.COOLING_ENTH_1], \"2%\": [_P.COOLING_ENTH_2]}\n            _e[\"all\"] = [_P.COOLING_ENTH_0_4, _P.COOLING_ENTH_1, _P.COOLING_ENTH_2]\n            selected.extend(_e.get(cooling, []))\n\n        # Dehumidification\n        if include_dehumidification:\n            _d = {\"0.4%\": [_P.DEHUMID_0_4], \"1%\": [_P.DEHUMID_1], \"2%\": [_P.DEHUMID_2]}\n            _d[\"all\"] = [_P.DEHUMID_0_4, _P.DEHUMID_1, _P.DEHUMID_2]\n            selected.extend(_d.get(cooling, []))\n\n        # Wind (both OneBuilding \"Htg Wind\" and EnergyPlus \"Coldest Month WS\" formats)\n        if include_wind:\n            selected.extend([_P.HTG_WIND_99_6, _P.HTG_WIND_99, _P.WIND_0_4, _P.WIND_1])\n\n        return selected\n\n    def apply_to_model(\n        self,\n        model: IDFDocument,\n        *,\n        heating: Literal[\"99.6%\", \"99%\", \"both\"] = \"99.6%\",\n        cooling: Literal[\"0.4%\", \"1%\", \"2%\", \"all\"] = \"1%\",\n        include_wet_bulb: bool = False,\n        include_enthalpy: bool = False,\n        include_dehumidification: bool = False,\n        include_wind: bool = False,\n        update_location: bool = True,\n        replace_existing: bool = True,\n    ) -&gt; list[str]:\n        \"\"\"Inject design day objects into an IDF model.\n\n        Selects the appropriate design days based on common ASHRAE sizing\n        practices and adds them as ``SizingPeriod:DesignDay`` objects.\n\n        Args:\n            model: The target [IDFDocument][idfkit.document.IDFDocument].\n            heating: Which heating percentile to include.\n            cooling: Which cooling dry-bulb percentile to include.\n            include_wet_bulb: Also include cooling wet-bulb design days.\n            include_enthalpy: Also include cooling enthalpy design days.\n            include_dehumidification: Also include dehumidification design days.\n            include_wind: Also include heating wind-speed design days.\n            update_location: Update the ``Site:Location`` object to match the\n                DDY file metadata.\n            replace_existing: Remove existing ``SizingPeriod:DesignDay`` objects\n                before adding new ones.\n\n        Returns:\n            List of design day names that were added.\n        \"\"\"\n        selected_types = self._select_types(\n            heating=heating,\n            cooling=cooling,\n            include_wet_bulb=include_wet_bulb,\n            include_enthalpy=include_enthalpy,\n            include_dehumidification=include_dehumidification,\n            include_wind=include_wind,\n        )\n\n        # Remove existing design days if requested\n        if replace_existing and \"SizingPeriod:DesignDay\" in model:\n            existing = list(model[\"SizingPeriod:DesignDay\"])\n            model.removeidfobjects(existing)\n\n        # Add selected design days (as copies so the DDY document is not mutated)\n        added_names: list[str] = []\n        for dd_type in selected_types:\n            dd = self._design_days.get(dd_type)\n            if dd is not None:\n                model.copyidfobject(dd)\n                added_names.append(dd.name)\n\n        # Update Site:Location if requested\n        if update_location and self._location is not None:\n            if \"Site:Location\" in model:\n                existing_locs = list(model[\"Site:Location\"])\n                model.removeidfobjects(existing_locs)\n            model.copyidfobject(self._location)\n\n        return added_names\n\n    def summary(self) -&gt; str:\n        \"\"\"Human-readable summary of all design days in the DDY file.\"\"\"\n        monthly_count = len(self.monthly)\n        lines = [f\"Design days from: {self._path.name}\", \"\"]\n        if self._location:\n            lines.append(f\"  Location: {self._location.name}\")\n        lines.append(f\"  Design days found: {len(self._all_objects)}\")\n        lines.append(f\"  Annual (classified): {len(self._design_days)}\")\n        if monthly_count:\n            lines.append(f\"  Monthly: {monthly_count}\")\n        lines.append(\"\")\n        for dd_type, dd in sorted(self._design_days.items(), key=lambda x: x[0].value):\n            lines.append(f\"  [{dd_type.value}] {dd.name}\")\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayManager.annual","title":"<code>annual</code>  <code>property</code>","text":"<p>All classified annual design day objects.</p>"},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayManager.monthly","title":"<code>monthly</code>  <code>property</code>","text":"<p>All monthly design day objects.</p> <p>Monthly design days follow the naming pattern <code>{Location} {Month} {pct}% Condns {type}</code> and are not classified into the DesignDayType enum.</p>"},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayManager.location","title":"<code>location</code>  <code>property</code>","text":"<p>The <code>Site:Location</code> object from the DDY file, if present.</p>"},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayManager.get","title":"<code>get(dd_type)</code>","text":"<p>Get a specific design day by type.</p> Source code in <code>src/idfkit/weather/designday.py</code> <pre><code>def get(self, dd_type: DesignDayType) -&gt; IDFObject | None:\n    \"\"\"Get a specific design day by type.\"\"\"\n    return self._design_days.get(dd_type)\n</code></pre>"},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayManager.apply_to_model","title":"<code>apply_to_model(model, *, heating='99.6%', cooling='1%', include_wet_bulb=False, include_enthalpy=False, include_dehumidification=False, include_wind=False, update_location=True, replace_existing=True)</code>","text":"<p>Inject design day objects into an IDF model.</p> <p>Selects the appropriate design days based on common ASHRAE sizing practices and adds them as <code>SizingPeriod:DesignDay</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The target IDFDocument.</p> required <code>heating</code> <code>Literal['99.6%', '99%', 'both']</code> <p>Which heating percentile to include.</p> <code>'99.6%'</code> <code>cooling</code> <code>Literal['0.4%', '1%', '2%', 'all']</code> <p>Which cooling dry-bulb percentile to include.</p> <code>'1%'</code> <code>include_wet_bulb</code> <code>bool</code> <p>Also include cooling wet-bulb design days.</p> <code>False</code> <code>include_enthalpy</code> <code>bool</code> <p>Also include cooling enthalpy design days.</p> <code>False</code> <code>include_dehumidification</code> <code>bool</code> <p>Also include dehumidification design days.</p> <code>False</code> <code>include_wind</code> <code>bool</code> <p>Also include heating wind-speed design days.</p> <code>False</code> <code>update_location</code> <code>bool</code> <p>Update the <code>Site:Location</code> object to match the DDY file metadata.</p> <code>True</code> <code>replace_existing</code> <code>bool</code> <p>Remove existing <code>SizingPeriod:DesignDay</code> objects before adding new ones.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of design day names that were added.</p> Source code in <code>src/idfkit/weather/designday.py</code> <pre><code>def apply_to_model(\n    self,\n    model: IDFDocument,\n    *,\n    heating: Literal[\"99.6%\", \"99%\", \"both\"] = \"99.6%\",\n    cooling: Literal[\"0.4%\", \"1%\", \"2%\", \"all\"] = \"1%\",\n    include_wet_bulb: bool = False,\n    include_enthalpy: bool = False,\n    include_dehumidification: bool = False,\n    include_wind: bool = False,\n    update_location: bool = True,\n    replace_existing: bool = True,\n) -&gt; list[str]:\n    \"\"\"Inject design day objects into an IDF model.\n\n    Selects the appropriate design days based on common ASHRAE sizing\n    practices and adds them as ``SizingPeriod:DesignDay`` objects.\n\n    Args:\n        model: The target [IDFDocument][idfkit.document.IDFDocument].\n        heating: Which heating percentile to include.\n        cooling: Which cooling dry-bulb percentile to include.\n        include_wet_bulb: Also include cooling wet-bulb design days.\n        include_enthalpy: Also include cooling enthalpy design days.\n        include_dehumidification: Also include dehumidification design days.\n        include_wind: Also include heating wind-speed design days.\n        update_location: Update the ``Site:Location`` object to match the\n            DDY file metadata.\n        replace_existing: Remove existing ``SizingPeriod:DesignDay`` objects\n            before adding new ones.\n\n    Returns:\n        List of design day names that were added.\n    \"\"\"\n    selected_types = self._select_types(\n        heating=heating,\n        cooling=cooling,\n        include_wet_bulb=include_wet_bulb,\n        include_enthalpy=include_enthalpy,\n        include_dehumidification=include_dehumidification,\n        include_wind=include_wind,\n    )\n\n    # Remove existing design days if requested\n    if replace_existing and \"SizingPeriod:DesignDay\" in model:\n        existing = list(model[\"SizingPeriod:DesignDay\"])\n        model.removeidfobjects(existing)\n\n    # Add selected design days (as copies so the DDY document is not mutated)\n    added_names: list[str] = []\n    for dd_type in selected_types:\n        dd = self._design_days.get(dd_type)\n        if dd is not None:\n            model.copyidfobject(dd)\n            added_names.append(dd.name)\n\n    # Update Site:Location if requested\n    if update_location and self._location is not None:\n        if \"Site:Location\" in model:\n            existing_locs = list(model[\"Site:Location\"])\n            model.removeidfobjects(existing_locs)\n        model.copyidfobject(self._location)\n\n    return added_names\n</code></pre>"},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayManager.summary","title":"<code>summary()</code>","text":"<p>Human-readable summary of all design days in the DDY file.</p> Source code in <code>src/idfkit/weather/designday.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Human-readable summary of all design days in the DDY file.\"\"\"\n    monthly_count = len(self.monthly)\n    lines = [f\"Design days from: {self._path.name}\", \"\"]\n    if self._location:\n        lines.append(f\"  Location: {self._location.name}\")\n    lines.append(f\"  Design days found: {len(self._all_objects)}\")\n    lines.append(f\"  Annual (classified): {len(self._design_days)}\")\n    if monthly_count:\n        lines.append(f\"  Monthly: {monthly_count}\")\n    lines.append(\"\")\n    for dd_type, dd in sorted(self._design_days.items(), key=lambda x: x[0].value):\n        lines.append(f\"  [{dd_type.value}] {dd.name}\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/weather/designday/#designdaytype","title":"DesignDayType","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType","title":"<code>idfkit.weather.designday.DesignDayType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Classification of ASHRAE annual design day conditions.</p> <p>Values encode the condition type and annual percentile.</p> Source code in <code>src/idfkit/weather/designday.py</code> <pre><code>class DesignDayType(Enum):\n    \"\"\"Classification of ASHRAE annual design day conditions.\n\n    Values encode the condition type and annual percentile.\n    \"\"\"\n\n    HEATING_99_6 = \"heating_99.6\"\n    HEATING_99 = \"heating_99\"\n    COOLING_DB_0_4 = \"cooling_db_0.4\"\n    COOLING_DB_1 = \"cooling_db_1\"\n    COOLING_DB_2 = \"cooling_db_2\"\n    COOLING_WB_0_4 = \"cooling_wb_0.4\"\n    COOLING_WB_1 = \"cooling_wb_1\"\n    COOLING_WB_2 = \"cooling_wb_2\"\n    COOLING_ENTH_0_4 = \"cooling_enth_0.4\"\n    COOLING_ENTH_1 = \"cooling_enth_1\"\n    COOLING_ENTH_2 = \"cooling_enth_2\"\n    DEHUMID_0_4 = \"dehumid_0.4\"\n    DEHUMID_1 = \"dehumid_1\"\n    DEHUMID_2 = \"dehumid_2\"\n    HUMIDIFICATION_99_6 = \"humidif_99.6\"\n    HUMIDIFICATION_99 = \"humidif_99\"\n    HTG_WIND_99_6 = \"htg_wind_99.6\"\n    HTG_WIND_99 = \"htg_wind_99\"\n    WIND_0_4 = \"wind_0.4\"\n    WIND_1 = \"wind_1\"\n</code></pre>"},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.HEATING_99_6","title":"<code>HEATING_99_6 = 'heating_99.6'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.HEATING_99","title":"<code>HEATING_99 = 'heating_99'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.HTG_WIND_99_6","title":"<code>HTG_WIND_99_6 = 'htg_wind_99.6'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_DB_0_4","title":"<code>COOLING_DB_0_4 = 'cooling_db_0.4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_DB_1","title":"<code>COOLING_DB_1 = 'cooling_db_1'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_DB_2","title":"<code>COOLING_DB_2 = 'cooling_db_2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_WB_0_4","title":"<code>COOLING_WB_0_4 = 'cooling_wb_0.4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_WB_1","title":"<code>COOLING_WB_1 = 'cooling_wb_1'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_WB_2","title":"<code>COOLING_WB_2 = 'cooling_wb_2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_ENTH_0_4","title":"<code>COOLING_ENTH_0_4 = 'cooling_enth_0.4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_ENTH_1","title":"<code>COOLING_ENTH_1 = 'cooling_enth_1'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.COOLING_ENTH_2","title":"<code>COOLING_ENTH_2 = 'cooling_enth_2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.DEHUMID_0_4","title":"<code>DEHUMID_0_4 = 'dehumid_0.4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.DEHUMID_1","title":"<code>DEHUMID_1 = 'dehumid_1'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.DesignDayType.DEHUMID_2","title":"<code>DEHUMID_2 = 'dehumid_2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/weather/designday/#apply_ashrae_sizing","title":"apply_ashrae_sizing","text":""},{"location":"api/weather/designday/#idfkit.weather.designday.apply_ashrae_sizing","title":"<code>idfkit.weather.designday.apply_ashrae_sizing(model, station, *, standard='general', version=None)</code>","text":"<p>Apply standard ASHRAE sizing design days to a model.</p> <p>This is the one-line convenience function for the most common use case.</p> Presets <ul> <li><code>\"90.1\"</code>: Heating 99.6% + Cooling 1% DB + Cooling 1% WB   (per ASHRAE Standard 90.1 requirements).</li> <li><code>\"general\"</code>: Heating 99.6% + Cooling 0.4% DB   (conservative general practice).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>IDFDocument</code> <p>The IDFDocument to modify.</p> required <code>station</code> <code>WeatherStation</code> <p>Weather station whose DDY file to use.</p> required <code>standard</code> <code>Literal['90.1', 'general']</code> <p>ASHRAE preset to apply.</p> <code>'general'</code> <code>version</code> <code>tuple[int, int, int] | None</code> <p>EnergyPlus version for schema resolution.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of design day names that were added.</p> <p>Raises:</p> Type Description <code>NoDesignDaysError</code> <p>If the station's DDY file contains no design days. The exception includes suggestions for nearby stations that may have design day data.</p> Source code in <code>src/idfkit/weather/designday.py</code> <pre><code>def apply_ashrae_sizing(\n    model: IDFDocument,\n    station: WeatherStation,\n    *,\n    standard: Literal[\"90.1\", \"general\"] = \"general\",\n    version: tuple[int, int, int] | None = None,\n) -&gt; list[str]:\n    \"\"\"Apply standard ASHRAE sizing design days to a model.\n\n    This is the one-line convenience function for the most common use case.\n\n    Presets:\n        - ``\"90.1\"``: Heating 99.6% + Cooling 1% DB + Cooling 1% WB\n          (per ASHRAE Standard 90.1 requirements).\n        - ``\"general\"``: Heating 99.6% + Cooling 0.4% DB\n          (conservative general practice).\n\n    Args:\n        model: The [IDFDocument][idfkit.document.IDFDocument] to modify.\n        station: Weather station whose DDY file to use.\n        standard: ASHRAE preset to apply.\n        version: EnergyPlus version for schema resolution.\n\n    Returns:\n        List of design day names that were added.\n\n    Raises:\n        NoDesignDaysError: If the station's DDY file contains no design days.\n            The exception includes suggestions for nearby stations that may\n            have design day data.\n    \"\"\"\n    ddm = DesignDayManager.from_station(station, version=version)\n    ddm.raise_if_empty()\n    if standard == \"90.1\":\n        return ddm.apply_to_model(model, heating=\"99.6%\", cooling=\"1%\", include_wet_bulb=True)\n    return ddm.apply_to_model(model, heating=\"99.6%\", cooling=\"0.4%\")\n</code></pre>"},{"location":"api/weather/designday/#nodesigndayserror","title":"NoDesignDaysError","text":"<p>See <code>NoDesignDaysError</code> in the Exceptions API.</p>"},{"location":"api/weather/download/","title":"Download API","text":"<p>Weather file downloading and caching.</p>"},{"location":"api/weather/download/#weatherdownloader","title":"WeatherDownloader","text":""},{"location":"api/weather/download/#idfkit.weather.download.WeatherDownloader","title":"<code>idfkit.weather.download.WeatherDownloader</code>","text":"<p>Download and cache weather files from climate.onebuilding.org.</p> <p>Downloaded ZIP archives are extracted and cached locally so that subsequent requests for the same station and dataset are served from disk without a network call.</p> <p>Examples:</p> <pre><code>from idfkit.weather import StationIndex, WeatherDownloader\n\nstation = StationIndex.load().search(\"chicago ohare\")[0].station\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\nprint(files.epw)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Path | None</code> <p>Override the default cache directory.</p> <code>None</code> <code>max_age</code> <code>timedelta | float | None</code> <p>Maximum age of cached files before re-downloading. Can be a timedelta or a number of seconds. If <code>None</code> (default), cached files never expire.</p> <code>None</code> Note <p>The cache has no size limit. For CI/CD environments with limited disk space, consider using clear_cache periodically or setting a <code>max_age</code> to force re-downloads of stale files.</p> Source code in <code>src/idfkit/weather/download.py</code> <pre><code>class WeatherDownloader:\n    \"\"\"Download and cache weather files from climate.onebuilding.org.\n\n    Downloaded ZIP archives are extracted and cached locally so that\n    subsequent requests for the same station and dataset are served from\n    disk without a network call.\n\n    Examples:\n        ```python\n        from idfkit.weather import StationIndex, WeatherDownloader\n\n        station = StationIndex.load().search(\"chicago ohare\")[0].station\n        downloader = WeatherDownloader()\n        files = downloader.download(station)\n        print(files.epw)\n        ```\n\n    Args:\n        cache_dir: Override the default cache directory.\n        max_age: Maximum age of cached files before re-downloading.\n            Can be a [timedelta][datetime.timedelta] or a number of seconds.\n            If ``None`` (default), cached files never expire.\n\n    Note:\n        The cache has no size limit. For CI/CD environments with limited disk\n        space, consider using [clear_cache][idfkit.weather.download.WeatherDownloader.clear_cache] periodically or setting\n        a ``max_age`` to force re-downloads of stale files.\n    \"\"\"\n\n    __slots__ = (\"_cache_dir\", \"_max_age_seconds\")\n\n    def __init__(\n        self,\n        cache_dir: Path | None = None,\n        max_age: timedelta | float | None = None,\n    ) -&gt; None:\n        self._cache_dir = cache_dir or default_cache_dir()\n        if max_age is None:\n            self._max_age_seconds: float | None = None\n        elif isinstance(max_age, timedelta):\n            self._max_age_seconds = max_age.total_seconds()\n        else:\n            self._max_age_seconds = float(max_age)\n\n    def _is_stale(self, path: Path) -&gt; bool:\n        \"\"\"Check if a cached file is older than max_age.\"\"\"\n        if self._max_age_seconds is None:\n            return False\n        if not path.exists():\n            return True\n        age = time.time() - path.stat().st_mtime\n        return age &gt; self._max_age_seconds\n\n    def download(self, station: WeatherStation) -&gt; WeatherFiles:\n        \"\"\"Download and extract weather files for *station*.\n\n        If the files are already cached and not stale, no network request is made.\n\n        Args:\n            station: The weather station to download files for.\n\n        Returns:\n            A [WeatherFiles][idfkit.weather.download.WeatherFiles] with paths to the extracted files.\n\n        Raises:\n            RuntimeError: If the download or extraction fails.\n        \"\"\"\n        # Derive a cache subdirectory from the ZIP filename\n        zip_filename = station.url.rsplit(\"/\", maxsplit=1)[-1]\n        stem = zip_filename.removesuffix(\".zip\")\n        station_dir = self._cache_dir / \"files\" / str(station.wmo) / stem\n        zip_path = station_dir / zip_filename\n\n        # Download if not cached or if stale\n        if not zip_path.exists() or self._is_stale(zip_path):\n            station_dir.mkdir(parents=True, exist_ok=True)\n            try:\n                req = Request(station.url, headers={\"User-Agent\": _USER_AGENT})  # noqa: S310\n                with urlopen(req, timeout=120) as resp:  # noqa: S310\n                    zip_path.write_bytes(resp.read())\n            except (HTTPError, URLError, TimeoutError, OSError) as exc:\n                msg = f\"Failed to download weather data from {station.url}: {exc}\"\n                raise RuntimeError(msg) from exc\n\n        # Extract if EPW doesn't already exist or if the ZIP is newer than\n        # the EPW (i.e. we just re-downloaded).  We compare against the ZIP's\n        # mtime rather than calling ``_is_stale(epw_path)`` because\n        # ``zipfile.extractall`` preserves archive-internal timestamps, so the\n        # extracted EPW's mtime can be arbitrarily old and would always appear\n        # stale.\n        epw_path = self._find_file(station_dir, \".epw\")\n        needs_extract = epw_path is None or (zip_path.exists() and epw_path.stat().st_mtime &lt; zip_path.stat().st_mtime)\n        if needs_extract:\n            try:\n                with zipfile.ZipFile(zip_path) as zf:\n                    zf.extractall(station_dir)\n            except zipfile.BadZipFile as exc:\n                msg = f\"Downloaded file is not a valid ZIP archive: {zip_path}\"\n                raise RuntimeError(msg) from exc\n            epw_path = self._find_file(station_dir, \".epw\")\n\n        if epw_path is None:\n            msg = f\"No .epw file found in downloaded archive for {station.display_name}\"\n            raise RuntimeError(msg)\n\n        ddy_path = self._find_file(station_dir, \".ddy\")\n        if ddy_path is None:\n            msg = f\"No .ddy file found in downloaded archive for {station.display_name}\"\n            raise RuntimeError(msg)\n\n        stat_path = self._find_file(station_dir, \".stat\")\n\n        return WeatherFiles(\n            epw=epw_path,\n            ddy=ddy_path,\n            stat=stat_path,\n            zip_path=zip_path,\n            station=station,\n        )\n\n    def get_epw(self, station: WeatherStation) -&gt; Path:\n        \"\"\"Download and return the path to the EPW file.\"\"\"\n        return self.download(station).epw\n\n    def get_ddy(self, station: WeatherStation) -&gt; Path:\n        \"\"\"Download and return the path to the DDY file.\"\"\"\n        return self.download(station).ddy\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Remove all cached weather files.\n\n        This removes the entire ``files/`` subdirectory within the cache,\n        which contains all downloaded ZIP archives and extracted files.\n        \"\"\"\n        files_dir = self._cache_dir / \"files\"\n        if files_dir.exists():\n            shutil.rmtree(files_dir)\n\n    @staticmethod\n    def _find_file(directory: Path, suffix: str) -&gt; Path | None:\n        \"\"\"Find the first file with the given suffix in *directory*.\"\"\"\n        for p in directory.iterdir():\n            if p.suffix.lower() == suffix.lower() and p.is_file():\n                return p\n        return None\n</code></pre>"},{"location":"api/weather/download/#idfkit.weather.download.WeatherDownloader.download","title":"<code>download(station)</code>","text":"<p>Download and extract weather files for station.</p> <p>If the files are already cached and not stale, no network request is made.</p> <p>Parameters:</p> Name Type Description Default <code>station</code> <code>WeatherStation</code> <p>The weather station to download files for.</p> required <p>Returns:</p> Type Description <code>WeatherFiles</code> <p>A WeatherFiles with paths to the extracted files.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the download or extraction fails.</p> Source code in <code>src/idfkit/weather/download.py</code> <pre><code>def download(self, station: WeatherStation) -&gt; WeatherFiles:\n    \"\"\"Download and extract weather files for *station*.\n\n    If the files are already cached and not stale, no network request is made.\n\n    Args:\n        station: The weather station to download files for.\n\n    Returns:\n        A [WeatherFiles][idfkit.weather.download.WeatherFiles] with paths to the extracted files.\n\n    Raises:\n        RuntimeError: If the download or extraction fails.\n    \"\"\"\n    # Derive a cache subdirectory from the ZIP filename\n    zip_filename = station.url.rsplit(\"/\", maxsplit=1)[-1]\n    stem = zip_filename.removesuffix(\".zip\")\n    station_dir = self._cache_dir / \"files\" / str(station.wmo) / stem\n    zip_path = station_dir / zip_filename\n\n    # Download if not cached or if stale\n    if not zip_path.exists() or self._is_stale(zip_path):\n        station_dir.mkdir(parents=True, exist_ok=True)\n        try:\n            req = Request(station.url, headers={\"User-Agent\": _USER_AGENT})  # noqa: S310\n            with urlopen(req, timeout=120) as resp:  # noqa: S310\n                zip_path.write_bytes(resp.read())\n        except (HTTPError, URLError, TimeoutError, OSError) as exc:\n            msg = f\"Failed to download weather data from {station.url}: {exc}\"\n            raise RuntimeError(msg) from exc\n\n    # Extract if EPW doesn't already exist or if the ZIP is newer than\n    # the EPW (i.e. we just re-downloaded).  We compare against the ZIP's\n    # mtime rather than calling ``_is_stale(epw_path)`` because\n    # ``zipfile.extractall`` preserves archive-internal timestamps, so the\n    # extracted EPW's mtime can be arbitrarily old and would always appear\n    # stale.\n    epw_path = self._find_file(station_dir, \".epw\")\n    needs_extract = epw_path is None or (zip_path.exists() and epw_path.stat().st_mtime &lt; zip_path.stat().st_mtime)\n    if needs_extract:\n        try:\n            with zipfile.ZipFile(zip_path) as zf:\n                zf.extractall(station_dir)\n        except zipfile.BadZipFile as exc:\n            msg = f\"Downloaded file is not a valid ZIP archive: {zip_path}\"\n            raise RuntimeError(msg) from exc\n        epw_path = self._find_file(station_dir, \".epw\")\n\n    if epw_path is None:\n        msg = f\"No .epw file found in downloaded archive for {station.display_name}\"\n        raise RuntimeError(msg)\n\n    ddy_path = self._find_file(station_dir, \".ddy\")\n    if ddy_path is None:\n        msg = f\"No .ddy file found in downloaded archive for {station.display_name}\"\n        raise RuntimeError(msg)\n\n    stat_path = self._find_file(station_dir, \".stat\")\n\n    return WeatherFiles(\n        epw=epw_path,\n        ddy=ddy_path,\n        stat=stat_path,\n        zip_path=zip_path,\n        station=station,\n    )\n</code></pre>"},{"location":"api/weather/download/#weatherfiles","title":"WeatherFiles","text":""},{"location":"api/weather/download/#idfkit.weather.download.WeatherFiles","title":"<code>idfkit.weather.download.WeatherFiles</code>  <code>dataclass</code>","text":"<p>Paths to downloaded and extracted weather files.</p> <p>Attributes:</p> Name Type Description <code>epw</code> <code>Path</code> <p>Path to the <code>.epw</code> file (always present after extraction).</p> <code>ddy</code> <code>Path</code> <p>Path to the <code>.ddy</code> file (always present after extraction).</p> <code>stat</code> <code>Path | None</code> <p>Path to the <code>.stat</code> file, or <code>None</code> if not included.</p> <code>zip_path</code> <code>Path</code> <p>Path to the original downloaded ZIP archive.</p> <code>station</code> <code>WeatherStation</code> <p>The station this download corresponds to.</p> Source code in <code>src/idfkit/weather/download.py</code> <pre><code>@dataclass(frozen=True)\nclass WeatherFiles:\n    \"\"\"Paths to downloaded and extracted weather files.\n\n    Attributes:\n        epw: Path to the ``.epw`` file (always present after extraction).\n        ddy: Path to the ``.ddy`` file (always present after extraction).\n        stat: Path to the ``.stat`` file, or ``None`` if not included.\n        zip_path: Path to the original downloaded ZIP archive.\n        station: The station this download corresponds to.\n    \"\"\"\n\n    epw: Path\n    ddy: Path\n    stat: Path | None\n    zip_path: Path\n    station: WeatherStation\n</code></pre>"},{"location":"api/weather/download/#idfkit.weather.download.WeatherFiles.epw","title":"<code>epw</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/download/#idfkit.weather.download.WeatherFiles.ddy","title":"<code>ddy</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/","title":"Station API","text":"<p>Weather station index, search, and geocoding.</p>"},{"location":"api/weather/station/#stationindex","title":"StationIndex","text":""},{"location":"api/weather/station/#idfkit.weather.index.StationIndex","title":"<code>idfkit.weather.index.StationIndex</code>","text":"<p>Searchable index of weather stations from climate.onebuilding.org.</p> <p>Use load to load the bundled (or user-refreshed) station index. No network access or <code>openpyxl</code> is required for load.</p> <p>Use check_for_updates to see if upstream data has changed, and refresh to re-download and rebuild the index.</p> <p>Examples:</p> <pre><code>index = StationIndex.load()\nresults = index.search(\"chicago ohare\", limit=3)\nfor r in results:\n    print(r.station.display_name, r.score)\n</code></pre> Source code in <code>src/idfkit/weather/index.py</code> <pre><code>class StationIndex:\n    \"\"\"Searchable index of weather stations from climate.onebuilding.org.\n\n    Use [load][idfkit.weather.index.StationIndex.load] to load the bundled (or user-refreshed) station index.\n    No network access or ``openpyxl`` is required for [load][idfkit.weather.index.StationIndex.load].\n\n    Use [check_for_updates][idfkit.weather.index.StationIndex.check_for_updates] to see if upstream data has changed, and\n    [refresh][idfkit.weather.index.StationIndex.refresh] to re-download and rebuild the index.\n\n    Examples:\n        ```python\n        index = StationIndex.load()\n        results = index.search(\"chicago ohare\", limit=3)\n        for r in results:\n            print(r.station.display_name, r.score)\n        ```\n    \"\"\"\n\n    __slots__ = (\"_by_wmo\", \"_last_modified\", \"_stations\")\n\n    _stations: list[WeatherStation]\n    _by_wmo: dict[str, list[WeatherStation]]\n    _last_modified: dict[str, str]\n\n    def __init__(self, stations: list[WeatherStation]) -&gt; None:\n        self._stations = stations\n        self._by_wmo: dict[str, list[WeatherStation]] = {}\n        for s in stations:\n            self._by_wmo.setdefault(s.wmo, []).append(s)\n        self._last_modified: dict[str, str] = {}\n\n    # --- Construction -------------------------------------------------------\n\n    @classmethod\n    def load(cls, *, cache_dir: Path | None = None) -&gt; StationIndex:\n        \"\"\"Load the station index from a local compressed file.\n\n        Checks for a user-refreshed cache first, then falls back to the\n        bundled index shipped with the package.  No network access is\n        required.\n\n        Args:\n            cache_dir: Override the default cache directory.\n        \"\"\"\n        cache = cache_dir or default_cache_dir()\n        cached_path = cache / _CACHED_INDEX\n\n        if cached_path.is_file():\n            source = cached_path\n        elif _BUNDLED_INDEX.is_file():\n            source = _BUNDLED_INDEX\n        else:\n            msg = (\n                \"No station index found. The bundled index is missing and no \"\n                \"cached index exists. Run StationIndex.refresh() to download one.\"\n            )\n            raise FileNotFoundError(msg)\n\n        stations, last_modified, _ = _load_compressed_index(source)\n        instance = cls(stations)\n        instance._last_modified = last_modified\n        return instance\n\n    @classmethod\n    def from_stations(cls, stations: list[WeatherStation]) -&gt; StationIndex:\n        \"\"\"Create an index from an explicit list of stations (useful for tests).\"\"\"\n        return cls(stations)\n\n    @classmethod\n    def refresh(cls, *, cache_dir: Path | None = None) -&gt; StationIndex:\n        \"\"\"Re-download Excel indexes from climate.onebuilding.org and rebuild the cache.\n\n        Requires ``openpyxl``.  Install with ``pip install idfkit[weather]``.\n\n        Args:\n            cache_dir: Override the default cache directory.\n        \"\"\"\n        cache = cache_dir or default_cache_dir()\n\n        all_stations: list[WeatherStation] = []\n        last_modified: dict[str, str] = {}\n        for fname in _INDEX_FILES:\n            local_path, lm = _ensure_index_file(fname, cache)\n            if lm is not None:\n                last_modified[fname] = lm\n            all_stations.extend(_parse_excel(local_path))\n\n        dest = cache / _CACHED_INDEX\n        _save_compressed_index(all_stations, last_modified, dest)\n\n        instance = cls(all_stations)\n        instance._last_modified = last_modified\n        return instance\n\n    # --- Freshness ----------------------------------------------------------\n\n    def check_for_updates(self) -&gt; bool:\n        \"\"\"Check if upstream Excel files have changed since this index was built.\n\n        Sends lightweight HEAD requests to climate.onebuilding.org.\n        Returns ``True`` if any file has a newer ``Last-Modified`` date.\n        Returns ``False`` if all files match or if the check fails (offline,\n        timeout, etc.).\n        \"\"\"\n        if not self._last_modified:\n            return False\n        for fname in _INDEX_FILES:\n            stored = self._last_modified.get(fname)\n            if stored is None:\n                continue\n            url = f\"{_SOURCES_BASE_URL}/{fname}\"\n            upstream = _head_last_modified(url)\n            if upstream is not None and upstream != stored:\n                return True\n        return False\n\n    # --- Properties ---------------------------------------------------------\n\n    @property\n    def stations(self) -&gt; list[WeatherStation]:\n        \"\"\"All stations in the index.\"\"\"\n        return list(self._stations)\n\n    def __len__(self) -&gt; int:\n        return len(self._stations)\n\n    # --- Exact lookups ------------------------------------------------------\n\n    def get_by_wmo(self, wmo: str) -&gt; list[WeatherStation]:\n        \"\"\"Look up stations by WMO number.\n\n        Args:\n            wmo: WMO station number as a string (e.g. ``\"722950\"``).\n\n        Returns a list because a single WMO number can correspond to\n        multiple stations or dataset variants.\n        \"\"\"\n        return list(self._by_wmo.get(wmo, []))\n\n    # --- Fuzzy text search --------------------------------------------------\n\n    def search(\n        self,\n        query: str,\n        *,\n        limit: int = 10,\n        country: str | None = None,\n    ) -&gt; list[SearchResult]:\n        \"\"\"Fuzzy-search stations by name, city, state, or WMO number.\n\n        Matching is case-insensitive and uses substring / token-prefix\n        heuristics (no external NLP dependencies).\n\n        Args:\n            query: Free-text search query.\n            limit: Maximum number of results to return.\n            country: If given, restrict to stations in this country code.\n        \"\"\"\n        q = query.strip().lower()\n        if not q:\n            return []\n        tokens = q.split()\n\n        scored: list[SearchResult] = []\n        for station in self._stations:\n            if country and station.country.upper() != country.upper():\n                continue\n            score, match_field = _score_station(station, q, tokens)\n            if score &gt; 0:\n                scored.append(SearchResult(station=station, score=score, match_field=match_field))\n\n        scored.sort(key=lambda r: r.score, reverse=True)\n        return scored[:limit]\n\n    # --- Spatial search -----------------------------------------------------\n\n    def nearest(\n        self,\n        latitude: float,\n        longitude: float,\n        *,\n        limit: int = 5,\n        max_distance_km: float | None = None,\n        country: str | None = None,\n    ) -&gt; list[SpatialResult]:\n        \"\"\"Find stations nearest to a geographic coordinate.\n\n        Uses the Haversine formula for great-circle distance.  A bounding-box\n        pre-filter is applied when *max_distance_km* is specified to avoid\n        computing distances for stations that are obviously too far.\n\n        Args:\n            latitude: Decimal degrees, north positive.\n            longitude: Decimal degrees, east positive.\n            limit: Maximum results to return.\n            max_distance_km: Exclude stations farther than this.\n            country: If given, restrict to this country code.\n        \"\"\"\n        # Bounding-box pre-filter (~111 km per degree of latitude)\n        if max_distance_km is not None:\n            delta_deg = max_distance_km / 111.0 + 1.0  # small margin\n            lat_min = latitude - delta_deg\n            lat_max = latitude + delta_deg\n            # Longitude degrees vary with latitude\n            cos_lat = math.cos(math.radians(latitude))\n            lon_delta = delta_deg / max(cos_lat, 0.01)\n            lon_min = longitude - lon_delta\n            lon_max = longitude + lon_delta\n        else:\n            lat_min = lat_max = lon_min = lon_max = 0.0  # unused\n\n        results: list[SpatialResult] = []\n        for station in self._stations:\n            if country and station.country.upper() != country.upper():\n                continue\n            if max_distance_km is not None:\n                if station.latitude &lt; lat_min or station.latitude &gt; lat_max:\n                    continue\n                if station.longitude &lt; lon_min or station.longitude &gt; lon_max:\n                    continue\n            dist = haversine_km(latitude, longitude, station.latitude, station.longitude)\n            if max_distance_km is not None and dist &gt; max_distance_km:\n                continue\n            results.append(SpatialResult(station=station, distance_km=dist))\n\n        results.sort(key=lambda r: r.distance_km)\n        return results[:limit]\n\n    # --- Filtering ----------------------------------------------------------\n\n    def filter(\n        self,\n        *,\n        country: str | None = None,\n        state: str | None = None,\n        wmo_region: int | None = None,\n    ) -&gt; list[WeatherStation]:\n        \"\"\"Filter stations by metadata criteria.\n\n        All specified criteria must match (logical AND).\n        \"\"\"\n        result: list[WeatherStation] = []\n        for s in self._stations:\n            if country and s.country.upper() != country.upper():\n                continue\n            if state and s.state.upper() != state.upper():\n                continue\n            if wmo_region is not None:\n                # Infer WMO region from the URL path\n                url_lower = s.url.lower()\n                if f\"wmo_region_{wmo_region}\" not in url_lower:\n                    continue\n            result.append(s)\n        return result\n\n    @property\n    def countries(self) -&gt; list[str]:\n        \"\"\"Sorted list of unique country codes in the index.\"\"\"\n        return sorted({s.country for s in self._stations})\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.index.StationIndex.countries","title":"<code>countries</code>  <code>property</code>","text":"<p>Sorted list of unique country codes in the index.</p>"},{"location":"api/weather/station/#idfkit.weather.index.StationIndex.load","title":"<code>load(*, cache_dir=None)</code>  <code>classmethod</code>","text":"<p>Load the station index from a local compressed file.</p> <p>Checks for a user-refreshed cache first, then falls back to the bundled index shipped with the package.  No network access is required.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Path | None</code> <p>Override the default cache directory.</p> <code>None</code> Source code in <code>src/idfkit/weather/index.py</code> <pre><code>@classmethod\ndef load(cls, *, cache_dir: Path | None = None) -&gt; StationIndex:\n    \"\"\"Load the station index from a local compressed file.\n\n    Checks for a user-refreshed cache first, then falls back to the\n    bundled index shipped with the package.  No network access is\n    required.\n\n    Args:\n        cache_dir: Override the default cache directory.\n    \"\"\"\n    cache = cache_dir or default_cache_dir()\n    cached_path = cache / _CACHED_INDEX\n\n    if cached_path.is_file():\n        source = cached_path\n    elif _BUNDLED_INDEX.is_file():\n        source = _BUNDLED_INDEX\n    else:\n        msg = (\n            \"No station index found. The bundled index is missing and no \"\n            \"cached index exists. Run StationIndex.refresh() to download one.\"\n        )\n        raise FileNotFoundError(msg)\n\n    stations, last_modified, _ = _load_compressed_index(source)\n    instance = cls(stations)\n    instance._last_modified = last_modified\n    return instance\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.index.StationIndex.refresh","title":"<code>refresh(*, cache_dir=None)</code>  <code>classmethod</code>","text":"<p>Re-download Excel indexes from climate.onebuilding.org and rebuild the cache.</p> <p>Requires <code>openpyxl</code>.  Install with <code>pip install idfkit[weather]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Path | None</code> <p>Override the default cache directory.</p> <code>None</code> Source code in <code>src/idfkit/weather/index.py</code> <pre><code>@classmethod\ndef refresh(cls, *, cache_dir: Path | None = None) -&gt; StationIndex:\n    \"\"\"Re-download Excel indexes from climate.onebuilding.org and rebuild the cache.\n\n    Requires ``openpyxl``.  Install with ``pip install idfkit[weather]``.\n\n    Args:\n        cache_dir: Override the default cache directory.\n    \"\"\"\n    cache = cache_dir or default_cache_dir()\n\n    all_stations: list[WeatherStation] = []\n    last_modified: dict[str, str] = {}\n    for fname in _INDEX_FILES:\n        local_path, lm = _ensure_index_file(fname, cache)\n        if lm is not None:\n            last_modified[fname] = lm\n        all_stations.extend(_parse_excel(local_path))\n\n    dest = cache / _CACHED_INDEX\n    _save_compressed_index(all_stations, last_modified, dest)\n\n    instance = cls(all_stations)\n    instance._last_modified = last_modified\n    return instance\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.index.StationIndex.check_for_updates","title":"<code>check_for_updates()</code>","text":"<p>Check if upstream Excel files have changed since this index was built.</p> <p>Sends lightweight HEAD requests to climate.onebuilding.org. Returns <code>True</code> if any file has a newer <code>Last-Modified</code> date. Returns <code>False</code> if all files match or if the check fails (offline, timeout, etc.).</p> Source code in <code>src/idfkit/weather/index.py</code> <pre><code>def check_for_updates(self) -&gt; bool:\n    \"\"\"Check if upstream Excel files have changed since this index was built.\n\n    Sends lightweight HEAD requests to climate.onebuilding.org.\n    Returns ``True`` if any file has a newer ``Last-Modified`` date.\n    Returns ``False`` if all files match or if the check fails (offline,\n    timeout, etc.).\n    \"\"\"\n    if not self._last_modified:\n        return False\n    for fname in _INDEX_FILES:\n        stored = self._last_modified.get(fname)\n        if stored is None:\n            continue\n        url = f\"{_SOURCES_BASE_URL}/{fname}\"\n        upstream = _head_last_modified(url)\n        if upstream is not None and upstream != stored:\n            return True\n    return False\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.index.StationIndex.search","title":"<code>search(query, *, limit=10, country=None)</code>","text":"<p>Fuzzy-search stations by name, city, state, or WMO number.</p> <p>Matching is case-insensitive and uses substring / token-prefix heuristics (no external NLP dependencies).</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Free-text search query.</p> required <code>limit</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <code>country</code> <code>str | None</code> <p>If given, restrict to stations in this country code.</p> <code>None</code> Source code in <code>src/idfkit/weather/index.py</code> <pre><code>def search(\n    self,\n    query: str,\n    *,\n    limit: int = 10,\n    country: str | None = None,\n) -&gt; list[SearchResult]:\n    \"\"\"Fuzzy-search stations by name, city, state, or WMO number.\n\n    Matching is case-insensitive and uses substring / token-prefix\n    heuristics (no external NLP dependencies).\n\n    Args:\n        query: Free-text search query.\n        limit: Maximum number of results to return.\n        country: If given, restrict to stations in this country code.\n    \"\"\"\n    q = query.strip().lower()\n    if not q:\n        return []\n    tokens = q.split()\n\n    scored: list[SearchResult] = []\n    for station in self._stations:\n        if country and station.country.upper() != country.upper():\n            continue\n        score, match_field = _score_station(station, q, tokens)\n        if score &gt; 0:\n            scored.append(SearchResult(station=station, score=score, match_field=match_field))\n\n    scored.sort(key=lambda r: r.score, reverse=True)\n    return scored[:limit]\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.index.StationIndex.nearest","title":"<code>nearest(latitude, longitude, *, limit=5, max_distance_km=None, country=None)</code>","text":"<p>Find stations nearest to a geographic coordinate.</p> <p>Uses the Haversine formula for great-circle distance.  A bounding-box pre-filter is applied when max_distance_km is specified to avoid computing distances for stations that are obviously too far.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Decimal degrees, north positive.</p> required <code>longitude</code> <code>float</code> <p>Decimal degrees, east positive.</p> required <code>limit</code> <code>int</code> <p>Maximum results to return.</p> <code>5</code> <code>max_distance_km</code> <code>float | None</code> <p>Exclude stations farther than this.</p> <code>None</code> <code>country</code> <code>str | None</code> <p>If given, restrict to this country code.</p> <code>None</code> Source code in <code>src/idfkit/weather/index.py</code> <pre><code>def nearest(\n    self,\n    latitude: float,\n    longitude: float,\n    *,\n    limit: int = 5,\n    max_distance_km: float | None = None,\n    country: str | None = None,\n) -&gt; list[SpatialResult]:\n    \"\"\"Find stations nearest to a geographic coordinate.\n\n    Uses the Haversine formula for great-circle distance.  A bounding-box\n    pre-filter is applied when *max_distance_km* is specified to avoid\n    computing distances for stations that are obviously too far.\n\n    Args:\n        latitude: Decimal degrees, north positive.\n        longitude: Decimal degrees, east positive.\n        limit: Maximum results to return.\n        max_distance_km: Exclude stations farther than this.\n        country: If given, restrict to this country code.\n    \"\"\"\n    # Bounding-box pre-filter (~111 km per degree of latitude)\n    if max_distance_km is not None:\n        delta_deg = max_distance_km / 111.0 + 1.0  # small margin\n        lat_min = latitude - delta_deg\n        lat_max = latitude + delta_deg\n        # Longitude degrees vary with latitude\n        cos_lat = math.cos(math.radians(latitude))\n        lon_delta = delta_deg / max(cos_lat, 0.01)\n        lon_min = longitude - lon_delta\n        lon_max = longitude + lon_delta\n    else:\n        lat_min = lat_max = lon_min = lon_max = 0.0  # unused\n\n    results: list[SpatialResult] = []\n    for station in self._stations:\n        if country and station.country.upper() != country.upper():\n            continue\n        if max_distance_km is not None:\n            if station.latitude &lt; lat_min or station.latitude &gt; lat_max:\n                continue\n            if station.longitude &lt; lon_min or station.longitude &gt; lon_max:\n                continue\n        dist = haversine_km(latitude, longitude, station.latitude, station.longitude)\n        if max_distance_km is not None and dist &gt; max_distance_km:\n            continue\n        results.append(SpatialResult(station=station, distance_km=dist))\n\n    results.sort(key=lambda r: r.distance_km)\n    return results[:limit]\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.index.StationIndex.filter","title":"<code>filter(*, country=None, state=None, wmo_region=None)</code>","text":"<p>Filter stations by metadata criteria.</p> <p>All specified criteria must match (logical AND).</p> Source code in <code>src/idfkit/weather/index.py</code> <pre><code>def filter(\n    self,\n    *,\n    country: str | None = None,\n    state: str | None = None,\n    wmo_region: int | None = None,\n) -&gt; list[WeatherStation]:\n    \"\"\"Filter stations by metadata criteria.\n\n    All specified criteria must match (logical AND).\n    \"\"\"\n    result: list[WeatherStation] = []\n    for s in self._stations:\n        if country and s.country.upper() != country.upper():\n            continue\n        if state and s.state.upper() != state.upper():\n            continue\n        if wmo_region is not None:\n            # Infer WMO region from the URL path\n            url_lower = s.url.lower()\n            if f\"wmo_region_{wmo_region}\" not in url_lower:\n                continue\n        result.append(s)\n    return result\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.index.StationIndex.get_by_wmo","title":"<code>get_by_wmo(wmo)</code>","text":"<p>Look up stations by WMO number.</p> <p>Parameters:</p> Name Type Description Default <code>wmo</code> <code>str</code> <p>WMO station number as a string (e.g. <code>\"722950\"</code>).</p> required <p>Returns a list because a single WMO number can correspond to multiple stations or dataset variants.</p> Source code in <code>src/idfkit/weather/index.py</code> <pre><code>def get_by_wmo(self, wmo: str) -&gt; list[WeatherStation]:\n    \"\"\"Look up stations by WMO number.\n\n    Args:\n        wmo: WMO station number as a string (e.g. ``\"722950\"``).\n\n    Returns a list because a single WMO number can correspond to\n    multiple stations or dataset variants.\n    \"\"\"\n    return list(self._by_wmo.get(wmo, []))\n</code></pre>"},{"location":"api/weather/station/#weatherstation","title":"WeatherStation","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation","title":"<code>idfkit.weather.station.WeatherStation</code>  <code>dataclass</code>","text":"<p>Metadata for a single weather file entry from climate.onebuilding.org.</p> <p>Each instance represents one downloadable weather dataset. The same physical station may appear multiple times with different <code>source</code> or year-range variants (e.g. <code>TMYx.2007-2021</code> vs <code>TMYx.2009-2023</code>).</p> <p>Attributes:</p> Name Type Description <code>country</code> <code>str</code> <p>ISO 3166 country code (e.g. <code>\"USA\"</code>).</p> <code>state</code> <code>str</code> <p>State or province abbreviation (e.g. <code>\"CA\"</code>).</p> <code>city</code> <code>str</code> <p>City or station name as it appears in the index (e.g. <code>\"Marina.Muni.AP\"</code>).</p> <code>wmo</code> <code>str</code> <p>WMO station number as a string to preserve leading zeros (e.g. <code>\"722950\"</code> or <code>\"012345\"</code>).</p> <code>source</code> <code>str</code> <p>Dataset source identifier (e.g. <code>\"SRC-TMYx\"</code>).</p> <code>latitude</code> <code>float</code> <p>Decimal degrees, north positive.</p> <code>longitude</code> <code>float</code> <p>Decimal degrees, east positive.</p> <code>timezone</code> <code>float</code> <p>Hours offset from GMT (e.g. <code>-8.0</code>).</p> <code>elevation</code> <code>float</code> <p>Meters above sea level.</p> <code>url</code> <code>str</code> <p>Full download URL for the ZIP archive.</p> Source code in <code>src/idfkit/weather/station.py</code> <pre><code>@dataclass(frozen=True)\nclass WeatherStation:\n    \"\"\"Metadata for a single weather file entry from climate.onebuilding.org.\n\n    Each instance represents one downloadable weather dataset. The same physical\n    station may appear multiple times with different ``source`` or year-range\n    variants (e.g. ``TMYx.2007-2021`` vs ``TMYx.2009-2023``).\n\n    Attributes:\n        country: ISO 3166 country code (e.g. ``\"USA\"``).\n        state: State or province abbreviation (e.g. ``\"CA\"``).\n        city: City or station name as it appears in the index\n            (e.g. ``\"Marina.Muni.AP\"``).\n        wmo: WMO station number as a string to preserve leading zeros\n            (e.g. ``\"722950\"`` or ``\"012345\"``).\n        source: Dataset source identifier (e.g. ``\"SRC-TMYx\"``).\n        latitude: Decimal degrees, north positive.\n        longitude: Decimal degrees, east positive.\n        timezone: Hours offset from GMT (e.g. ``-8.0``).\n        elevation: Meters above sea level.\n        url: Full download URL for the ZIP archive.\n    \"\"\"\n\n    country: str\n    state: str\n    city: str\n    wmo: str\n    source: str\n    latitude: float\n    longitude: float\n    timezone: float\n    elevation: float\n    url: str\n\n    def to_dict(self) -&gt; dict[str, str | float]:\n        \"\"\"Serialize to a plain dictionary for JSON storage.\"\"\"\n        return {\n            \"country\": self.country,\n            \"state\": self.state,\n            \"city\": self.city,\n            \"wmo\": self.wmo,\n            \"source\": self.source,\n            \"latitude\": self.latitude,\n            \"longitude\": self.longitude,\n            \"timezone\": self.timezone,\n            \"elevation\": self.elevation,\n            \"url\": self.url,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict[str, str | float]) -&gt; WeatherStation:\n        \"\"\"Deserialize from a plain dictionary.\"\"\"\n        return cls(\n            country=str(data[\"country\"]),\n            state=str(data[\"state\"]),\n            city=str(data[\"city\"]),\n            wmo=str(data[\"wmo\"]),\n            source=str(data[\"source\"]),\n            latitude=float(data[\"latitude\"]),\n            longitude=float(data[\"longitude\"]),\n            timezone=float(data[\"timezone\"]),\n            elevation=float(data[\"elevation\"]),\n            url=str(data[\"url\"]),\n        )\n\n    @property\n    def display_name(self) -&gt; str:\n        \"\"\"Human-readable station name with location context.\n\n        Dots in the city name are replaced with spaces for readability.\n        \"\"\"\n        name = self.city.replace(\".\", \" \").replace(\"-\", \" \").strip()\n        parts: list[str] = []\n        if name:\n            parts.append(name)\n        if self.state:\n            parts.append(self.state)\n        parts.append(self.country)\n        return \", \".join(parts)\n\n    @property\n    def dataset_variant(self) -&gt; str:\n        \"\"\"Extract the TMYx dataset variant from the download URL.\n\n        Returns a string like ``\"TMYx\"``, ``\"TMYx.2007-2021\"``, or\n        ``\"TMYx.2009-2023\"``.\n        \"\"\"\n        # URL ends with e.g. ...722950_TMYx.2009-2023.zip\n        filename = self.url.rsplit(\"/\", maxsplit=1)[-1]\n        # Remove .zip extension\n        stem = filename.removesuffix(\".zip\")\n        # Dataset variant is everything after the last underscore\n        # e.g. \"USA_CA_Marina.Muni.AP.690070_TMYx\" -&gt; \"TMYx\"\n        # e.g. \"USA_CA_Twentynine.Palms.SELF.690150_TMYx.2004-2018\" -&gt; \"TMYx.2004-2018\"\n        parts = stem.rsplit(\"_\", maxsplit=1)\n        if len(parts) == 2:\n            return parts[1]\n        return stem\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.city","title":"<code>city</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.state","title":"<code>state</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.country","title":"<code>country</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.wmo","title":"<code>wmo</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.latitude","title":"<code>latitude</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.longitude","title":"<code>longitude</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.elevation","title":"<code>elevation</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.url","title":"<code>url</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.WeatherStation.display_name","title":"<code>display_name</code>  <code>property</code>","text":"<p>Human-readable station name with location context.</p> <p>Dots in the city name are replaced with spaces for readability.</p>"},{"location":"api/weather/station/#searchresult","title":"SearchResult","text":""},{"location":"api/weather/station/#idfkit.weather.station.SearchResult","title":"<code>idfkit.weather.station.SearchResult</code>  <code>dataclass</code>","text":"<p>A text search result with relevance score.</p> Source code in <code>src/idfkit/weather/station.py</code> <pre><code>@dataclass(frozen=True)\nclass SearchResult:\n    \"\"\"A text search result with relevance score.\"\"\"\n\n    station: WeatherStation\n    score: float\n    \"\"\"Relevance score from 0.0 to 1.0, higher is better.\"\"\"\n    match_field: str\n    \"\"\"Which field matched: ``\"wmo\"``, ``\"name\"``, ``\"state\"``, ``\"country\"``.\"\"\"\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.station.SearchResult.station","title":"<code>station</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.SearchResult.score","title":"<code>score</code>  <code>instance-attribute</code>","text":"<p>Relevance score from 0.0 to 1.0, higher is better.</p>"},{"location":"api/weather/station/#spatialresult","title":"SpatialResult","text":""},{"location":"api/weather/station/#idfkit.weather.station.SpatialResult","title":"<code>idfkit.weather.station.SpatialResult</code>  <code>dataclass</code>","text":"<p>A spatial proximity result with great-circle distance.</p> Source code in <code>src/idfkit/weather/station.py</code> <pre><code>@dataclass(frozen=True)\nclass SpatialResult:\n    \"\"\"A spatial proximity result with great-circle distance.\"\"\"\n\n    station: WeatherStation\n    distance_km: float\n    \"\"\"Great-circle distance in kilometres.\"\"\"\n</code></pre>"},{"location":"api/weather/station/#idfkit.weather.station.SpatialResult.station","title":"<code>station</code>  <code>instance-attribute</code>","text":""},{"location":"api/weather/station/#idfkit.weather.station.SpatialResult.distance_km","title":"<code>distance_km</code>  <code>instance-attribute</code>","text":"<p>Great-circle distance in kilometres.</p>"},{"location":"api/weather/station/#geocode","title":"geocode","text":""},{"location":"api/weather/station/#idfkit.weather.geocode.geocode","title":"<code>idfkit.weather.geocode.geocode(address)</code>","text":"<p>Convert a street address to <code>(latitude, longitude)</code> via Nominatim.</p> <p>Uses the free OpenStreetMap Nominatim geocoding service.  No API key is required.  Requests are rate-limited to one per second in compliance with Nominatim usage policy.</p> <p>This function is thread-safe. Concurrent calls from multiple threads will be serialized to respect the rate limit.</p> <p>Composable with spatial search: Use the splat operator to combine with nearest for address-based weather station lookup:</p> <pre><code>```python\nfrom idfkit.weather import StationIndex, geocode\n\n# Find weather stations near an address (one line!)\nresults = StationIndex.load().nearest(*geocode(\"350 Fifth Avenue, New York, NY\"))\n\nfor r in results[:3]:\n    print(f\"{r.station.display_name}: {r.distance_km:.0f} km\")\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>address</code> <code>str</code> <p>A free-form address string (e.g. <code>\"Willis Tower, Chicago\"</code>).</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>A <code>(latitude, longitude)</code> tuple in decimal degrees.</p> <p>Raises:</p> Type Description <code>GeocodingError</code> <p>If the address cannot be resolved or the service is unreachable.</p> Example <p>lat, lon = geocode(\"Empire State Building, NYC\") print(f\"{lat:.4f}, {lon:.4f}\") 40.7484, -73.9857</p> Source code in <code>src/idfkit/weather/geocode.py</code> <pre><code>def geocode(address: str) -&gt; tuple[float, float]:\n    \"\"\"Convert a street address to ``(latitude, longitude)`` via Nominatim.\n\n    Uses the free OpenStreetMap Nominatim geocoding service.  No API key is\n    required.  Requests are rate-limited to one per second in compliance with\n    Nominatim usage policy.\n\n    This function is thread-safe. Concurrent calls from multiple threads will\n    be serialized to respect the rate limit.\n\n    **Composable with spatial search:** Use the splat operator to combine with\n    [nearest][idfkit.weather.index.StationIndex.nearest] for address-based\n    weather station lookup:\n\n        ```python\n        from idfkit.weather import StationIndex, geocode\n\n        # Find weather stations near an address (one line!)\n        results = StationIndex.load().nearest(*geocode(\"350 Fifth Avenue, New York, NY\"))\n\n        for r in results[:3]:\n            print(f\"{r.station.display_name}: {r.distance_km:.0f} km\")\n        ```\n\n    Args:\n        address: A free-form address string (e.g. ``\"Willis Tower, Chicago\"``).\n\n    Returns:\n        A ``(latitude, longitude)`` tuple in decimal degrees.\n\n    Raises:\n        GeocodingError: If the address cannot be resolved or the service is\n            unreachable.\n\n    Example:\n        &gt;&gt;&gt; lat, lon = geocode(\"Empire State Building, NYC\")\n        &gt;&gt;&gt; print(f\"{lat:.4f}, {lon:.4f}\")\n        40.7484, -73.9857\n    \"\"\"\n    # Wait for rate limit\n    _nominatim_limiter.wait()\n\n    params = urllib.parse.urlencode({\"q\": address, \"format\": \"json\", \"limit\": \"1\"})\n    url = f\"{_NOMINATIM_URL}?{params}\"\n\n    req = urllib.request.Request(url, headers={\"User-Agent\": _USER_AGENT})  # noqa: S310\n    try:\n        with urllib.request.urlopen(req, timeout=10) as resp:  # noqa: S310\n            data = json.loads(resp.read())\n            if data:\n                return float(data[0][\"lat\"]), float(data[0][\"lon\"])\n    except (URLError, TimeoutError, json.JSONDecodeError, KeyError, IndexError) as exc:\n        msg = f\"Failed to geocode address: {address}\"\n        raise GeocodingError(msg) from exc\n    msg = f\"No results found for address: {address}\"\n    raise GeocodingError(msg)\n</code></pre>"},{"location":"api/weather/station/#geocodingerror","title":"GeocodingError","text":""},{"location":"api/weather/station/#idfkit.weather.geocode.GeocodingError","title":"<code>idfkit.weather.geocode.GeocodingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an address cannot be geocoded.</p> Source code in <code>src/idfkit/weather/geocode.py</code> <pre><code>class GeocodingError(Exception):\n    \"\"\"Raised when an address cannot be geocoded.\"\"\"\n</code></pre>"},{"location":"concepts/caching/","title":"Caching Strategy","text":"<p>idfkit uses content-addressed caching to avoid redundant work across both simulation and weather operations.</p>"},{"location":"concepts/caching/#content-addressed-caching","title":"Content-Addressed Caching","text":"<p>Cache keys are computed from the content of inputs, not filenames or timestamps. This means:</p> <ul> <li>Same inputs \u2192 same cache key \u2192 cache hit</li> <li>Different inputs \u2192 different cache key \u2192 fresh computation</li> <li>Moving or renaming files doesn't affect caching</li> </ul>"},{"location":"concepts/caching/#simulation-cache","title":"Simulation Cache","text":"<p>The <code>SimulationCache</code> stores complete simulation results keyed by a SHA-256 digest of:</p> <ol> <li>Normalised IDF content \u2014 Model with <code>Output:SQLite</code> ensured</li> <li>Weather file bytes \u2014 Complete weather file content</li> <li>Simulation flags \u2014 <code>annual</code>, <code>design_day</code>, <code>expand_objects</code>, etc.</li> </ol> <pre><code>from idfkit.simulation import simulate, SimulationCache\n\ncache = SimulationCache()\n\n# First run: executes EnergyPlus\nresult1 = simulate(model, weather, cache=cache)\n\n# Second run: instant cache hit\nresult2 = simulate(model, weather, cache=cache)\n</code></pre>"},{"location":"concepts/caching/#how-it-works","title":"How It Works","text":"<ol> <li>Before simulation, compute the cache key</li> <li>Check if a cached result exists</li> <li>If hit: return the cached <code>SimulationResult</code> immediately</li> <li>If miss: run EnergyPlus, cache the result, return</li> </ol>"},{"location":"concepts/caching/#what-gets-cached","title":"What Gets Cached","text":"<p>The entire simulation run directory is copied into the cache:</p> <ul> <li>SQLite output database</li> <li>Error report</li> <li>RDD/MDD variable files</li> <li>All other output files</li> </ul> <p>This means cached results have full access to all output data, identical to a fresh run.</p>"},{"location":"concepts/caching/#cache-location","title":"Cache Location","text":"<p>The cache uses platform-appropriate directories:</p> Platform Default Location Linux <code>~/.cache/idfkit/simulation/</code> macOS <code>~/Library/Caches/idfkit/simulation/</code> Windows <code>%LOCALAPPDATA%\\idfkit\\cache\\simulation\\</code> <p>Override with a custom path:</p> <pre><code>cache = SimulationCache(cache_dir=Path(\"/path/to/cache\"))\n</code></pre>"},{"location":"concepts/caching/#cache-management","title":"Cache Management","text":"<pre><code># Check if a result would hit cache\nkey = cache.compute_key(model, weather, design_day=True)\nif cache.contains(key):\n    print(\"Would be a cache hit\")\n\n# Clear all cached results\ncache.clear()\n</code></pre>"},{"location":"concepts/caching/#weather-cache","title":"Weather Cache","text":"<p>Weather data uses a simpler file-based cache:</p> Data Type Cache Behavior Station indexes Cached until explicit refresh EPW/DDY files Cached permanently by URL hash"},{"location":"concepts/caching/#station-index-cache","title":"Station Index Cache","text":"<p>The pre-compiled station index is bundled with the package. When you call <code>StationIndex.refresh()</code>, updated indexes are cached locally:</p> <pre><code>~/.cache/idfkit/weather/indexes/\n\u251c\u2500\u2500 africa.parquet\n\u251c\u2500\u2500 americas.parquet\n\u251c\u2500\u2500 asia.parquet\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"concepts/caching/#weather-file-cache","title":"Weather File Cache","text":"<p>Downloaded weather files are cached by URL:</p> <pre><code>~/.cache/idfkit/weather/files/\n\u251c\u2500\u2500 abc123def456.epw\n\u251c\u2500\u2500 abc123def456.ddy\n\u2514\u2500\u2500 ...\n</code></pre> <p>Files are never automatically deleted. Manual cleanup:</p> <pre><code>import shutil\nfrom idfkit.weather.download import default_cache_dir\n\nshutil.rmtree(default_cache_dir())\n</code></pre>"},{"location":"concepts/caching/#cache-invalidation","title":"Cache Invalidation","text":"<p>Content-addressed caching means automatic invalidation:</p> <ul> <li>Change the model \u2192 different key \u2192 fresh simulation</li> <li>Change the weather file \u2192 different key \u2192 fresh simulation</li> <li>Same inputs \u2192 same key \u2192 cache hit</li> </ul> <p>No manual cache invalidation is needed in normal workflows.</p>"},{"location":"concepts/caching/#parallel-safety","title":"Parallel Safety","text":"<p>Both caches are thread-safe and process-safe:</p> <ul> <li>Atomic writes using temporary directories + rename</li> <li>Safe for concurrent <code>simulate_batch()</code> with shared cache</li> <li>Multiple processes can share the same cache directory</li> </ul> <pre><code>from idfkit.simulation import simulate_batch, SimulationCache\n\ncache = SimulationCache()\n\n# Safe: all workers share the cache\nbatch = simulate_batch(jobs, max_workers=8, cache=cache)\n</code></pre>"},{"location":"concepts/caching/#memory-vs-disk","title":"Memory vs Disk","text":"<p>idfkit uses disk-based caching rather than in-memory caching because:</p> <ul> <li>Simulation results are often large (SQLite databases)</li> <li>Cache persists across Python sessions</li> <li>Multiple processes can share the cache</li> <li>Memory isn't exhausted by large batch runs</li> </ul>"},{"location":"concepts/caching/#disabling-caching","title":"Disabling Caching","text":"<p>Pass <code>cache=None</code> (the default) to skip caching:</p> <pre><code># No caching \u2014 always runs EnergyPlus\nresult = simulate(model, weather)\n\n# With caching\ncache = SimulationCache()\nresult = simulate(model, weather, cache=cache)\n</code></pre>"},{"location":"concepts/caching/#see-also","title":"See Also","text":"<ul> <li>Simulation Caching \u2014 Practical guide</li> <li>Weather Downloads \u2014 Weather file caching</li> <li>Simulation Architecture \u2014 Design decisions</li> </ul>"},{"location":"concepts/cloud-storage/","title":"Cloud &amp; Remote Storage","text":"<p>idfkit's simulation module supports pluggable storage backends through the <code>FileSystem</code> protocol, enabling cloud-native workflows with S3 and other storage systems.</p>"},{"location":"concepts/cloud-storage/#the-filesystem-protocol","title":"The FileSystem Protocol","text":"<p>The <code>FileSystem</code> protocol defines a minimal interface for file operations:</p> <pre><code>class FileSystem(Protocol):\n    def read_bytes(self, path: str | Path) -&gt; bytes: ...\n    def write_bytes(self, path: str | Path, data: bytes) -&gt; None: ...\n    def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str: ...\n    def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None: ...\n    def exists(self, path: str | Path) -&gt; bool: ...\n    def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None: ...\n    def copy(self, src: str | Path, dst: str | Path) -&gt; None: ...\n    def glob(self, path: str | Path, pattern: str) -&gt; list[str]: ...\n    def remove(self, path: str | Path) -&gt; None: ...\n</code></pre>"},{"location":"concepts/cloud-storage/#built-in-implementations","title":"Built-in Implementations","text":""},{"location":"concepts/cloud-storage/#localfilesystem","title":"LocalFileSystem","text":"<p>The default backend, wrapping <code>pathlib.Path</code> operations:</p> <pre><code>from idfkit.simulation import LocalFileSystem\n\nfs = LocalFileSystem()  # This is the default\nresult = simulate(model, weather)  # Implicitly uses LocalFileSystem\n</code></pre>"},{"location":"concepts/cloud-storage/#s3filesystem","title":"S3FileSystem","text":"<p>Amazon S3 backend for cloud workflows:</p> <pre><code>from idfkit.simulation import S3FileSystem\n\nfs = S3FileSystem(\n    bucket=\"my-simulations\",\n    prefix=\"batch-42/\",\n)\n\nresult = simulate(model, weather, output_dir=\"run-001\", fs=fs)\n</code></pre> <p>Requires the <code>boto3</code> package: <code>pip install idfkit[s3]</code></p>"},{"location":"concepts/cloud-storage/#cloud-workflow-pattern","title":"Cloud Workflow Pattern","text":"<p>For cloud-based parametric simulations (AWS Batch, Kubernetes, etc.), the typical workflow is:</p>"},{"location":"concepts/cloud-storage/#1-local-preparation","title":"1. Local Preparation","text":"<p>Create simulation jobs with S3 output paths:</p> <pre><code>from idfkit.simulation import SimulationJob, S3FileSystem\n\nfs = S3FileSystem(bucket=\"simulations\", prefix=\"study-001/\")\n\njobs = [\n    SimulationJob(\n        model=variant,\n        weather=\"weather.epw\",\n        label=f\"case-{i}\",\n        output_dir=f\"case-{i}\",\n        fs=fs,\n    )\n    for i, variant in enumerate(variants)\n]\n</code></pre>"},{"location":"concepts/cloud-storage/#2-cloud-execution","title":"2. Cloud Execution","text":"<p>Workers run simulations locally, results upload to S3:</p> <pre><code># In your AWS Batch / Kubernetes job:\nfrom idfkit.simulation import simulate, S3FileSystem\n\nfs = S3FileSystem(bucket=\"simulations\", prefix=\"study-001/\")\nresult = simulate(model, weather, output_dir=\"case-42\", fs=fs)\n\n# Result files are now in s3://simulations/study-001/case-42/\n</code></pre>"},{"location":"concepts/cloud-storage/#3-result-collection","title":"3. Result Collection","text":"<p>Retrieve results from S3 from any machine:</p> <pre><code>from idfkit.simulation import SimulationResult, S3FileSystem\n\nfs = S3FileSystem(bucket=\"simulations\", prefix=\"study-001/\")\n\n# Reconstruct result from S3\nresult = SimulationResult.from_directory(\"case-42\", fs=fs)\n\n# Query data (transparently reads from S3)\nts = result.sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\n</code></pre>"},{"location":"concepts/cloud-storage/#s3-configuration","title":"S3 Configuration","text":""},{"location":"concepts/cloud-storage/#authentication","title":"Authentication","text":"<p>S3FileSystem uses boto3's credential chain:</p> <ol> <li>Explicit credentials in constructor</li> <li>Environment variables (<code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>)</li> <li>IAM role (on EC2/ECS/Lambda)</li> <li>Shared credentials file (<code>~/.aws/credentials</code>)</li> </ol> <pre><code># IAM role (recommended for cloud)\nfs = S3FileSystem(bucket=\"my-bucket\")\n\n# Explicit credentials (for testing)\nfs = S3FileSystem(\n    bucket=\"my-bucket\",\n    aws_access_key_id=\"AKIA...\",\n    aws_secret_access_key=\"...\",\n)\n</code></pre>"},{"location":"concepts/cloud-storage/#s3-compatible-services","title":"S3-Compatible Services","text":"<p>Works with MinIO, LocalStack, and other S3-compatible services:</p> <pre><code># MinIO\nfs = S3FileSystem(\n    bucket=\"local-bucket\",\n    endpoint_url=\"http://localhost:9000\",\n    aws_access_key_id=\"minioadmin\",\n    aws_secret_access_key=\"minioadmin\",\n)\n\n# LocalStack\nfs = S3FileSystem(\n    bucket=\"test-bucket\",\n    endpoint_url=\"http://localhost:4566\",\n    region_name=\"us-east-1\",\n)\n</code></pre>"},{"location":"concepts/cloud-storage/#key-prefixes","title":"Key Prefixes","text":"<p>Use prefixes to namespace simulations:</p> <pre><code># All files stored under \"project-x/batch-42/\"\nfs = S3FileSystem(\n    bucket=\"simulations\",\n    prefix=\"project-x/batch-42/\",\n)\n\n# output_dir=\"run-001\" \u2192 s3://simulations/project-x/batch-42/run-001/\n</code></pre>"},{"location":"concepts/cloud-storage/#implementing-custom-backends","title":"Implementing Custom Backends","text":"<p>Implement the <code>FileSystem</code> protocol for other storage systems:</p> <pre><code>class AzureBlobFileSystem:\n    \"\"\"Azure Blob Storage backend.\"\"\"\n\n    def __init__(self, container: str, connection_string: str):\n        from azure.storage.blob import ContainerClient\n\n        self._client = ContainerClient.from_connection_string(connection_string, container)\n\n    def read_bytes(self, path: str | Path) -&gt; bytes:\n        blob = self._client.get_blob_client(str(path))\n        return blob.download_blob().readall()\n\n    def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n        blob = self._client.get_blob_client(str(path))\n        blob.upload_blob(data, overwrite=True)\n\n    # ... implement remaining methods\n</code></pre>"},{"location":"concepts/cloud-storage/#async-file-system","title":"Async File System","text":"<p>For use with <code>async_simulate()</code> and the async batch functions, an <code>AsyncFileSystem</code> protocol is available.  This avoids blocking the event loop during file uploads and result reads \u2014 important for network-backed storage like S3.</p>"},{"location":"concepts/cloud-storage/#built-in-asynclocalfilesystem","title":"Built-in: AsyncLocalFileSystem","text":"<p>Wraps <code>LocalFileSystem</code> via <code>asyncio.to_thread()</code>:</p> <pre><code>from idfkit.simulation import AsyncLocalFileSystem, async_simulate\n\nfs = AsyncLocalFileSystem()\nresult = await async_simulate(\n    model, \"weather.epw\",\n    output_dir=\"run-001\",\n    fs=fs,\n)\n\n# Non-blocking result access\nerrors = await result.async_errors()\nsql = await result.async_sql()\n</code></pre>"},{"location":"concepts/cloud-storage/#built-in-asyncs3filesystem","title":"Built-in: AsyncS3FileSystem","text":"<p>Non-blocking S3 backend powered by <code>aiobotocore</code>:</p> <pre><code>from idfkit.simulation import AsyncS3FileSystem, async_simulate\n\nasync with AsyncS3FileSystem(bucket=\"my-bucket\", prefix=\"sims/\") as fs:\n    result = await async_simulate(\n        model, \"weather.epw\",\n        output_dir=\"run-001\",\n        fs=fs,\n    )\n    errors = await result.async_errors()\n</code></pre> <p>Requires: <code>pip install idfkit[async-s3]</code></p> <p>The <code>AsyncS3FileSystem</code> must be used as an async context manager (<code>async with</code>) which manages the underlying aiobotocore client lifecycle. It accepts the same <code>**boto_kwargs</code> as <code>S3FileSystem</code> (e.g., <code>region_name</code>, <code>endpoint_url</code>, explicit credentials).</p> <p>S3-compatible services (MinIO, LocalStack) work identically:</p> <pre><code>async with AsyncS3FileSystem(\n    bucket=\"local-bucket\",\n    endpoint_url=\"http://localhost:9000\",\n    aws_access_key_id=\"minioadmin\",\n    aws_secret_access_key=\"minioadmin\",\n) as fs:\n    ...\n</code></pre>"},{"location":"concepts/cloud-storage/#custom-async-backend","title":"Custom Async Backend","text":"<p>Implement the <code>AsyncFileSystem</code> protocol for other storage systems (Azure Blob Storage, GCS, etc.):</p> <pre><code>from pathlib import Path\n\nfrom idfkit.simulation import AsyncFileSystem\n\n\nclass AsyncGCSFileSystem:\n    \"\"\"Example async GCS backend \u2014 implements AsyncFileSystem.\"\"\"\n\n    async def read_bytes(self, path: str | Path) -&gt; bytes:\n        ...\n\n    async def write_bytes(self, path: str | Path, data: bytes) -&gt; None:\n        ...\n\n    async def read_text(self, path: str | Path, encoding: str = \"utf-8\") -&gt; str:\n        return (await self.read_bytes(path)).decode(encoding)\n\n    async def write_text(self, path: str | Path, text: str, encoding: str = \"utf-8\") -&gt; None:\n        await self.write_bytes(path, text.encode(encoding))\n\n    async def exists(self, path: str | Path) -&gt; bool:\n        ...\n\n    async def makedirs(self, path: str | Path, *, exist_ok: bool = False) -&gt; None:\n        ...\n\n    async def copy(self, src: str | Path, dst: str | Path) -&gt; None:\n        ...\n\n    async def glob(self, path: str | Path, pattern: str) -&gt; list[str]:\n        ...\n\n    async def remove(self, path: str | Path) -&gt; None:\n        ...\n</code></pre>"},{"location":"concepts/cloud-storage/#backward-compatibility","title":"Backward Compatibility","text":"<p>A sync <code>FileSystem</code> passed to <code>async_simulate()</code> is automatically wrapped in <code>asyncio.to_thread()</code> for the upload step, so existing code continues to work without changes.  However, using <code>AsyncFileSystem</code> avoids the thread-pool overhead and provides true non-blocking I/O.</p>"},{"location":"concepts/cloud-storage/#energyplus-execution","title":"EnergyPlus Execution","text":"<p>Important: EnergyPlus always runs locally. The FileSystem abstraction covers:</p> <ul> <li>Pre-simulation: Preparing run directory</li> <li>Post-simulation: Uploading results</li> <li>Result reading: Downloading files on demand</li> </ul> <p>The actual simulation happens in a local temporary directory, then results are copied to the configured FileSystem.</p>"},{"location":"concepts/cloud-storage/#performance-considerations","title":"Performance Considerations","text":""},{"location":"concepts/cloud-storage/#lazy-loading","title":"Lazy Loading","text":"<p>Result files are read on-demand, so only accessed data is downloaded:</p> <pre><code>result = SimulationResult.from_directory(\"run-001\", fs=s3_fs)\n\n# Nothing downloaded yet\n# ...\n\n# Downloads only the SQLite file\nts = result.sql.get_timeseries(...)\n</code></pre>"},{"location":"concepts/cloud-storage/#local-caching","title":"Local Caching","text":"<p>For repeated access, consider downloading to local disk:</p> <pre><code>import tempfile\n\n# Download entire result directory\nwith tempfile.TemporaryDirectory() as tmp:\n    # Copy from S3 to local\n    for path in s3_fs.glob(\"run-001\", \"*\"):\n        data = s3_fs.read_bytes(path)\n        local_path = Path(tmp) / Path(path).name\n        local_path.write_bytes(data)\n\n    # Use local result\n    result = SimulationResult.from_directory(tmp)\n    # Multiple queries without network calls\n</code></pre>"},{"location":"concepts/cloud-storage/#see-also","title":"See Also","text":"<ul> <li>Simulation Architecture \u2014 Overall design</li> <li>Caching Strategy \u2014 Local caching</li> <li>Cloud Simulations Example \u2014 Complete example</li> </ul>"},{"location":"concepts/simulation-architecture/","title":"Simulation Architecture","text":"<p>This page explains the design decisions behind idfkit's simulation module and why certain approaches were chosen.</p>"},{"location":"concepts/simulation-architecture/#subprocess-execution","title":"Subprocess Execution","text":"<p>idfkit runs EnergyPlus as a subprocess rather than linking to its libraries directly. This approach has several benefits:</p> <ul> <li>Isolation \u2014 Each simulation runs in its own directory with clean state</li> <li>Compatibility \u2014 Works with any EnergyPlus version (8.9+)</li> <li>Robustness \u2014 Crashes in EnergyPlus don't crash your Python process</li> <li>Simplicity \u2014 No C++ bindings or version-specific compilation needed</li> </ul> <p>The <code>simulate()</code> function:</p> <ol> <li>Creates an isolated temporary directory</li> <li>Copies the model (as IDF) and weather file</li> <li>Injects <code>Output:SQLite</code> if not present</li> <li>Invokes the EnergyPlus executable</li> <li>Returns a <code>SimulationResult</code> with access to all outputs</li> </ol> <pre><code>from idfkit.simulation import simulate\n\nresult = simulate(model, \"weather.epw\", design_day=True)\nprint(f\"Outputs in: {result.run_dir}\")\n</code></pre>"},{"location":"concepts/simulation-architecture/#sqlite-over-eso","title":"SQLite Over ESO","text":"<p>idfkit's result parsers focus on the SQLite output database rather than the traditional ESO/MTR text files. The SQLite format:</p> <ul> <li>Contains all simulation data in one queryable file</li> <li>Provides structured access to time-series and tabular data</li> <li>Is faster to parse than text formats</li> <li>Includes metadata (environments, variables, units)</li> </ul> <p>The module automatically ensures <code>Output:SQLite</code> is present in your model, so you don't need to add it manually.</p> <pre><code># All time-series data is accessible via SQL queries\nts = result.sql.get_timeseries(\n    variable_name=\"Zone Mean Air Temperature\",\n    key_value=\"ZONE 1\",\n)\n\n# Tabular reports (normally in HTML) are also in SQLite\ntables = result.sql.get_tabular_data(\"AnnualBuildingUtilityPerformanceSummary\")\n</code></pre>"},{"location":"concepts/simulation-architecture/#what-about-esohtml","title":"What About ESO/HTML?","text":"<p>The following output formats are not parsed because SQLite provides the same data more reliably:</p> Format Alternative ESO/MTR (time-series) <code>result.sql.get_timeseries()</code> HTML (tabular reports) <code>result.sql.get_tabular_data()</code> EIO (metadata) SQLite metadata tables <p>If you have a specific need for these formats, please open an issue.</p>"},{"location":"concepts/simulation-architecture/#lazy-loading","title":"Lazy Loading","text":"<p><code>SimulationResult</code> uses lazy loading \u2014 output files are only parsed when you access them:</p> <pre><code>result = simulate(model, weather)  # Fast: just runs EnergyPlus\n\n# These are lazy \u2014 parsed on first access:\nresult.errors  # Parses ERR file\nresult.sql  # Opens SQLite database\nresult.variables  # Parses RDD file\n</code></pre> <p>This keeps memory usage low and startup fast, especially for batch simulations where you might only need specific outputs.</p>"},{"location":"concepts/simulation-architecture/#model-immutability","title":"Model Immutability","text":"<p>The <code>simulate()</code> function copies your model before simulation:</p> <pre><code>result = simulate(model, weather)\n\n# model is unchanged \u2014 Output:SQLite was added to a copy\nassert \"Output:SQLite\" not in model\n</code></pre> <p>This ensures:</p> <ul> <li>Your original model isn't mutated</li> <li>Multiple simulations can run concurrently with the same base model</li> <li>No unexpected side effects</li> </ul>"},{"location":"concepts/simulation-architecture/#energyplus-discovery","title":"EnergyPlus Discovery","text":"<p>idfkit auto-discovers EnergyPlus installations using a priority chain:</p> <ol> <li>Explicit path \u2014 Pass <code>energyplus_dir</code> to <code>simulate()</code> or <code>find_energyplus()</code></li> <li>Environment variable \u2014 Set <code>ENERGYPLUS_DIR</code></li> <li>System PATH \u2014 Looks for <code>energyplus</code> executable</li> <li>Platform defaults:<ul> <li>macOS: <code>/Applications/EnergyPlus-*/</code></li> <li>Linux: <code>/usr/local/EnergyPlus-*/</code></li> <li>Windows: <code>C:\\EnergyPlusV*/</code></li> </ul> </li> </ol> <p>When multiple versions are found in the default directories, the most recent version is selected.</p> <pre><code>from idfkit.simulation import find_energyplus\n\nconfig = find_energyplus()\nprint(f\"Version: {config.version}\")\nprint(f\"Path: {config.executable}\")\n</code></pre>"},{"location":"concepts/simulation-architecture/#concurrent-execution","title":"Concurrent Execution","text":"<p>For parametric studies, <code>simulate_batch()</code> runs simulations in parallel using a thread pool:</p> <pre><code>from idfkit.simulation import simulate_batch, SimulationJob\n\njobs = [\n    SimulationJob(model=variant1, weather=\"weather.epw\", label=\"case-1\"),\n    SimulationJob(model=variant2, weather=\"weather.epw\", label=\"case-2\"),\n]\n\nbatch = simulate_batch(jobs, max_workers=4)\n</code></pre> <p>Each simulation runs in its own subprocess and directory, so there are no conflicts between concurrent runs.</p>"},{"location":"concepts/simulation-architecture/#async-execution","title":"Async Execution","text":"<p>The async simulation API (<code>async_simulate</code>, <code>async_simulate_batch</code>, <code>async_simulate_batch_stream</code>) provides non-blocking counterparts to the sync API using Python's <code>asyncio</code> module.</p>"},{"location":"concepts/simulation-architecture/#why-async","title":"Why Async?","text":"<p>The sync API blocks the calling thread during <code>subprocess.run()</code>.  This is fine for scripts but problematic when:</p> <ul> <li>Running inside an async web server (FastAPI, aiohttp)</li> <li>Mixing simulations with other async I/O (network, database)</li> <li>Wanting streaming progress without callbacks</li> </ul>"},{"location":"concepts/simulation-architecture/#how-it-works","title":"How It Works","text":"<p>The async runner replaces <code>subprocess.run()</code> with <code>asyncio.create_subprocess_exec()</code>.  All preparation steps (model copy, directory setup, cache lookup) are synchronous and fast \u2014 only the EnergyPlus subprocess execution is truly async.</p> <p>Preprocessing (ExpandObjects, Slab, Basement) uses <code>subprocess.run()</code> internally.  Rather than rewriting the entire preprocessor stack, these are delegated to a thread via <code>asyncio.to_thread()</code> so they don't block the event loop.</p>"},{"location":"concepts/simulation-architecture/#concurrency-model","title":"Concurrency Model","text":"API Concurrency mechanism <code>simulate_batch()</code> <code>ThreadPoolExecutor</code> with <code>max_workers</code> <code>async_simulate_batch()</code> <code>asyncio.Semaphore</code> with <code>max_concurrent</code> <p>Both achieve the same effect: limiting the number of concurrent EnergyPlus subprocesses to avoid overwhelming the system.</p>"},{"location":"concepts/simulation-architecture/#streaming","title":"Streaming","text":"<p><code>async_simulate_batch_stream()</code> uses an <code>asyncio.Queue</code> to decouple producer tasks from the consumer's <code>async for</code> loop.  Events arrive in completion order.  Breaking out of the loop cancels remaining tasks.</p> <pre><code>from idfkit.simulation import async_simulate_batch_stream\n\nasync for event in async_simulate_batch_stream(jobs, max_concurrent=4):\n    print(f\"[{event.completed}/{event.total}] {event.label}\")\n</code></pre>"},{"location":"concepts/simulation-architecture/#see-also","title":"See Also","text":"<ul> <li>Caching Strategy \u2014 Content-addressed result caching</li> <li>Cloud &amp; Remote Storage \u2014 S3 and custom backends</li> <li>Running Simulations \u2014 Practical guide</li> </ul>"},{"location":"concepts/weather-pipeline/","title":"Weather Data Pipeline","text":"<p>This page explains how idfkit's weather module works and the concepts behind weather station data and design days.</p>"},{"location":"concepts/weather-pipeline/#data-source-climateonebuildingorg","title":"Data Source: climate.onebuilding.org","text":"<p>idfkit's weather station index is built from the climate.onebuilding.org TMYx weather file collection. This is the most comprehensive free source of EnergyPlus weather files, containing:</p> <ul> <li>~55,000 dataset entries from 10 world regions</li> <li>~17,300 unique physical weather stations</li> <li>Coverage of 248 countries and territories</li> </ul> <p>The difference between entries and stations exists because each physical station may have multiple TMYx year-range variants (e.g., <code>TMYx.2007-2021</code>, <code>TMYx.2009-2023</code>), each stored as a separate entry with its own download URL.</p>"},{"location":"concepts/weather-pipeline/#station-index-architecture","title":"Station Index Architecture","text":"<p>The <code>StationIndex</code> provides two modes of operation:</p>"},{"location":"concepts/weather-pipeline/#bundled-index-no-dependencies","title":"Bundled Index (No Dependencies)","text":"<p><code>StationIndex.load()</code> loads a pre-compiled index bundled with the package:</p> <pre><code>from idfkit.weather import StationIndex\n\nindex = StationIndex.load()  # Instant, no network\nprint(f\"{len(index)} entries, {len(index.countries)} countries\")\n</code></pre> <p>This works without any extra dependencies or network access.</p>"},{"location":"concepts/weather-pipeline/#live-refresh-requires-openpyxl","title":"Live Refresh (Requires openpyxl)","text":"<p><code>StationIndex.refresh()</code> downloads the latest Excel indexes from climate.onebuilding.org and rebuilds the index:</p> <pre><code># Check if upstream data has changed\nif index.check_for_updates():\n    index = StationIndex.refresh()  # Downloads ~10 Excel files\n</code></pre> <p>This requires the <code>openpyxl</code> package (<code>pip install idfkit[weather]</code>).</p>"},{"location":"concepts/weather-pipeline/#station-vs-entry","title":"Station vs Entry","text":"<p>Understanding the distinction:</p> Concept Description Station A physical weather monitoring location (e.g., Chicago O'Hare) Entry A specific TMYx dataset for a station (e.g., TMYx.2007-2021) <p>A single station often has multiple entries with different year ranges. When searching, results include all matching entries. Use the station's <code>wmo</code> number to identify the same physical location across entries.</p>"},{"location":"concepts/weather-pipeline/#wmo-numbers","title":"WMO Numbers","text":"<p>WMO (World Meteorological Organization) numbers identify weather stations internationally. Important notes:</p> <ul> <li>WMO numbers are not unique per station \u2014 multiple stations can share one</li> <li>Use <code>display_name</code> for human-readable identification</li> <li>Use <code>url</code> for the exact dataset you want to download</li> </ul> <pre><code># Multiple entries can have the same WMO\nresults = index.search(\"725300\")  # Chicago O'Hare WMO\nfor r in results:\n    print(f\"{r.station.source_data}: {r.station.url}\")\n</code></pre>"},{"location":"concepts/weather-pipeline/#spatial-search","title":"Spatial Search","text":"<p>The <code>nearest()</code> method uses the Haversine formula for great-circle distance calculations:</p> <pre><code># Find nearest stations to a coordinate\nresults = index.nearest(41.88, -87.63, limit=5)\n\nfor r in results:\n    print(f\"{r.station.display_name}: {r.distance_km:.1f} km\")\n</code></pre> <p>Combine with <code>geocode()</code> for address-based lookups:</p> <pre><code>from idfkit.weather import geocode\n\nlat, lon = geocode(\"350 Fifth Avenue, New York, NY\")\nresults = index.nearest(lat, lon)\n</code></pre>"},{"location":"concepts/weather-pipeline/#design-day-classification","title":"Design Day Classification","text":"<p>DDY files contain <code>SizingPeriod:DesignDay</code> objects using ASHRAE naming conventions. The <code>DesignDayManager</code> parses these and classifies each design day by type:</p> Type Pattern Example <code>HEATING_99_6</code> <code>Htg 99.6% Condns DB</code> Chicago Ann Htg 99.6% Condns DB <code>HEATING_99</code> <code>Htg 99% Condns DB</code> Chicago Ann Htg 99% Condns DB <code>COOLING_DB_0_4</code> <code>Clg .4% Condns DB=&gt;MWB</code> Chicago Ann Clg .4% Condns DB=&gt;MWB <code>COOLING_DB_1</code> <code>Clg 1% Condns DB=&gt;MWB</code> Chicago Ann Clg 1% Condns DB=&gt;MWB <code>COOLING_WB_1</code> <code>Clg 1% Condns WB=&gt;MDB</code> Chicago Ann Clg 1% Condns WB=&gt;MDB <p>Real DDY files typically contain 114+ design days:</p> <ul> <li>18 annual design days (heating, cooling, dehumidification, etc.)</li> <li>96 monthly design days (12 months \u00d7 4 percentiles \u00d7 2 types)</li> </ul>"},{"location":"concepts/weather-pipeline/#ashrae-standards","title":"ASHRAE Standards","text":"<p>Different ASHRAE standards recommend different design day percentiles:</p> Standard Heating Cooling ASHRAE 90.1 99.6% 1% ASHRAE 62.1 99% 1% <p>Use <code>apply_to_model()</code> or <code>apply_ashrae_sizing()</code> with the appropriate percentiles:</p> <pre><code>from idfkit.weather import DesignDayManager\n\nddm = DesignDayManager(\"chicago.ddy\")\n\n# ASHRAE 90.1 (stricter heating condition)\nddm.apply_to_model(model, heating=\"99.6%\", cooling=\"1%\")\n\n# Or use the convenience function with standard presets\nfrom idfkit.weather import apply_ashrae_sizing\n\napply_ashrae_sizing(model, station, standard=\"90.1\")\n</code></pre>"},{"location":"concepts/weather-pipeline/#caching","title":"Caching","text":"<p>Weather data is cached to avoid redundant downloads:</p> Data Cache Location Lifetime Station indexes <code>~/.cache/idfkit/weather/indexes/</code> Until refresh Weather files (EPW, DDY) <code>~/.cache/idfkit/weather/files/</code> Permanent <p>The cache location follows platform conventions:</p> <ul> <li>Linux: <code>~/.cache/idfkit/</code></li> <li>macOS: <code>~/Library/Caches/idfkit/</code></li> <li>Windows: <code>%LOCALAPPDATA%\\idfkit\\cache\\</code></li> </ul>"},{"location":"concepts/weather-pipeline/#see-also","title":"See Also","text":"<ul> <li>Station Search \u2014 Practical search guide</li> <li>Design Days \u2014 Applying design days to models</li> <li>Caching Strategy \u2014 General caching architecture</li> </ul>"},{"location":"design/schedule-evaluator/","title":"Schedule Evaluator Module Design","text":""},{"location":"design/schedule-evaluator/#overview","title":"Overview","text":"<p>A lightweight module to evaluate EnergyPlus schedules without running a simulation. Returns the schedule value at any given datetime or produces hourly time series.</p>"},{"location":"design/schedule-evaluator/#goals","title":"Goals","text":"<ol> <li>Minimal dependencies - Core functionality requires only stdlib; pandas/matplotlib optional</li> <li>Works with existing idfkit - Operates on <code>IDFObject</code> instances from <code>IDFDocument</code></li> <li>Correct EnergyPlus semantics - Matches E+ interpretation of schedule syntax</li> <li>Composable API - Low-level <code>evaluate()</code> + high-level <code>to_series()</code></li> </ol>"},{"location":"design/schedule-evaluator/#supported-schedule-types","title":"Supported Schedule Types","text":"Type Priority Complexity <code>Schedule:Constant</code> P0 Trivial <code>Schedule:Day:Hourly</code> P0 Simple - 24 values <code>Schedule:Day:Interval</code> P0 Medium - time/value pairs <code>Schedule:Day:List</code> P1 Medium - values at fixed intervals <code>Schedule:Week:Daily</code> P0 Simple - 7 day schedule refs <code>Schedule:Week:Compact</code> P1 Medium - day type rules <code>Schedule:Year</code> P0 Medium - date ranges \u2192 week refs <code>Schedule:Compact</code> P0 Complex - nested DSL <code>Schedule:File</code> P2 External CSV parsing"},{"location":"design/schedule-evaluator/#module-structure","title":"Module Structure","text":"<pre><code>src/idfkit/schedules/\n\u251c\u2500\u2500 __init__.py          # Public API exports\n\u251c\u2500\u2500 evaluate.py          # Core evaluation logic + dispatch\n\u251c\u2500\u2500 types.py             # DayType, Interpolation enums, SpecialDay dataclass\n\u251c\u2500\u2500 compact.py           # Schedule:Compact parser\n\u251c\u2500\u2500 day.py               # Day schedule handlers (Hourly, Interval, List)\n\u251c\u2500\u2500 week.py              # Week schedule handlers (Daily, Compact)\n\u251c\u2500\u2500 year.py              # Year schedule + date matching\n\u251c\u2500\u2500 file.py              # Schedule:File CSV reader with FileSystem support\n\u251c\u2500\u2500 holidays.py          # RunPeriodControl:SpecialDays parser\n\u2514\u2500\u2500 series.py            # pandas integration (optional)\n</code></pre>"},{"location":"design/schedule-evaluator/#public-api","title":"Public API","text":""},{"location":"design/schedule-evaluator/#core-function","title":"Core Function","text":"<pre><code>def evaluate(\n    schedule: IDFObject,\n    dt: datetime,\n    document: IDFDocument | None = None,\n    day_type: DayType = DayType.NORMAL,\n    fs: FileSystem | None = None,\n) -&gt; float:\n    \"\"\"\n    Get schedule value at a specific datetime.\n\n    Args:\n        schedule: An IDF schedule object (any supported type)\n        dt: The datetime to evaluate\n        document: Required for schedules that reference others (Year, Week)\n                  If None, extracted from schedule._document\n        day_type: Override with design day schedule (for sizing calcs)\n        fs: FileSystem for Schedule:File (default: LocalFileSystem)\n\n    Returns:\n        The schedule value as a float\n\n    Raises:\n        ScheduleEvaluationError: If schedule type unsupported or malformed\n    \"\"\"\n</code></pre>"},{"location":"design/schedule-evaluator/#batch-evaluation","title":"Batch Evaluation","text":"<pre><code>def values(\n    schedule: IDFObject,\n    year: int = 2024,\n    timestep: int = 1,  # per hour\n    start_date: tuple[int, int] = (1, 1),  # (month, day)\n    end_date: tuple[int, int] = (12, 31),\n    document: IDFDocument | None = None,\n    day_type: DayType = DayType.NORMAL,\n    interpolation: Interpolation = Interpolation.NO,\n    fs: FileSystem | None = None,\n) -&gt; list[float]:\n    \"\"\"\n    Generate schedule values for a date range.\n\n    Returns one value per timestep for the entire period.\n    Default: 8760 hourly values for a full year.\n\n    Args:\n        timestep: Values per hour (1, 2, 4, 6, 12, or 60)\n        interpolation: How to handle sub-hourly alignment\n        day_type: Use design day schedule for all days\n    \"\"\"\n</code></pre>"},{"location":"design/schedule-evaluator/#pandas-integration-optional","title":"Pandas Integration (optional)","text":"<pre><code>def to_series(\n    schedule: IDFObject,\n    year: int = 2024,\n    freq: str = \"h\",  # hourly\n    start_date: tuple[int, int] = (1, 1),\n    end_date: tuple[int, int] = (12, 31),\n    document: IDFDocument | None = None,\n    day_type: DayType = DayType.NORMAL,\n    interpolation: Interpolation = Interpolation.NO,\n    fs: FileSystem | None = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Convert schedule to pandas Series with DatetimeIndex.\n\n    Requires: pandas (optional dependency)\n    \"\"\"\n</code></pre>"},{"location":"design/schedule-evaluator/#convenience-on-idfdocument","title":"Convenience on IDFDocument","text":"<pre><code>class IDFDocument:\n    def evaluate_schedule(\n        self,\n        name: str,\n        dt: datetime,\n        day_type: DayType = DayType.NORMAL,\n    ) -&gt; float:\n        \"\"\"Shorthand for evaluate(self.get_schedule(name), dt, self)\"\"\"\n\n    def schedule_values(\n        self,\n        name: str,\n        year: int = 2024,\n        timestep: int = 1,\n        day_type: DayType = DayType.NORMAL,\n        interpolation: Interpolation = Interpolation.NO,\n    ) -&gt; list[float]:\n        \"\"\"Shorthand for values(self.get_schedule(name), ...)\"\"\"\n</code></pre>"},{"location":"design/schedule-evaluator/#schedulecompact-parser","title":"Schedule:Compact Parser","text":"<p>The most complex part. Schedule:Compact uses a mini-DSL:</p> <pre><code>Schedule:Compact,\n  Office Occupancy,        ! Name\n  Fraction,                ! Schedule Type Limits\n  Through: 12/31,          ! Date range (implicit start 1/1)\n  For: Weekdays,           ! Day types\n  Until: 08:00, 0.0,       ! Time, Value pairs\n  Until: 18:00, 1.0,\n  Until: 24:00, 0.0,\n  For: Weekends Holidays,\n  Until: 24:00, 0.0;\n</code></pre>"},{"location":"design/schedule-evaluator/#parsing-strategy","title":"Parsing Strategy","text":"<pre><code>@dataclass\nclass CompactPeriod:\n    \"\"\"A 'Through:' block covering a date range.\"\"\"\n\n    end_month: int\n    end_day: int\n    day_rules: list[CompactDayRule]\n\n\n@dataclass\nclass CompactDayRule:\n    \"\"\"A 'For:' block with day types and time-value pairs.\"\"\"\n\n    day_types: set[str]  # {\"Weekdays\", \"Weekends\", \"Holidays\", ...}\n    time_values: list[tuple[time, float]]  # [(08:00, 0.0), (18:00, 1.0), ...]\n\n\ndef parse_compact(obj: IDFObject) -&gt; list[CompactPeriod]:\n    \"\"\"Parse Schedule:Compact fields into structured data.\"\"\"\n</code></pre>"},{"location":"design/schedule-evaluator/#day-type-mapping","title":"Day Type Mapping","text":"<p>EnergyPlus day types to Python weekday:</p> E+ Day Type Python weekday() Sunday 6 Monday 0 Tuesday 1 Wednesday 2 Thursday 3 Friday 4 Saturday 5 Weekdays 0-4 Weekends 5-6 AllDays 0-6 Holidays (requires holiday list) SummerDesignDay (special) WinterDesignDay (special) AllOtherDays (fallback)"},{"location":"design/schedule-evaluator/#hierarchical-schedule-resolution","title":"Hierarchical Schedule Resolution","text":"<p><code>Schedule:Year</code> references <code>Schedule:Week:*</code> which references <code>Schedule:Day:*</code>:</p> <pre><code>def evaluate_year(obj: IDFObject, dt: datetime, doc: IDFDocument) -&gt; float:\n    # 1. Find which date range contains dt\n    # 2. Get the referenced week schedule name\n    # 3. Look up week schedule in document\n    # 4. Evaluate week schedule for dt\n    week_name = find_week_for_date(obj, dt)\n    week_obj = doc.get_schedule(week_name) or doc[week_type][week_name]\n    return evaluate_week(week_obj, dt, doc)\n\n\ndef evaluate_week_daily(obj: IDFObject, dt: datetime, doc: IDFDocument) -&gt; float:\n    # Schedule:Week:Daily has 12 fields: Sunday-Saturday + Holiday + Summer/Winter DD + Custom\n    day_index = dt.weekday()  # 0=Mon, need to map to E+ order (Sun=0)\n    field_map = {6: 0, 0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6}  # Python weekday \u2192 E+ field\n    day_name = obj[field_name_for_index(field_map[day_index])]\n    day_obj = doc[day_schedule_type][day_name]\n    return evaluate_day(day_obj, dt, doc)\n</code></pre>"},{"location":"design/schedule-evaluator/#scheduleday-evaluation","title":"Schedule:Day Evaluation","text":""},{"location":"design/schedule-evaluator/#scheduledayhourly","title":"Schedule:Day:Hourly","text":"<p>24 values, one per hour:</p> <pre><code>def evaluate_day_hourly(obj: IDFObject, dt: datetime) -&gt; float:\n    hour = dt.hour  # 0-23\n    field_name = f\"Hour {hour + 1}\"  # \"Hour 1\" through \"Hour 24\"\n    return float(obj[field_name])\n</code></pre>"},{"location":"design/schedule-evaluator/#scheduledayinterval","title":"Schedule:Day:Interval","text":"<p>Time/value pairs where value applies UNTIL that time:</p> <pre><code>def evaluate_day_interval(obj: IDFObject, dt: datetime) -&gt; float:\n    # Fields: Time 1, Value Until Time 1, Time 2, Value Until Time 2, ...\n    current_time = dt.time()\n    last_value = 0.0\n\n    for i in range(1, 145):  # Max 144 intervals\n        time_field = f\"Time {i}\"\n        value_field = f\"Value Until Time {i}\"\n        if not obj.get(time_field):\n            break\n        until_time = parse_time(obj[time_field])  # \"HH:MM\"\n        if current_time &lt; until_time:\n            return float(obj[value_field])\n        last_value = float(obj[value_field])\n\n    return last_value\n</code></pre>"},{"location":"design/schedule-evaluator/#error-handling","title":"Error Handling","text":"<pre><code>class ScheduleEvaluationError(Exception):\n    \"\"\"Raised when schedule cannot be evaluated.\"\"\"\n\n    pass\n\n\nclass UnsupportedScheduleType(ScheduleEvaluationError):\n    \"\"\"Schedule type not yet implemented.\"\"\"\n\n    pass\n\n\nclass ScheduleReferenceError(ScheduleEvaluationError):\n    \"\"\"Referenced schedule not found in document.\"\"\"\n\n    pass\n\n\nclass MalformedScheduleError(ScheduleEvaluationError):\n    \"\"\"Schedule syntax is invalid.\"\"\"\n\n    pass\n</code></pre>"},{"location":"design/schedule-evaluator/#testing-strategy","title":"Testing Strategy","text":"<ol> <li>Unit tests per schedule type - Test each parser/evaluator in isolation</li> <li>Known-value tests - Compare against EnergyPlus ESO output for same schedule</li> <li>Round-trip tests - <code>values()</code> output matches E+ hourly report</li> <li>Edge cases - Leap years, DST (E+ doesn't use DST), midnight boundaries</li> </ol>"},{"location":"design/schedule-evaluator/#example-test","title":"Example Test","text":"<pre><code>def test_compact_weekday_schedule():\n    doc = load_idf(\"tests/fixtures/office_schedules.idf\")\n    schedule = doc.get_schedule(\"Office Occupancy\")\n\n    # Monday 10am should be occupied\n    assert evaluate(schedule, datetime(2024, 1, 8, 10, 0)) == 1.0\n\n    # Saturday 10am should be unoccupied\n    assert evaluate(schedule, datetime(2024, 1, 6, 10, 0)) == 0.0\n\n    # Monday 6am should be unoccupied (before 8am)\n    assert evaluate(schedule, datetime(2024, 1, 8, 6, 0)) == 0.0\n</code></pre>"},{"location":"design/schedule-evaluator/#dependencies","title":"Dependencies","text":""},{"location":"design/schedule-evaluator/#required","title":"Required","text":"<ul> <li>None (stdlib only for core)</li> </ul>"},{"location":"design/schedule-evaluator/#internal-from-idfkit","title":"Internal (from idfkit)","text":"<ul> <li><code>idfkit.simulation.fs.FileSystem</code> - For Schedule:File CSV reading</li> <li><code>idfkit.simulation.fs.LocalFileSystem</code> - Default filesystem</li> </ul>"},{"location":"design/schedule-evaluator/#optional","title":"Optional","text":"<ul> <li><code>pandas</code> - for <code>to_series()</code> and DataFrame integration</li> </ul> <pre><code>[project.optional-dependencies]\n# No new deps needed - reuse existing\ndataframes = [\"pandas&gt;=2.0\"]  # Already exists\n</code></pre>"},{"location":"design/schedule-evaluator/#filesystem-integration","title":"FileSystem Integration","text":"<p>The <code>FileSystem</code> protocol enables Schedule:File to work with remote storage:</p> <pre><code>from idfkit import load_idf\nfrom idfkit.simulation.fs import S3FileSystem\nfrom idfkit.schedules import values\n\n# Load model from S3\nfs = S3FileSystem(bucket=\"models\", prefix=\"building-42/\")\nmodel = load_idf(\"model.idf\")  # Local IDF\n\n# Evaluate Schedule:File that references CSV on S3\nschedule = model.get_schedule(\"External Occupancy\")\nhourly = values(schedule, fs=fs)  # Reads CSV from S3\n</code></pre>"},{"location":"design/schedule-evaluator/#implementation-order","title":"Implementation Order","text":"<ol> <li>Phase 1: Foundation (~120 LOC)</li> <li><code>types.py</code>: Enums (<code>DayType</code>, <code>Interpolation</code>), <code>SpecialDay</code> dataclass</li> <li><code>holidays.py</code>: Parse <code>RunPeriodControl:SpecialDays</code></li> <li> <p><code>day.py</code>: <code>Schedule:Constant</code>, <code>Schedule:Day:Hourly</code>, <code>Schedule:Day:Interval</code></p> </li> <li> <p>Phase 2: Hierarchical schedules (~150 LOC)</p> </li> <li><code>week.py</code>: <code>Schedule:Week:Daily</code>, <code>Schedule:Week:Compact</code></li> <li><code>year.py</code>: <code>Schedule:Year</code>, date range matching</li> <li> <p>Reference resolution across schedule types</p> </li> <li> <p>Phase 3: Compact parser (~200 LOC)</p> </li> <li><code>compact.py</code>: <code>Schedule:Compact</code> DSL parser</li> <li><code>Through:</code>, <code>For:</code>, <code>Until:</code> syntax</li> <li> <p>Day type matching (Weekdays, Weekends, Holidays, Design days)</p> </li> <li> <p>Phase 4: Schedule:File (~100 LOC)</p> </li> <li><code>file.py</code>: CSV parsing with <code>FileSystem</code> protocol</li> <li>Column/separator handling</li> <li> <p>Value caching</p> </li> <li> <p>Phase 5: Integration (~80 LOC)</p> </li> <li><code>evaluate.py</code>: Dispatch + interpolation logic</li> <li><code>series.py</code>: <code>to_series()</code> pandas wrapper</li> <li><code>IDFDocument</code> convenience methods</li> </ol> <p>Total estimate: ~650 LOC + tests</p>"},{"location":"design/schedule-evaluator/#design-decisions","title":"Design Decisions","text":""},{"location":"design/schedule-evaluator/#1-holidays","title":"1. Holidays","text":"<p>Holidays are extracted from <code>RunPeriodControl:SpecialDays</code> objects in the document.</p> <pre><code>@dataclass\nclass SpecialDay:\n    \"\"\"A special day period from RunPeriodControl:SpecialDays.\"\"\"\n\n    name: str\n    start_date: date  # Parsed from \"January 1\" or \"1/1\" etc.\n    duration: int  # Days\n    day_type: str  # \"Holiday\", \"CustomDay1\", \"CustomDay2\", etc.\n\n\ndef extract_special_days(doc: IDFDocument) -&gt; list[SpecialDay]:\n    \"\"\"Parse all RunPeriodControl:SpecialDays objects.\"\"\"\n    ...\n\n\ndef get_holidays(doc: IDFDocument, year: int) -&gt; set[date]:\n    \"\"\"Get all dates marked as Holiday for a given year.\"\"\"\n    ...\n</code></pre> <p>Day types from <code>RunPeriodControl:SpecialDays</code>: - <code>Holiday</code> - Standard holiday - <code>CustomDay1</code>, <code>CustomDay2</code> - User-defined special day types</p>"},{"location":"design/schedule-evaluator/#2-design-days","title":"2. Design Days","text":"<p>Expose <code>SummerDesignDay</code> and <code>WinterDesignDay</code> via explicit parameter:</p> <pre><code>class DayType(Enum):\n    \"\"\"Special day type for evaluation.\"\"\"\n\n    NORMAL = \"normal\"  # Use calendar day\n    SUMMER_DESIGN = \"summer\"  # Use SummerDesignDay schedule\n    WINTER_DESIGN = \"winter\"  # Use WinterDesignDay schedule\n\n\ndef evaluate(\n    schedule: IDFObject,\n    dt: datetime,\n    document: IDFDocument | None = None,\n    day_type: DayType = DayType.NORMAL,\n) -&gt; float:\n    \"\"\"\n    Get schedule value at a specific datetime.\n\n    Args:\n        day_type: Override calendar day with design day schedule.\n                  Used for sizing calculations.\n    \"\"\"\n</code></pre>"},{"location":"design/schedule-evaluator/#3-interpolation","title":"3. Interpolation","text":"<p>Match EnergyPlus interpolation behavior exactly. E+ has two modes:</p> <p>\"No\" (default): Step function - value at each interval applies until the next interval. <pre><code>Schedule interval: 0-15min=0.0, 15-30min=0.5\nTimestep 10min: value = 0.0\nTimestep 20min: value = 0.5\n</code></pre></p> <p>\"Average\": Linear interpolation when timestep doesn't align with intervals. <pre><code>Schedule interval: 0-15min=0.0, 15-30min=0.5\nTimestep 10min: value = 0.0\nTimestep 20min: value = 0.25  (average of 0.0 and 0.5)\n</code></pre></p> <pre><code>class Interpolation(Enum):\n    NO = \"no\"           # Step function (default)\n    AVERAGE = \"average\" # Linear interpolation\n    LINEAR = \"linear\"   # Alias for AVERAGE\n\ndef values(\n    schedule: IDFObject,\n    year: int = 2024,\n    timestep: int = 1,  # per hour\n    interpolation: Interpolation = Interpolation.NO,\n    ...\n) -&gt; list[float]:\n    \"\"\"\n    Generate schedule values with specified interpolation.\n\n    The interpolation mode affects how values are computed when the\n    evaluation timestep doesn't align with the schedule's native intervals.\n    \"\"\"\n</code></pre>"},{"location":"design/schedule-evaluator/#4-schedulefile-support","title":"4. Schedule:File Support","text":"<p>Support external CSV files via the existing <code>FileSystem</code> protocol:</p> <pre><code>def evaluate_schedule_file(\n    obj: IDFObject,\n    dt: datetime,\n    fs: FileSystem | None = None,\n    base_path: Path | str | None = None,\n) -&gt; float:\n    \"\"\"\n    Evaluate a Schedule:File at a specific datetime.\n\n    Args:\n        obj: The Schedule:File IDF object\n        dt: Datetime to evaluate\n        fs: FileSystem for reading the CSV (default: LocalFileSystem)\n        base_path: Base directory for resolving relative file paths\n                   (default: directory containing the IDF)\n    \"\"\"\n</code></pre> <p>Schedule:File fields: | Field | Description | |-------|-------------| | Name | Schedule name | | Schedule Type Limits Name | Reference to ScheduleTypeLimits | | File Name | Path to CSV file (relative or absolute) | | Column Number | 1-based column index in CSV | | Rows to Skip at Top | Header rows to skip | | Number of Hours of Data | Usually 8760 (or 8784 for leap year) | | Column Separator | Comma, Tab, Space, Semicolon | | Interpolate to Timestep | \"No\" or \"Average\" | | Minutes per Item | 60, 30, 15, 10, 5, or 1 |</p> <p>CSV parsing with FileSystem: <pre><code>from idfkit.simulation.fs import FileSystem\n\n\ndef _read_schedule_csv(\n    file_path: str,\n    column: int,\n    skip_rows: int,\n    separator: str,\n    fs: FileSystem,\n) -&gt; list[float]:\n    \"\"\"Read schedule values from CSV using FileSystem protocol.\"\"\"\n    text = fs.read_text(file_path)\n    lines = text.strip().split(\"\\n\")[skip_rows:]\n    sep = {\"Comma\": \",\", \"Tab\": \"\\t\", \"Space\": \" \", \"Semicolon\": \";\"}[separator]\n    values = []\n    for line in lines:\n        cols = line.split(sep)\n        values.append(float(cols[column - 1]))  # 1-based index\n    return values\n</code></pre></p> <p>Caching: Schedule:File data should be cached after first read to avoid repeated I/O:</p> <pre><code>class ScheduleFileCache:\n    \"\"\"Cache for Schedule:File CSV data.\"\"\"\n\n    _cache: dict[str, list[float]]  # file_path -&gt; values\n\n    def get_values(\n        self,\n        obj: IDFObject,\n        fs: FileSystem,\n        base_path: Path,\n    ) -&gt; list[float]:\n        \"\"\"Get cached values or read from file.\"\"\"\n</code></pre>"},{"location":"examples/celery-integration/","title":"Celery Integration","text":"<p>This tutorial shows how to run EnergyPlus simulations as distributed Celery tasks.  The pattern works well when you need to:</p> <ul> <li>Run hundreds or thousands of simulations across a cluster of machines.</li> <li>Integrate simulation jobs into a larger web application or data pipeline.</li> <li>Get automatic retries, rate limiting, and monitoring for free.</li> </ul> <p>When to use Celery vs <code>simulate_batch()</code></p> <p>idfkit's built-in <code>simulate_batch()</code> is the simplest way to run simulations in parallel on a single machine. Reach for Celery when you need to distribute work across multiple machines, integrate with an existing task queue, or require features like retries, priority queues, and persistent result storage.</p>"},{"location":"examples/celery-integration/#prerequisites","title":"Prerequisites","text":"<p>Install idfkit and Celery with a Redis broker:</p> <pre><code>pip install idfkit celery[redis]\n</code></pre> <p>You also need a running Redis instance.  The fastest way to get one locally:</p> <pre><code>docker run -d -p 6379:6379 redis:7-alpine\n</code></pre>"},{"location":"examples/celery-integration/#project-layout","title":"Project Layout","text":"<pre><code># project/\n# \u251c\u2500\u2500 celeryconfig.py       # Celery configuration\n# \u251c\u2500\u2500 tasks.py              # Task definitions\n# \u251c\u2500\u2500 submit.py             # Client that submits jobs\n# \u251c\u2500\u2500 models/               # EnergyPlus IDF/epJSON files\n# \u2502   \u2514\u2500\u2500 office.idf\n# \u2514\u2500\u2500 weather/              # Weather files\n#     \u2514\u2500\u2500 chicago.epw\n</code></pre>"},{"location":"examples/celery-integration/#step-1-configure-celery","title":"Step 1: Configure Celery","text":"<pre><code># celeryconfig.py\nbroker_url = \"redis://localhost:6379/0\"\nresult_backend = \"redis://localhost:6379/1\"\n\n# Serialisation \u2014 JSON is safe and human-readable\ntask_serializer = \"json\"\nresult_serializer = \"json\"\naccept_content = [\"json\"]\n\n# Prevent a slow simulation from being acknowledged before it finishes.\n# With acks_late the message is re-delivered if the worker crashes mid-run.\ntask_acks_late = True\ntask_reject_on_worker_lost = True\n\n# One simulation per worker process \u2014 EnergyPlus is CPU-bound\nworker_concurrency = 1\n\n# Long timeout for annual simulations (4 hours)\ntask_time_limit = 14400\ntask_soft_time_limit = 14000\n\n# Result expiry (24 hours)\nresult_expires = 86400\n</code></pre> <p>Key choices explained:</p> Setting Why <code>task_acks_late = True</code> The message stays in Redis until the task finishes. If a worker crashes mid-simulation, another worker picks up the job automatically. <code>worker_concurrency = 1</code> EnergyPlus is CPU-bound. Running one simulation per worker process avoids CPU contention. Scale by adding more worker processes or machines instead. <code>task_serializer = \"json\"</code> Task arguments must be JSON-serializable (strings, numbers, bools). This avoids pickle security issues and makes task payloads inspectable. <code>task_time_limit</code> Kills the worker process if a simulation exceeds the hard limit, preventing runaway jobs from blocking the queue."},{"location":"examples/celery-integration/#step-2-define-a-simulation-task","title":"Step 2: Define a Simulation Task","text":"<pre><code># tasks.py\nfrom pathlib import Path\n\nfrom celery import Celery\n\nfrom idfkit import load_idf\nfrom idfkit.simulation import simulate\n\napp = Celery(\"tasks\")\napp.config_from_object(\"celeryconfig\")\n\n\n@app.task(bind=True, name=\"simulate_building\")\ndef simulate_building(\n    self,\n    idf_path: str,\n    weather_path: str,\n    output_dir: str,\n    design_day: bool = False,\n) -&gt; dict:\n    \"\"\"Run a single EnergyPlus simulation and return a result summary.\"\"\"\n    model = load_idf(idf_path)\n    result = simulate(\n        model,\n        weather_path,\n        output_dir=output_dir,\n        design_day=design_day,\n        timeout=14000.0,\n    )\n    return {\n        \"success\": result.success,\n        \"runtime\": result.runtime_seconds,\n        \"output_dir\": str(Path(result.run_dir).resolve()),\n    }\n</code></pre> <p>Pass file paths, not objects</p> <p>Celery serializes task arguments to JSON.  Pass file paths (strings) to the task and call <code>load_idf()</code> inside the worker.  Never try to pass an <code>IDFDocument</code> directly \u2014 it is not JSON-serializable.</p>"},{"location":"examples/celery-integration/#step-3-submit-jobs","title":"Step 3: Submit Jobs","text":""},{"location":"examples/celery-integration/#single-simulation","title":"Single Simulation","text":"<pre><code># submit.py \u2014 send a single simulation to the queue\nfrom tasks import simulate_building\n\nresult = simulate_building.delay(\n    idf_path=\"models/office.idf\",\n    weather_path=\"weather/chicago.epw\",\n    output_dir=\"/tmp/sim-results/run-001\",\n    design_day=True,\n)\n\n# Block until done (or poll with result.ready())\nsummary = result.get(timeout=3600)\nprint(summary)\n# {\"success\": True, \"runtime\": 42.3, \"output_dir\": \"/tmp/sim-results/run-001\"}\n</code></pre>"},{"location":"examples/celery-integration/#fan-out-batch","title":"Fan-Out Batch","text":"<p>Use Celery's <code>group</code> primitive to submit many simulations at once:</p> <pre><code>from celery import group\nfrom tasks import simulate_building\n\n# Fan-out: submit many simulations at once\njobs = group(\n    simulate_building.s(\n        idf_path=f\"models/variant_{i}.idf\",\n        weather_path=\"weather/chicago.epw\",\n        output_dir=f\"/tmp/sim-results/variant-{i}\",\n    )\n    for i in range(20)\n)\n\nbatch = jobs.apply_async()\n\n# Wait for all results\nresults = batch.get(timeout=7200)\nsucceeded = [r for r in results if r[\"success\"]]\nprint(f\"{len(succeeded)}/{len(results)} simulations succeeded\")\n</code></pre> <p>Each job runs on whichever worker is available.  With 4 workers, up to 4 simulations run in parallel.</p>"},{"location":"examples/celery-integration/#parametric-studies","title":"Parametric Studies","text":"<p>For parametric sweeps, pass scalar parameters to the task and build the model variant on the worker:</p>"},{"location":"examples/celery-integration/#task-definition","title":"Task Definition","text":"<pre><code># tasks.py \u2014 parametric task that builds variants on the worker\nfrom pathlib import Path\n\nfrom celery import Celery\n\nfrom idfkit import load_idf\nfrom idfkit.simulation import simulate\n\napp = Celery(\"tasks\")\napp.config_from_object(\"celeryconfig\")\n\n\n@app.task(bind=True, name=\"run_parametric_case\")\ndef run_parametric_case(\n    self,\n    base_idf_path: str,\n    weather_path: str,\n    output_dir: str,\n    wall_conductivity: float | None = None,\n    window_u_factor: float | None = None,\n    infiltration_rate: float | None = None,\n    label: str = \"\",\n) -&gt; dict:\n    \"\"\"Apply parameter overrides to a base model and simulate.\"\"\"\n    model = load_idf(base_idf_path)\n\n    # Apply overrides\n    if wall_conductivity is not None:\n        for mat in model[\"Material\"]:\n            if \"wall\" in mat.name.lower():\n                mat.conductivity = wall_conductivity\n\n    if window_u_factor is not None:\n        for win in model[\"WindowMaterial:SimpleGlazingSystem\"]:\n            win.u_factor = window_u_factor\n\n    if infiltration_rate is not None:\n        for inf in model[\"ZoneInfiltration:DesignFlowRate\"]:\n            inf.design_flow_rate = infiltration_rate\n\n    result = simulate(model, weather_path, output_dir=output_dir, design_day=True)\n    return {\n        \"label\": label,\n        \"success\": result.success,\n        \"runtime\": result.runtime_seconds,\n        \"output_dir\": str(Path(result.run_dir).resolve()),\n    }\n</code></pre>"},{"location":"examples/celery-integration/#submitting-a-parameter-grid","title":"Submitting a Parameter Grid","text":"<pre><code>import itertools\n\nfrom celery import group\nfrom tasks import run_parametric_case\n\nconductivities = [0.5, 1.0, 1.5]\nu_factors = [1.5, 2.5, 3.5]\n\njobs = group(\n    run_parametric_case.s(\n        base_idf_path=\"models/office.idf\",\n        weather_path=\"weather/chicago.epw\",\n        output_dir=f\"/tmp/parametric/k{k}_u{u}\",\n        wall_conductivity=k,\n        window_u_factor=u,\n        label=f\"k={k}, U={u}\",\n    )\n    for k, u in itertools.product(conductivities, u_factors)\n)\n\nbatch = jobs.apply_async()\nresults = batch.get(timeout=7200)\n\nfor r in sorted(results, key=lambda x: x[\"label\"]):\n    status = \"OK\" if r[\"success\"] else \"FAIL\"\n    print(f\"[{status}] {r['label']}  ({r['runtime']:.1f}s)\")\n</code></pre> <p>This fans out 9 jobs (3 conductivities x 3 U-factors) across all available workers.</p>"},{"location":"examples/celery-integration/#error-handling-and-retries","title":"Error Handling and Retries","text":"<p>EnergyPlus can fail for transient reasons (file system issues, resource exhaustion).  Celery's built-in retry mechanism handles this:</p> <pre><code>from pathlib import Path\n\nfrom celery import Celery\n\nfrom idfkit import load_idf\nfrom idfkit.exceptions import SimulationError\nfrom idfkit.simulation import simulate\n\napp = Celery(\"tasks\")\napp.config_from_object(\"celeryconfig\")\n\n\n@app.task(\n    bind=True,\n    name=\"simulate_with_retry\",\n    autoretry_for=(SimulationError, OSError),\n    retry_backoff=60,\n    retry_backoff_max=600,\n    max_retries=3,\n)\ndef simulate_with_retry(\n    self,\n    idf_path: str,\n    weather_path: str,\n    output_dir: str,\n) -&gt; dict:\n    \"\"\"Simulate with automatic retry on transient failures.\"\"\"\n    model = load_idf(idf_path)\n    result = simulate(model, weather_path, output_dir=output_dir, design_day=True)\n\n    if not result.success:\n        # Raise to trigger retry for EnergyPlus errors that may be transient\n        raise SimulationError(\n            f\"Simulation failed (attempt {self.request.retries + 1})\",\n            exit_code=1,\n        )\n\n    return {\n        \"success\": True,\n        \"runtime\": result.runtime_seconds,\n        \"output_dir\": str(Path(result.run_dir).resolve()),\n        \"retries\": self.request.retries,\n    }\n</code></pre> <p>The <code>autoretry_for</code> parameter catches both idfkit <code>SimulationError</code> and OS-level errors.  The task retries up to 3 times with exponential backoff (60s, then 120s, capped at 600s).</p>"},{"location":"examples/celery-integration/#progress-reporting","title":"Progress Reporting","text":"<p>idfkit's <code>on_progress</code> callback integrates naturally with Celery's custom state updates:</p>"},{"location":"examples/celery-integration/#task-with-progress","title":"Task with Progress","text":"<pre><code>from pathlib import Path\n\nfrom celery import Celery\n\nfrom idfkit import load_idf\nfrom idfkit.simulation import SimulationProgress, simulate\n\napp = Celery(\"tasks\")\napp.config_from_object(\"celeryconfig\")\n\n\n@app.task(bind=True, name=\"simulate_with_progress\")\ndef simulate_with_progress(\n    self,\n    idf_path: str,\n    weather_path: str,\n    output_dir: str,\n) -&gt; dict:\n    \"\"\"Report EnergyPlus progress back to Celery task state.\"\"\"\n\n    def report_progress(progress: SimulationProgress) -&gt; None:\n        self.update_state(\n            state=\"SIMULATING\",\n            meta={\n                \"environment\": progress.environment,\n                \"percent\": progress.percent,\n            },\n        )\n\n    model = load_idf(idf_path)\n    result = simulate(\n        model,\n        weather_path,\n        output_dir=output_dir,\n        on_progress=report_progress,\n    )\n\n    return {\n        \"success\": result.success,\n        \"runtime\": result.runtime_seconds,\n        \"output_dir\": str(Path(result.run_dir).resolve()),\n    }\n</code></pre>"},{"location":"examples/celery-integration/#polling-progress-from-the-client","title":"Polling Progress from the Client","text":"<pre><code>import time\n\nfrom tasks import simulate_with_progress\n\nresult = simulate_with_progress.delay(\n    idf_path=\"models/office.idf\",\n    weather_path=\"weather/chicago.epw\",\n    output_dir=\"/tmp/sim-results/progress-demo\",\n)\n\nwhile not result.ready():\n    meta = result.info  # dict with progress metadata\n    if isinstance(meta, dict) and meta.get(\"percent\") is not None:\n        pct = meta[\"percent\"]\n        env = meta.get(\"environment\", \"\")\n        print(f\"  {pct:.0f}%  \u2014  {env}\")\n    time.sleep(2)\n\nprint(\"Done:\", result.get())\n</code></pre>"},{"location":"examples/celery-integration/#caching","title":"Caching","text":"<p>Share a <code>SimulationCache</code> across workers to skip duplicate simulations.  If workers run on the same machine (or share a network file system), point the cache at a shared directory:</p> <pre><code>from pathlib import Path\n\nfrom celery import Celery\n\nfrom idfkit import load_idf\nfrom idfkit.simulation import SimulationCache, simulate\n\napp = Celery(\"tasks\")\napp.config_from_object(\"celeryconfig\")\n\n# Shared cache directory \u2014 must be accessible by all workers.\n# Use a network file system (NFS, EFS) or a local directory if\n# workers run on the same machine.\nCACHE_DIR = Path(\"/shared/simulation-cache\")\ncache = SimulationCache(cache_dir=CACHE_DIR)\n\n\n@app.task(bind=True, name=\"simulate_cached\")\ndef simulate_cached(\n    self,\n    idf_path: str,\n    weather_path: str,\n    output_dir: str,\n    design_day: bool = False,\n) -&gt; dict:\n    \"\"\"Simulate with content-addressed caching to skip duplicate work.\"\"\"\n    model = load_idf(idf_path)\n    result = simulate(\n        model,\n        weather_path,\n        output_dir=output_dir,\n        design_day=design_day,\n        cache=cache,\n    )\n    return {\n        \"success\": result.success,\n        \"runtime\": result.runtime_seconds,\n        \"output_dir\": str(Path(result.run_dir).resolve()),\n    }\n</code></pre> <p>When the same model + weather combination is submitted again, the worker returns the cached result instantly instead of re-running EnergyPlus.</p>"},{"location":"examples/celery-integration/#cloud-storage-s3","title":"Cloud Storage (S3)","text":"<p>Upload simulation results directly to S3 from workers:</p> <pre><code>from celery import Celery\n\nfrom idfkit import load_idf\nfrom idfkit.simulation import S3FileSystem, simulate\n\napp = Celery(\"tasks\")\napp.config_from_object(\"celeryconfig\")\n\n\n@app.task(bind=True, name=\"simulate_to_s3\")\ndef simulate_to_s3(\n    self,\n    idf_path: str,\n    weather_path: str,\n    s3_prefix: str,\n    design_day: bool = False,\n) -&gt; dict:\n    \"\"\"Run a simulation and upload results to S3.\"\"\"\n    model = load_idf(idf_path)\n\n    fs = S3FileSystem(bucket=\"my-sim-bucket\", prefix=s3_prefix)\n    result = simulate(\n        model,\n        weather_path,\n        output_dir=\"results\",\n        design_day=design_day,\n        fs=fs,\n    )\n\n    return {\n        \"success\": result.success,\n        \"runtime\": result.runtime_seconds,\n        \"s3_prefix\": s3_prefix,\n    }\n</code></pre> <p>Workers need AWS credentials (environment variables, IAM role, or <code>~/.aws/credentials</code>).  See Cloud Simulations (S3) for details.</p>"},{"location":"examples/celery-integration/#task-composition","title":"Task Composition","text":"<p>Celery's <code>chain</code> primitive lets you compose multi-step workflows \u2014 for example, running a simulation and then post-processing the results:</p> <pre><code>from celery import Celery, chain\n\napp = Celery(\"tasks\")\napp.config_from_object(\"celeryconfig\")\n\n\n@app.task(name=\"collect_results\")\ndef collect_results(sim_result: dict) -&gt; dict:\n    \"\"\"Post-process a simulation result (runs after the simulation task).\"\"\"\n    if not sim_result[\"success\"]:\n        return {\"error\": \"simulation failed\", **sim_result}\n\n    from idfkit.simulation import SimulationResult\n\n    result = SimulationResult.from_directory(sim_result[\"output_dir\"])\n    heating = result.sql.get_timeseries(\n        variable_name=\"Zone Ideal Loads Heating Energy\",\n        key_value=\"OFFICE\",\n    )\n    return {\n        **sim_result,\n        \"peak_heating_W\": float(heating.max()),\n    }\n\n\n# Compose: simulate \u2192 collect_results\nfrom tasks import simulate_building\n\nworkflow = chain(\n    simulate_building.s(\n        idf_path=\"models/office.idf\",\n        weather_path=\"weather/chicago.epw\",\n        output_dir=\"/tmp/sim-results/chained\",\n        design_day=True,\n    ),\n    collect_results.s(),\n)\n\nfinal = workflow.apply_async()\nprint(final.get(timeout=3600))\n</code></pre>"},{"location":"examples/celery-integration/#deployment-with-docker","title":"Deployment with Docker","text":""},{"location":"examples/celery-integration/#dockerfile","title":"Dockerfile","text":"<pre><code># Dockerfile\n#\n# FROM nrel/energyplus:25.2.0\n#\n# # Install Python and uv\n# RUN apt-get update &amp;&amp; apt-get install -y python3 python3-pip curl \\\n#     &amp;&amp; curl -LsSf https://astral.sh/uv/install.sh | sh\n#\n# WORKDIR /app\n# COPY pyproject.toml uv.lock ./\n# RUN uv sync --frozen\n#\n# COPY celeryconfig.py tasks.py ./\n# COPY models/ ./models/\n# COPY weather/ ./weather/\n#\n# CMD [\"uv\", \"run\", \"celery\", \"-A\", \"tasks\", \"worker\", \"--loglevel=info\"]\n</code></pre>"},{"location":"examples/celery-integration/#docker-compose","title":"Docker Compose","text":"<pre><code># docker-compose.yml\n#\n# services:\n#   redis:\n#     image: redis:7-alpine\n#     ports:\n#       - \"6379:6379\"\n#\n#   worker:\n#     build: .\n#     command: celery -A tasks worker --loglevel=info --concurrency=1\n#     environment:\n#       - ENERGYPLUS_DIR=/usr/local/EnergyPlus-25-2-0\n#     volumes:\n#       - ./models:/app/models:ro\n#       - ./weather:/app/weather:ro\n#       - sim-results:/tmp/sim-results\n#     depends_on:\n#       - redis\n#     deploy:\n#       replicas: 4          # 4 workers = 4 parallel simulations\n#       resources:\n#         limits:\n#           cpus: \"1\"         # 1 CPU per worker\n#           memory: 2G\n#\n#   flower:\n#     image: mher/flower\n#     command: celery --broker=redis://redis:6379/0 flower\n#     ports:\n#       - \"5555:5555\"\n#     depends_on:\n#       - redis\n#\n# volumes:\n#   sim-results:\n</code></pre> <p>Start everything with:</p> <pre><code>docker compose up -d --scale worker=4\n</code></pre> <p>This starts 4 worker containers (4 concurrent simulations), a Redis broker, and the Flower monitoring dashboard at <code>http://localhost:5555</code>.</p>"},{"location":"examples/celery-integration/#monitoring","title":"Monitoring","text":""},{"location":"examples/celery-integration/#flower-dashboard","title":"Flower Dashboard","text":"<p>Flower provides a real-time web UI for monitoring workers, tasks, and queues:</p> <pre><code>pip install flower\ncelery -A tasks flower --port=5555\n</code></pre> <p>Open <code>http://localhost:5555</code> to see active workers, task history, success rates, and runtime distributions.</p>"},{"location":"examples/celery-integration/#cli-inspection","title":"CLI Inspection","text":"<pre><code># List active workers\ncelery -A tasks inspect active\n\n# Purge all pending tasks\ncelery -A tasks purge\n\n# View task result\ncelery -A tasks result &lt;task-id&gt;\n</code></pre>"},{"location":"examples/celery-integration/#best-practices","title":"Best Practices","text":"<ol> <li> <p>One simulation per worker process.    Set <code>worker_concurrency = 1</code>.  EnergyPlus is CPU-bound, and running    multiple simulations in one process causes contention.  Scale    horizontally by adding workers.</p> </li> <li> <p>Use <code>acks_late</code> with <code>reject_on_worker_lost</code>.    This ensures that if a worker dies mid-simulation, the job is re-queued    and retried by another worker.</p> </li> <li> <p>Pass paths, not objects.    All task arguments must be JSON-serializable.  Pass IDF file paths and    scalar parameters \u2014 load models inside the worker.</p> </li> <li> <p>Set time limits.    Use <code>task_time_limit</code> (hard kill) and <code>task_soft_time_limit</code> (raises    <code>SoftTimeLimitExceeded</code>) to prevent runaway simulations from blocking    workers indefinitely.</p> </li> <li> <p>Use a shared cache for parametric studies.    Point <code>SimulationCache</code> at a network file system or shared volume so    workers skip duplicate model + weather combinations.</p> </li> <li> <p>Pin the EnergyPlus version.    Ensure all workers use the same EnergyPlus version (set <code>ENERGYPLUS_DIR</code>    or embed it in the Docker image) to avoid inconsistent results.</p> </li> <li> <p>Separate queues for fast and slow jobs.    Route design-day simulations to a \"fast\" queue and annual simulations to    a \"slow\" queue using Celery's    task routing.</p> </li> <li> <p>Monitor with Flower.    Run Flower to track worker health,    task throughput, and failure rates in production.</p> </li> </ol>"},{"location":"examples/celery-integration/#see-also","title":"See Also","text":"<ul> <li>Batch Processing \u2014 Single-machine parallel execution</li> <li>Async Simulation \u2014 Non-blocking execution for async apps</li> <li>Cloud Simulations (S3) \u2014 S3 result storage</li> <li>Caching \u2014 Content-addressed simulation caching</li> </ul>"},{"location":"examples/cloud-simulations/","title":"Cloud Simulations (S3)","text":"<p>This example demonstrates running simulations with results stored in Amazon S3, suitable for distributed cloud workflows.</p>"},{"location":"examples/cloud-simulations/#prerequisites","title":"Prerequisites","text":"<p>Install S3 support:</p> <pre><code>pip install idfkit[s3]\n</code></pre> <p>Configure AWS credentials:</p> <pre><code># Option 1: Environment variables\nexport AWS_ACCESS_KEY_ID=AKIA...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_DEFAULT_REGION=us-east-1\n\n# Option 2: AWS credentials file (~/.aws/credentials)\n# Option 3: IAM role (on EC2, ECS, Lambda)\n</code></pre>"},{"location":"examples/cloud-simulations/#basic-s3-usage","title":"Basic S3 Usage","text":"<pre><code>from idfkit import load_idf\nfrom idfkit.simulation import simulate, S3FileSystem\n\n# Create S3-backed filesystem\nfs = S3FileSystem(\n    bucket=\"my-simulations\",\n    prefix=\"project-x/\",\n)\n\n# Run simulation with S3 storage\nmodel = load_idf(\"building.idf\")\nresult = simulate(\n    model,\n    \"weather.epw\",\n    output_dir=\"run-001\",  # Required with fs\n    fs=fs,\n)\n\n# Results are now in s3://my-simulations/project-x/run-001/\nprint(f\"Results stored at: {result.run_dir}\")\n</code></pre>"},{"location":"examples/cloud-simulations/#cloud-workflow-pattern","title":"Cloud Workflow Pattern","text":"<p>For large-scale simulations on AWS Batch, Kubernetes, or similar:</p>"},{"location":"examples/cloud-simulations/#step-1-create-jobs-locally","title":"Step 1: Create Jobs Locally","text":"<pre><code>from idfkit.simulation import SimulationJob, S3FileSystem\n\nfs = S3FileSystem(bucket=\"simulations\", prefix=\"study-001/\")\n\n# Create job specifications\njobs = []\nfor i, variant in enumerate(model_variants):\n    jobs.append(\n        SimulationJob(\n            model=variant,\n            weather=\"weather.epw\",\n            label=f\"case-{i:04d}\",\n            output_dir=f\"case-{i:04d}\",\n        )\n    )\n\n# Save job specs (e.g., as JSON or pickle)\n</code></pre>"},{"location":"examples/cloud-simulations/#step-2-run-on-cloud-workers","title":"Step 2: Run on Cloud Workers","text":"<p>Each worker runs a subset of jobs:</p> <pre><code># worker.py (runs on AWS Batch, Kubernetes, etc.)\nfrom idfkit.simulation import simulate, S3FileSystem\n\nfs = S3FileSystem(bucket=\"simulations\", prefix=\"study-001/\")\n\n# Run single job\nresult = simulate(\n    model,\n    weather_path,  # Must be local\n    output_dir=f\"case-{job_id}\",\n    fs=fs,\n)\n\n# Results uploaded to S3 automatically\n</code></pre>"},{"location":"examples/cloud-simulations/#step-3-collect-results","title":"Step 3: Collect Results","text":"<p>From any machine with S3 access:</p> <pre><code>from idfkit.simulation import SimulationResult, S3FileSystem\n\nfs = S3FileSystem(bucket=\"simulations\", prefix=\"study-001/\")\n\n# Reconstruct results\nresults = []\nfor i in range(num_cases):\n    result = SimulationResult.from_directory(f\"case-{i:04d}\", fs=fs)\n    results.append(result)\n\n# Analyze\nfor i, result in enumerate(results):\n    ts = result.sql.get_timeseries(\n        \"Zone Mean Air Temperature\",\n        \"ZONE 1\",\n    )\n    print(f\"Case {i}: max temp = {max(ts.values):.1f}\u00b0C\")\n</code></pre>"},{"location":"examples/cloud-simulations/#batch-processing-with-s3","title":"Batch Processing with S3","text":"<pre><code>from idfkit.simulation import simulate_batch, SimulationJob, S3FileSystem\n\nfs = S3FileSystem(bucket=\"my-bucket\", prefix=\"batch-42/\")\n\njobs = [\n    SimulationJob(\n        model=variant,\n        weather=\"weather.epw\",\n        label=f\"case-{i}\",\n        output_dir=f\"case-{i}\",\n    )\n    for i, variant in enumerate(variants)\n]\n\nbatch = simulate_batch(jobs, max_workers=4, fs=fs)\n\n# All results stored in S3\nfor i, result in enumerate(batch):\n    print(f\"Case {i}: s3://my-bucket/batch-42/case-{i}/\")\n</code></pre>"},{"location":"examples/cloud-simulations/#s3-compatible-services","title":"S3-Compatible Services","text":"<p>Works with MinIO, LocalStack, and other S3-compatible APIs:</p>"},{"location":"examples/cloud-simulations/#minio","title":"MinIO","text":"<pre><code>fs = S3FileSystem(\n    bucket=\"local-bucket\",\n    endpoint_url=\"http://localhost:9000\",\n    aws_access_key_id=\"minioadmin\",\n    aws_secret_access_key=\"minioadmin\",\n)\n</code></pre>"},{"location":"examples/cloud-simulations/#localstack","title":"LocalStack","text":"<pre><code>fs = S3FileSystem(\n    bucket=\"test-bucket\",\n    endpoint_url=\"http://localhost:4566\",\n    region_name=\"us-east-1\",\n)\n</code></pre>"},{"location":"examples/cloud-simulations/#digitalocean-spaces","title":"DigitalOcean Spaces","text":"<pre><code>fs = S3FileSystem(\n    bucket=\"my-space\",\n    endpoint_url=\"https://nyc3.digitaloceanspaces.com\",\n    region_name=\"nyc3\",\n)\n</code></pre>"},{"location":"examples/cloud-simulations/#weather-file-handling","title":"Weather File Handling","text":"<p>Important: Weather files must be local. Download before simulating:</p> <pre><code>from idfkit.weather import StationIndex, WeatherDownloader\n\n# Download weather file locally\nindex = StationIndex.load()\nstation = index.search(\"chicago\")[0].station\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\n\n# Then use local weather with S3 output\nfs = S3FileSystem(bucket=\"results\", prefix=\"study/\")\nresult = simulate(\n    model,\n    files.epw,  # Local path\n    output_dir=\"run-001\",\n    fs=fs,\n)\n</code></pre>"},{"location":"examples/cloud-simulations/#performance-considerations","title":"Performance Considerations","text":""},{"location":"examples/cloud-simulations/#minimize-s3-round-trips","title":"Minimize S3 Round-Trips","text":"<pre><code># Query results once, process locally\nresult = SimulationResult.from_directory(\"run-001\", fs=fs)\n\n# This downloads the SQL file\nsql = result.sql\n\n# Multiple queries are local (file is cached)\nts1 = sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\nts2 = sql.get_timeseries(\"Zone Air Relative Humidity\", \"ZONE 1\")\n</code></pre>"},{"location":"examples/cloud-simulations/#batch-downloads","title":"Batch Downloads","text":"<p>For heavy analysis, download everything locally first:</p> <pre><code>import tempfile\nfrom pathlib import Path\n\nwith tempfile.TemporaryDirectory() as tmp:\n    # Download all files for a run\n    for obj in fs.glob(\"run-001\", \"*\"):\n        data = fs.read_bytes(obj)\n        local_path = Path(tmp) / Path(obj).name\n        local_path.write_bytes(data)\n\n    # Now use local result\n    result = SimulationResult.from_directory(tmp)\n    # Multiple queries without network calls\n</code></pre>"},{"location":"examples/cloud-simulations/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Store only necessary output files (filter before upload)</li> <li>Use S3 lifecycle policies to move old results to Glacier</li> <li>Consider S3 Intelligent Tiering for varying access patterns</li> <li>Use regional buckets close to compute resources</li> </ul>"},{"location":"examples/cloud-simulations/#security","title":"Security","text":"<ul> <li>Use IAM roles instead of access keys when possible</li> <li>Apply bucket policies to restrict access</li> <li>Enable S3 versioning for important results</li> <li>Consider server-side encryption</li> </ul>"},{"location":"examples/cloud-simulations/#see-also","title":"See Also","text":"<ul> <li>Cloud Storage Concepts \u2014 Architecture details</li> <li>Batch Processing \u2014 Parallel execution</li> <li>Caching \u2014 Local result caching</li> </ul>"},{"location":"examples/parametric-study/","title":"Parametric Study Example","text":"In\u00a0[\u00a0]: Copied! <pre>from idfkit import new_document\n</pre> from idfkit import new_document In\u00a0[\u00a0]: Copied! <pre>def create_model(insulation_thickness: float):\n    \"\"\"Create a simple box model with specified insulation thickness.\"\"\"\n    model = new_document(version=(24, 1, 0))\n\n    # Building and zone\n    model.add(\"Building\", \"Test Building\", north_axis=0, terrain=\"City\")\n    model.add(\"Zone\", \"Zone1\", x_origin=0, y_origin=0, z_origin=0)\n\n    # Materials\n    model.add(\n        \"Material\",\n        \"Concrete\",\n        roughness=\"MediumRough\",\n        thickness=0.2,\n        conductivity=1.73,\n        density=2243,\n        specific_heat=837,\n    )\n    model.add(\n        \"Material\",\n        \"Insulation\",\n        roughness=\"MediumSmooth\",\n        thickness=insulation_thickness,\n        conductivity=0.04,\n        density=32,\n        specific_heat=830,\n    )\n\n    # Construction\n    model.add(\n        \"Construction\",\n        \"Wall_Construction\",\n        outside_layer=\"Concrete\",\n        layer_2=\"Insulation\",\n    )\n\n    # ... add surfaces, etc. (simplified for this example)\n\n    return model\n\n\n# Create variants\nthicknesses = [0.025, 0.05, 0.075, 0.10, 0.15]\nmodels = {t: create_model(t) for t in thicknesses}\n\nprint(f\"Created {len(models)} model variants\")\n</pre> def create_model(insulation_thickness: float):     \"\"\"Create a simple box model with specified insulation thickness.\"\"\"     model = new_document(version=(24, 1, 0))      # Building and zone     model.add(\"Building\", \"Test Building\", north_axis=0, terrain=\"City\")     model.add(\"Zone\", \"Zone1\", x_origin=0, y_origin=0, z_origin=0)      # Materials     model.add(         \"Material\",         \"Concrete\",         roughness=\"MediumRough\",         thickness=0.2,         conductivity=1.73,         density=2243,         specific_heat=837,     )     model.add(         \"Material\",         \"Insulation\",         roughness=\"MediumSmooth\",         thickness=insulation_thickness,         conductivity=0.04,         density=32,         specific_heat=830,     )      # Construction     model.add(         \"Construction\",         \"Wall_Construction\",         outside_layer=\"Concrete\",         layer_2=\"Insulation\",     )      # ... add surfaces, etc. (simplified for this example)      return model   # Create variants thicknesses = [0.025, 0.05, 0.075, 0.10, 0.15] models = {t: create_model(t) for t in thicknesses}  print(f\"Created {len(models)} model variants\") In\u00a0[\u00a0]: Copied! <pre># Simulation code (requires EnergyPlus)\n\"\"\"\nfrom idfkit.simulation import simulate_batch, SimulationJob, SimulationCache\n\n# Create simulation jobs\njobs = [\n    SimulationJob(\n        model=model,\n        weather=\"chicago.epw\",\n        label=f\"insulation-{thickness}m\",\n        design_day=True,\n    )\n    for thickness, model in models.items()\n]\n\n# Run with progress tracking\ncache = SimulationCache()\n\ndef progress(completed, total, label, success):\n    status = \"OK\" if success else \"FAIL\"\n    print(f\"[{completed}/{total}] {label}: {status}\")\n\nbatch = simulate_batch(\n    jobs,\n    max_workers=4,\n    cache=cache,\n    progress=progress,\n)\n\nprint(f\"\\nCompleted: {len(batch.succeeded)}/{len(batch)}\")\n\"\"\"\nprint(\"Simulation code shown above (requires EnergyPlus)\")\n</pre> # Simulation code (requires EnergyPlus) \"\"\" from idfkit.simulation import simulate_batch, SimulationJob, SimulationCache  # Create simulation jobs jobs = [     SimulationJob(         model=model,         weather=\"chicago.epw\",         label=f\"insulation-{thickness}m\",         design_day=True,     )     for thickness, model in models.items() ]  # Run with progress tracking cache = SimulationCache()  def progress(completed, total, label, success):     status = \"OK\" if success else \"FAIL\"     print(f\"[{completed}/{total}] {label}: {status}\")  batch = simulate_batch(     jobs,     max_workers=4,     cache=cache,     progress=progress, )  print(f\"\\nCompleted: {len(batch.succeeded)}/{len(batch)}\") \"\"\" print(\"Simulation code shown above (requires EnergyPlus)\") In\u00a0[\u00a0]: Copied! <pre># Analysis code (requires simulation results)\n\"\"\"\nimport matplotlib.pyplot as plt\n\n# Extract peak cooling loads\nresults_data = []\n\nfor thickness, result in zip(thicknesses, batch):\n    if result.success:\n        # Get cooling design day results\n        ts = result.sql.get_timeseries(\n            \"Zone Ideal Loads Supply Air Total Cooling Energy\",\n            \"Zone1\",\n        )\n        peak_cooling = max(ts.values)\n        results_data.append({\n            \"thickness\": thickness * 100,  # Convert to cm\n            \"peak_cooling\": peak_cooling / 1000,  # Convert to kW\n        })\n\n# Plot results\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.bar(\n    [d[\"thickness\"] for d in results_data],\n    [d[\"peak_cooling\"] for d in results_data],\n)\nax.set_xlabel(\"Insulation Thickness (cm)\")\nax.set_ylabel(\"Peak Cooling Load (kW)\")\nax.set_title(\"Impact of Insulation Thickness on Peak Cooling Load\")\n\nplt.tight_layout()\nplt.show()\n\"\"\"\nprint(\"Analysis code shown above (requires simulation results)\")\n</pre> # Analysis code (requires simulation results) \"\"\" import matplotlib.pyplot as plt  # Extract peak cooling loads results_data = []  for thickness, result in zip(thicknesses, batch):     if result.success:         # Get cooling design day results         ts = result.sql.get_timeseries(             \"Zone Ideal Loads Supply Air Total Cooling Energy\",             \"Zone1\",         )         peak_cooling = max(ts.values)         results_data.append({             \"thickness\": thickness * 100,  # Convert to cm             \"peak_cooling\": peak_cooling / 1000,  # Convert to kW         })  # Plot results fig, ax = plt.subplots(figsize=(10, 6))  ax.bar(     [d[\"thickness\"] for d in results_data],     [d[\"peak_cooling\"] for d in results_data], ) ax.set_xlabel(\"Insulation Thickness (cm)\") ax.set_ylabel(\"Peak Cooling Load (kW)\") ax.set_title(\"Impact of Insulation Thickness on Peak Cooling Load\")  plt.tight_layout() plt.show() \"\"\" print(\"Analysis code shown above (requires simulation results)\")"},{"location":"examples/parametric-study/#parametric-study-example","title":"Parametric Study Example\u00b6","text":"<p>This notebook demonstrates how to run a parametric study varying insulation thickness and comparing results across variants.</p>"},{"location":"examples/parametric-study/#setup","title":"Setup\u00b6","text":"<p>First, ensure you have idfkit installed with plotting support:</p> <pre>pip install idfkit[plot,dataframes]\n</pre>"},{"location":"examples/parametric-study/#create-a-base-model","title":"Create a Base Model\u00b6","text":"<p>We'll create a simple single-zone model with configurable wall insulation.</p>"},{"location":"examples/parametric-study/#run-batch-simulation","title":"Run Batch Simulation\u00b6","text":"<p>Use <code>simulate_batch()</code> to run all variants in parallel.</p> <p>Note: This requires EnergyPlus to be installed. The code below shows the pattern but won't execute without EnergyPlus.</p>"},{"location":"examples/parametric-study/#analyze-results","title":"Analyze Results\u00b6","text":"<p>Extract and compare results across variants.</p>"},{"location":"examples/parametric-study/#summary","title":"Summary\u00b6","text":"<p>This example demonstrated:</p> <ol> <li>Creating model variants with <code>model.copy()</code> and field modification</li> <li>Running parallel simulations with <code>simulate_batch()</code></li> <li>Using <code>SimulationCache</code> to avoid redundant work</li> <li>Extracting and comparing time-series results</li> </ol>"},{"location":"examples/parametric-study/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Design Day Sizing \u2014 Apply weather data to models</li> <li>Batch Processing \u2014 Detailed batch guide</li> <li>SQL Queries \u2014 Query result data</li> </ul>"},{"location":"examples/scythe/","title":"Distributed Simulations with Scythe","text":"<p>Scythe is a lightweight framework for running embarrassingly parallel experiments at scale via the Hatchet distributed task queue. It handles artifact management, S3 storage, and result collection so you can focus on the simulation logic.</p> <p>By combining idfkit for EnergyPlus model manipulation and simulation with Scythe for distributed orchestration, you can run large parametric studies across hundreds or thousands of building variants without writing your own queuing or storage infrastructure.</p> <p>Note</p> <p>Scythe is an independent project in early development. See the Scythe documentation for the latest API details and setup instructions.</p>"},{"location":"examples/scythe/#prerequisites","title":"Prerequisites","text":"<p>Install both packages:</p> <pre><code>pip install idfkit scythe-engine\n</code></pre> <p>Workers need EnergyPlus installed. The NREL Docker images are a convenient base for containerized deployments.</p> <p>You also need a running Hatchet instance (self-hosted or cloud) and an S3-compatible bucket for artifacts. See hatchet-sst for a self-hosting guide.</p>"},{"location":"examples/scythe/#how-it-works","title":"How It Works","text":"<p>Scythe follows a scatter-gather pattern:</p> <ol> <li>Define input and output schemas as Pydantic models</li> <li>Register an experiment function that maps one input to one output</li> <li>Allocate a batch of input specs; Scythe uploads artifacts and enqueues tasks</li> <li>Workers pull tasks from the Hatchet queue and execute them</li> <li>Gather results from S3 as organized Parquet files</li> </ol> <p>idfkit fits into step 2: the registered experiment function uses idfkit to load an IDF, apply parameter changes, run the EnergyPlus simulation, and extract results.</p>"},{"location":"examples/scythe/#step-1-define-inputoutput-schemas","title":"Step 1: Define Input/Output Schemas","text":"<p>Schemas inherit from Scythe's <code>ExperimentInputSpec</code> and <code>ExperimentOutputSpec</code>. Use <code>FileReference</code> fields for files that Scythe should manage (upload to / download from S3 automatically).</p> <pre><code>from typing import Literal\n\nfrom pydantic import Field\nfrom scythe.base import ExperimentInputSpec, ExperimentOutputSpec\nfrom scythe.utils.filesys import FileReference\n\n\nclass BuildingSimInput(ExperimentInputSpec):\n    \"\"\"Input specification for a parametric building energy study.\"\"\"\n\n    r_value: float = Field(description=\"Wall insulation R-value [m2K/W]\", ge=0, le=15)\n    lpd: float = Field(description=\"Lighting power density [W/m2]\", ge=0, le=20)\n    setpoint: float = Field(description=\"Cooling setpoint [deg C]\", ge=18, le=30)\n    economizer: Literal[\"NoEconomizer\", \"DifferentialDryBulb\", \"DifferentialEnthalpy\"] = Field(\n        description=\"Economizer type\"\n    )\n    idf_file: FileReference = Field(description=\"Base IDF model file\")\n    weather_file: FileReference = Field(description=\"EPW weather file\")\n    design_day_file: FileReference = Field(description=\"DDY design day file\")\n\n\nclass BuildingSimOutput(ExperimentOutputSpec):\n    \"\"\"Output specification with scalar results and time-series data.\"\"\"\n\n    heating_kwh_m2: float = Field(description=\"Annual heating [kWh/m2]\", ge=0)\n    cooling_kwh_m2: float = Field(description=\"Annual cooling [kWh/m2]\", ge=0)\n    lighting_kwh_m2: float = Field(description=\"Annual lighting [kWh/m2]\", ge=0)\n    fans_kwh_m2: float = Field(description=\"Annual fan energy [kWh/m2]\", ge=0)\n    total_eui: float = Field(description=\"Total site EUI [kWh/m2]\", ge=0)\n    timeseries: FileReference = Field(description=\"Hourly results CSV\")\n</code></pre> <p>Key points:</p> <ul> <li>Scalar fields (<code>r_value</code>, <code>lpd</code>, etc.) are collected into a Parquet table   for analysis.</li> <li><code>FileReference</code> fields accept local paths, HTTP URLs, or S3 URIs. Scythe   uploads local files to S3 at allocation time and resolves them back to local   paths on the worker.</li> <li>Pydantic <code>Field</code> constraints (<code>ge</code>, <code>le</code>) provide automatic validation.</li> </ul>"},{"location":"examples/scythe/#step-2-register-the-experiment-function","title":"Step 2: Register the Experiment Function","text":"<p>The experiment function receives a single <code>BuildingSimInput</code> and a temporary working directory, and returns a <code>BuildingSimOutput</code>. This is where idfkit does the heavy lifting.</p> <pre><code>@ExperimentRegistry.Register()\ndef simulate_building(input_spec: BuildingSimInput, tempdir: Path) -&gt; BuildingSimOutput:\n    \"\"\"Run a single parametric EnergyPlus simulation using idfkit.\"\"\"\n    from idfkit import load_idf\n    from idfkit.simulation import simulate\n    from idfkit.weather import DesignDayManager\n\n    # Load the base IDF model (FileReference resolves to a local path)\n    model = load_idf(input_spec.idf_file)\n\n    # Apply parametric overrides\n    for material in model[\"Material\"]:\n        if \"wall_insulation\" in material.Name.lower():\n            material.Thermal_Resistance = input_spec.r_value\n\n    for lights in model[\"Lights\"]:\n        lights.Watts_per_Zone_Floor_Area = input_spec.lpd\n\n    for thermostat in model[\"ThermostatSetpoint:DualSetpoint\"]:\n        # Adjust cooling setpoint schedule\n        pass  # modify as needed for your model\n\n    # Inject ASHRAE design days from the DDY file\n    ddm = DesignDayManager(input_spec.design_day_file)\n    ddm.apply_to_model(model)\n\n    # Run the simulation\n    result = simulate(\n        model,\n        weather=input_spec.weather_file,\n        output_dir=tempdir / \"run\",\n        annual=True,\n    )\n\n    # Extract end-use totals from the SQL output\n    sql = result.sql\n    rows = sql.get_tabular_data(\n        report_name=\"AnnualBuildingUtilityPerformanceSummary\",\n        table_name=\"End Uses\",\n    )\n\n    # Build a lookup: (row_name, column_name) -&gt; value\n    end_use = {(r.row_name, r.column_name): r.value for r in rows}\n\n    # Get conditioned floor area from the building summary\n    area_rows = sql.get_tabular_data(\n        report_name=\"AnnualBuildingUtilityPerformanceSummary\",\n        table_name=\"Building Area\",\n    )\n    floor_area = float(next(r.value for r in area_rows if r.row_name == \"Net Conditioned Building Area\"))\n\n    # Write hourly time-series to CSV for the FileReference output\n    csv_path = tempdir / \"timeseries.csv\"\n    if result.csv is not None:\n        with open(csv_path, \"w\") as f:\n            f.write(\"timestamp,\" + \",\".join(c.header for c in result.csv.columns) + \"\\n\")\n            for i, ts in enumerate(result.csv.timestamps):\n                vals = \",\".join(str(c.values[i]) for c in result.csv.columns)\n                f.write(f\"{ts},{vals}\\n\")\n\n    return BuildingSimOutput(\n        heating_kwh_m2=float(end_use.get((\"Heating\", \"Electricity\"), 0)) / floor_area,\n        cooling_kwh_m2=float(end_use.get((\"Cooling\", \"Electricity\"), 0)) / floor_area,\n        lighting_kwh_m2=float(end_use.get((\"Interior Lighting\", \"Electricity\"), 0)) / floor_area,\n        fans_kwh_m2=float(end_use.get((\"Fans\", \"Electricity\"), 0)) / floor_area,\n        total_eui=sum(float(v) for (row, col), v in end_use.items() if row == \"Total End Uses\" and col == \"Electricity\")\n        / floor_area,\n        timeseries=csv_path,\n        dataframes={},\n    )\n</code></pre> <p>Inside the function you have full access to the idfkit API:</p> <ul> <li><code>load_idf()</code> parses the model file that Scythe downloaded to a local path</li> <li>Object manipulation applies parametric changes (R-values, lighting, setpoints)</li> <li><code>DesignDayManager</code> loads a DDY file by path and injects design days into the model</li> <li><code>simulate()</code> runs EnergyPlus and returns structured results</li> <li><code>result.sql.get_tabular_data()</code> queries the SQLite output for tabular end-use data</li> <li><code>result.csv</code> provides access to time-series column data</li> </ul> <p>Any file you write to <code>tempdir</code> and return as a <code>FileReference</code> gets uploaded to S3 automatically.</p>"},{"location":"examples/scythe/#step-3-prepare-weather-data","title":"Step 3: Prepare Weather Data","text":"<p>Use idfkit's weather module to find stations and download EPW/DDY files before allocating experiments:</p> <pre><code>from idfkit.weather import StationIndex, WeatherDownloader\n\n# Find the closest weather station\nindex = StationIndex.load()\nresults = index.nearest(latitude=42.36, longitude=-71.06, limit=3)\n\n# Download EPW and DDY files\ndownloader = WeatherDownloader()\nfor result in results:\n    files = downloader.download(result.station)\n    print(f\"{result.station.display_name} ({result.distance_km:.1f} km): {files.epw}, {files.ddy}\")\n\n# Use these local paths (or upload to S3) as FileReference inputs to Scythe\n</code></pre> <p>You can pass local file paths, HTTP URLs, or S3 URIs as <code>FileReference</code> values in your input specs. Scythe handles the upload and distribution to workers.</p>"},{"location":"examples/scythe/#step-4-allocate-the-experiment","title":"Step 4: Allocate the Experiment","text":"<p>Create a parameter grid, build input specs, and let Scythe enqueue everything:</p> <pre><code>import itertools\n\nimport boto3\nimport pandas as pd\nfrom scythe.base import BaseExperiment\nfrom scythe.utils.recursion import RecursionMap\n\n# Assume these are defined in your experiments module\nfrom experiments.building_energy import BuildingSimInput, simulate_building\n\n# Define parameter grid\nr_values = [2.0, 3.5, 5.0, 7.0]\nlpds = [5.0, 8.0, 12.0]\nsetpoints = [22.0, 24.0, 26.0]\neconomizers = [\"NoEconomizer\", \"DifferentialDryBulb\"]\n\n# Build a DataFrame of all combinations\ncombos = list(itertools.product(r_values, lpds, setpoints, economizers))\ndf = pd.DataFrame(combos, columns=[\"r_value\", \"lpd\", \"setpoint\", \"economizer\"])\n\n# Add file references (same base model + weather for all runs)\ndf[\"idf_file\"] = \"s3://my-bucket/models/office_base.idf\"\ndf[\"weather_file\"] = \"s3://my-bucket/weather/USA_MA_Boston-Logan.epw\"\ndf[\"design_day_file\"] = \"s3://my-bucket/weather/USA_MA_Boston-Logan.ddy\"\n\n# Validate and allocate\nspecs = [BuildingSimInput.model_validate(row.to_dict()) for _, row in df.iterrows()]\n\nexperiment = BaseExperiment(experiment=simulate_building)\nexperiment.allocate(\n    specs,\n    version=\"bumpminor\",\n    s3_client=boto3.client(\"s3\"),\n    recursion_map=RecursionMap(factor=2, max_depth=3),\n)\n</code></pre> <p>This creates <code>4 x 3 x 3 x 2 = 72</code> simulation tasks. Scythe uploads the IDF, EPW, and DDY files to S3 (deduplicating shared files), serializes the specs, and pushes work items to the Hatchet queue.</p> <p>The <code>RecursionMap</code> controls how Scythe fans out work across workers. A <code>factor=2, max_depth=3</code> configuration splits the batch into progressively smaller chunks for efficient scheduling.</p>"},{"location":"examples/scythe/#step-5-run-workers","title":"Step 5: Run Workers","text":"<p>Each worker imports the registered experiments and starts the Scythe worker loop:</p> <pre><code>from scythe.worker import ScytheWorkerConfig\n\n# Import your experiments so they are registered\nfrom experiments.building_energy import simulate_building  # noqa: F401  # pyright: ignore[reportUnusedImport]\n\nif __name__ == \"__main__\":\n    worker_config = ScytheWorkerConfig()\n    worker_config.start()\n</code></pre> <p>Workers pull tasks from the Hatchet queue, download input artifacts from S3, call the experiment function, and upload outputs back to S3.</p>"},{"location":"examples/scythe/#docker-setup","title":"Docker Setup","text":"<p>For containerized workers with EnergyPlus pre-installed:</p> <pre><code>FROM nrel/energyplus:24.2.0\n\nRUN pip install idfkit scythe-engine\n\nCOPY experiments/ /app/experiments/\nCOPY main.py /app/main.py\n\nWORKDIR /app\nCMD [\"python\", \"main.py\"]\n</code></pre> <p>Scale horizontally by running multiple container instances against the same Hatchet queue.</p>"},{"location":"examples/scythe/#step-6-gather-results","title":"Step 6: Gather Results","text":"<p>Once all tasks complete, Scythe organizes outputs in S3:</p> <pre><code>s3://my-bucket/experiments/&lt;name&gt;/&lt;version&gt;/\n    manifest.yml              # Experiment metadata\n    specs.pq                  # All input specs as Parquet\n    scalars.pq                # All scalar outputs (heating, cooling, etc.)\n    result_file_refs.pq       # S3 paths to output files (timeseries CSVs)\n    experiment_io_spec.yml    # JSON Schema of input/output definitions\n</code></pre> <p>Load the scalar results directly with pandas:</p> <pre><code>import pandas as pd\n\nscalars = pd.read_parquet(\"s3://my-bucket/experiments/.../scalars.pq\")\nprint(scalars.describe())\n</code></pre> <p>The <code>scalars.pq</code> file contains a MultiIndex linking each output row back to its input parameters, making it straightforward to pivot, group, and plot results.</p>"},{"location":"examples/scythe/#idfkit-features-useful-in-scythe-experiments","title":"idfkit Features Useful in Scythe Experiments","text":"idfkit Feature Use Case in Scythe <code>load_idf()</code> / <code>load_epjson()</code> Load base model from <code>FileReference</code> Object field manipulation Apply parametric changes per spec <code>DesignDayManager</code> Inject design days from DDY files <code>simulate()</code> Run EnergyPlus inside the experiment function <code>result.sql.get_tabular_data()</code> Query tabular end-use summaries <code>result.csv</code> Extract time-series for <code>FileReference</code> output <code>rotate_building()</code> Orientation studies <code>model.copy()</code> Create variants from a shared base Weather station search Prepare EPW/DDY files before allocation"},{"location":"examples/scythe/#tips","title":"Tips","text":"<ul> <li>Keep experiment functions focused. Do model setup, simulation, and result   extraction in the registered function. Avoid heavy post-processing; save raw   outputs and analyze after gathering.</li> <li>Use <code>FileReference</code> for large outputs. Scalar fields go into the Parquet   summary; file references point to full time-series or report files in S3.</li> <li>Deduplicate shared inputs. When all specs share the same IDF or EPW file,   Scythe uploads it once. Use S3 URIs to avoid redundant uploads.</li> <li>Pin EnergyPlus versions. Use <code>idfkit.find_energyplus(version=...)</code> or set   <code>ENERGYPLUS_DIR</code> in your Docker image to ensure reproducible results.</li> <li>Test locally first. Call your experiment function directly with a single   spec and a temporary directory before allocating a full batch.</li> </ul>"},{"location":"examples/scythe/#see-also","title":"See Also","text":"<ul> <li>Scythe documentation</li> <li>Scythe example repository</li> <li>Batch Processing -- idfkit's built-in thread-pool batch runner</li> <li>Cloud Simulations (S3) -- Using idfkit with S3 directly</li> <li>Running Simulations -- Single simulation guide</li> </ul>"},{"location":"examples/sizing-workflow/","title":"Design Day Sizing Workflow","text":"In\u00a0[\u00a0]: Copied! <pre>from idfkit.weather import (\n    DesignDayManager,\n    StationIndex,\n    WeatherDownloader,\n    geocode,\n)\n</pre> from idfkit.weather import (     DesignDayManager,     StationIndex,     WeatherDownloader,     geocode, ) In\u00a0[\u00a0]: Copied! <pre>project_address = \"Willis Tower, Chicago, IL\"\n\nlat, lon = geocode(project_address)\nprint(f\"Project location: {lat:.4f}, {lon:.4f}\")\n</pre> project_address = \"Willis Tower, Chicago, IL\"  lat, lon = geocode(project_address) print(f\"Project location: {lat:.4f}, {lon:.4f}\") In\u00a0[\u00a0]: Copied! <pre>index = StationIndex.load()\n\nresults = index.nearest(lat, lon, limit=5)\n\nprint(\"Nearest weather stations:\")\nfor r in results:\n    print(f\"  {r.station.display_name}: {r.distance_km:.1f} km\")\n\n# Use the nearest station\nstation = results[0].station\nprint(f\"\\nSelected: {station.display_name}\")\n</pre> index = StationIndex.load()  results = index.nearest(lat, lon, limit=5)  print(\"Nearest weather stations:\") for r in results:     print(f\"  {r.station.display_name}: {r.distance_km:.1f} km\")  # Use the nearest station station = results[0].station print(f\"\\nSelected: {station.display_name}\") In\u00a0[\u00a0]: Copied! <pre>downloader = WeatherDownloader()\nfiles = downloader.download(station)\n\nprint(f\"EPW file: {files.epw}\")\nprint(f\"DDY file: {files.ddy}\")\n</pre> downloader = WeatherDownloader() files = downloader.download(station)  print(f\"EPW file: {files.epw}\") print(f\"DDY file: {files.ddy}\") In\u00a0[\u00a0]: Copied! <pre>ddm = DesignDayManager(files.ddy)\n\nprint(ddm.summary())\n</pre> ddm = DesignDayManager(files.ddy)  print(ddm.summary()) In\u00a0[\u00a0]: Copied! <pre>from idfkit import new_document\n\n# Create a simple model (or use load_idf() for an existing file)\nmodel = new_document(version=(24, 1, 0))\nmodel.add(\"Building\", \"Chicago Office\", north_axis=0, terrain=\"City\")\nmodel.add(\"Zone\", \"Zone1\", x_origin=0, y_origin=0, z_origin=0)\n\n# ... add materials, constructions, surfaces, etc.\n\nprint(f\"Model has {len(model)} objects\")\n</pre> from idfkit import new_document  # Create a simple model (or use load_idf() for an existing file) model = new_document(version=(24, 1, 0)) model.add(\"Building\", \"Chicago Office\", north_axis=0, terrain=\"City\") model.add(\"Zone\", \"Zone1\", x_origin=0, y_origin=0, z_origin=0)  # ... add materials, constructions, surfaces, etc.  print(f\"Model has {len(model)} objects\") In\u00a0[\u00a0]: Copied! <pre>added = ddm.apply_to_model(\n    model,\n    heating=\"99.6%\",  # ASHRAE 90.1 heating\n    cooling=\"1%\",  # ASHRAE 90.1 cooling\n    include_wet_bulb=True,  # Include wet-bulb design day\n    update_location=True,  # Update Site:Location\n)\n\nprint(f\"Added {len(added)} design days:\")\nfor name in added:\n    print(f\"  {name}\")\n</pre> added = ddm.apply_to_model(     model,     heating=\"99.6%\",  # ASHRAE 90.1 heating     cooling=\"1%\",  # ASHRAE 90.1 cooling     include_wet_bulb=True,  # Include wet-bulb design day     update_location=True,  # Update Site:Location )  print(f\"Added {len(added)} design days:\") for name in added:     print(f\"  {name}\") In\u00a0[\u00a0]: Copied! <pre># Check Site:Location\nif \"Site:Location\" in model:\n    location = model[\"Site:Location\"][0]\n    print(f\"Site Location: {location.name}\")\n    print(f\"  Latitude: {location.latitude}\")\n    print(f\"  Longitude: {location.longitude}\")\n\n# Check design days\nif \"SizingPeriod:DesignDay\" in model:\n    print(f\"\\nDesign days in model: {len(model['SizingPeriod:DesignDay'])}\")\n    for dd in model[\"SizingPeriod:DesignDay\"]:\n        print(f\"  {dd.name}\")\n</pre> # Check Site:Location if \"Site:Location\" in model:     location = model[\"Site:Location\"][0]     print(f\"Site Location: {location.name}\")     print(f\"  Latitude: {location.latitude}\")     print(f\"  Longitude: {location.longitude}\")  # Check design days if \"SizingPeriod:DesignDay\" in model:     print(f\"\\nDesign days in model: {len(model['SizingPeriod:DesignDay'])}\")     for dd in model[\"SizingPeriod:DesignDay\"]:         print(f\"  {dd.name}\") In\u00a0[\u00a0]: Copied! <pre># Simulation code (requires EnergyPlus)\n\"\"\"\nfrom idfkit.simulation import simulate\n\nresult = simulate(\n    model,\n    files.epw,\n    design_day=True,  # Design-day-only for sizing\n)\n\nprint(f\"Success: {result.success}\")\nprint(f\"Runtime: {result.runtime_seconds:.1f}s\")\n\nif result.success:\n    # Check sizing results\n    tables = result.sql.get_tabular_data(\n        report_name=\"HVACSizingSummary\"\n    )\n    print(f\"\\nSizing report has {len(tables)} rows\")\n\"\"\"\nprint(\"Simulation code shown above (requires EnergyPlus)\")\n</pre> # Simulation code (requires EnergyPlus) \"\"\" from idfkit.simulation import simulate  result = simulate(     model,     files.epw,     design_day=True,  # Design-day-only for sizing )  print(f\"Success: {result.success}\") print(f\"Runtime: {result.runtime_seconds:.1f}s\")  if result.success:     # Check sizing results     tables = result.sql.get_tabular_data(         report_name=\"HVACSizingSummary\"     )     print(f\"\\nSizing report has {len(tables)} rows\") \"\"\" print(\"Simulation code shown above (requires EnergyPlus)\")"},{"location":"examples/sizing-workflow/#design-day-sizing-workflow","title":"Design Day Sizing Workflow\u00b6","text":"<p>This notebook demonstrates the complete workflow from finding a weather station by address to applying ASHRAE design day conditions and running a sizing simulation.</p>"},{"location":"examples/sizing-workflow/#setup","title":"Setup\u00b6","text":"<p>Install idfkit with weather support:</p> <pre>pip install idfkit[weather]\n</pre>"},{"location":"examples/sizing-workflow/#step-1-geocode-the-project-address","title":"Step 1: Geocode the Project Address\u00b6","text":"<p>Convert the project address to latitude/longitude coordinates.</p>"},{"location":"examples/sizing-workflow/#step-2-find-the-nearest-weather-station","title":"Step 2: Find the Nearest Weather Station\u00b6","text":"<p>Search the station index for the nearest weather data.</p>"},{"location":"examples/sizing-workflow/#step-3-download-weather-files","title":"Step 3: Download Weather Files\u00b6","text":"<p>Download the EPW and DDY files for this station.</p>"},{"location":"examples/sizing-workflow/#step-4-explore-design-day-conditions","title":"Step 4: Explore Design Day Conditions\u00b6","text":"<p>Parse the DDY file and examine available design days.</p>"},{"location":"examples/sizing-workflow/#step-5-createload-your-model","title":"Step 5: Create/Load Your Model\u00b6","text":"<p>Load an existing model or create one.</p>"},{"location":"examples/sizing-workflow/#step-6-apply-design-days-to-model","title":"Step 6: Apply Design Days to Model\u00b6","text":"<p>Inject ASHRAE 90.1 design day conditions.</p>"},{"location":"examples/sizing-workflow/#step-7-verify-model-setup","title":"Step 7: Verify Model Setup\u00b6","text":"<p>Check that design days and location were added correctly.</p>"},{"location":"examples/sizing-workflow/#step-8-run-sizing-simulation","title":"Step 8: Run Sizing Simulation\u00b6","text":"<p>Execute a design-day-only simulation for HVAC sizing.</p> <p>Note: This requires EnergyPlus to be installed.</p>"},{"location":"examples/sizing-workflow/#summary","title":"Summary\u00b6","text":"<p>This workflow demonstrated:</p> <ol> <li>Geocoding \u2014 Convert address to coordinates with <code>geocode()</code></li> <li>Station search \u2014 Find nearest station with <code>index.nearest()</code></li> <li>Download \u2014 Get weather files with <code>WeatherDownloader</code></li> <li>Design days \u2014 Parse and apply with <code>DesignDayManager</code></li> <li>Simulation \u2014 Run sizing with <code>simulate(design_day=True)</code></li> </ol>"},{"location":"examples/sizing-workflow/#one-liner-alternative","title":"One-Liner Alternative\u00b6","text":"<p>For simple cases, use <code>apply_ashrae_sizing()</code>:</p> <pre>from idfkit.weather import apply_ashrae_sizing\n\napply_ashrae_sizing(model, ddy_path=files.ddy, standard=\"90.1\")\n</pre>"},{"location":"examples/sizing-workflow/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Parametric Study \u2014 Vary parameters and compare</li> <li>Design Days Guide \u2014 Detailed design day info</li> <li>Station Search Guide \u2014 Advanced search</li> </ul>"},{"location":"getting-started/core-tutorial/","title":"Core Tutorial","text":"In\u00a0[1]: Copied! <pre>import idfkit\n\nprint(f\"idfkit version: {idfkit.__version__}\")\n</pre> import idfkit  print(f\"idfkit version: {idfkit.__version__}\") <pre>idfkit version: 0.1.0\n</pre> In\u00a0[2]: Copied! <pre>from idfkit import new_document\n\nmodel = new_document(version=(24, 1, 0))\nprint(model)\nprint(f\"Version : {model.version}\")\nprint(f\"Objects : {len(model)}\")\n</pre> from idfkit import new_document  model = new_document(version=(24, 1, 0)) print(model) print(f\"Version : {model.version}\") print(f\"Objects : {len(model)}\") <pre>IDFDocument(version=24.1.0, objects=0)\n\nVersion : (24, 1, 0)\nObjects : 0\n</pre> In\u00a0[3]: Copied! <pre># Add a Building object\nmodel.add(\n    \"Building\",\n    \"My Office Building\",\n    {\n        \"north_axis\": 0,\n        \"terrain\": \"City\",\n        \"loads_convergence_tolerance_value\": 0.04,\n        \"temperature_convergence_tolerance_value\": 0.4,\n        \"solar_distribution\": \"FullInteriorAndExterior\",\n        \"maximum_number_of_warmup_days\": 25,\n    },\n)\n\n# Add zones using keyword arguments\nmodel.add(\"Zone\", \"Office_Zone\", x_origin=0.0, y_origin=0.0, z_origin=0.0)\nmodel.add(\"Zone\", \"Corridor_Zone\", x_origin=10.0, y_origin=0.0, z_origin=0.0)\n\nprint(model)\n</pre> # Add a Building object model.add(     \"Building\",     \"My Office Building\",     {         \"north_axis\": 0,         \"terrain\": \"City\",         \"loads_convergence_tolerance_value\": 0.04,         \"temperature_convergence_tolerance_value\": 0.4,         \"solar_distribution\": \"FullInteriorAndExterior\",         \"maximum_number_of_warmup_days\": 25,     }, )  # Add zones using keyword arguments model.add(\"Zone\", \"Office_Zone\", x_origin=0.0, y_origin=0.0, z_origin=0.0) model.add(\"Zone\", \"Corridor_Zone\", x_origin=10.0, y_origin=0.0, z_origin=0.0)  print(model) <pre>IDFDocument(version=24.1.0, objects=3)\n\n  Building: 1 objects\n  Zone: 2 objects\n</pre> In\u00a0[4]: Copied! <pre># Bracket notation\nzones = model[\"Zone\"]\nprint(f\"Number of zones: {len(zones)}\")\n\n# Attribute notation (shorthand)\nzones_alt = model.zones\nprint(f\"Same collection? {zones is zones_alt}\")\n\n# Get a specific object by name\noffice = model[\"Zone\"][\"Office_Zone\"]\nprint(f\"Zone name: {office.name}\")\nprint(f\"Zone type: {office.obj_type}\")\n</pre> # Bracket notation zones = model[\"Zone\"] print(f\"Number of zones: {len(zones)}\")  # Attribute notation (shorthand) zones_alt = model.zones print(f\"Same collection? {zones is zones_alt}\")  # Get a specific object by name office = model[\"Zone\"][\"Office_Zone\"] print(f\"Zone name: {office.name}\") print(f\"Zone type: {office.obj_type}\") <pre>Number of zones: 2\nSame collection? True\nZone name: Office_Zone\nZone type: Zone\n</pre> In\u00a0[5]: Copied! <pre># Read field values\nprint(f\"X origin: {office.x_origin}\")\nprint(f\"Y origin: {office.y_origin}\")\n\n# Modify a field\noffice.x_origin = 5.0\nprint(f\"Updated X origin: {office.x_origin}\")\n\n# Access via index (0 = name, 1+ = fields)\nprint(f\"Name via index: {office[0]}\")\n</pre> # Read field values print(f\"X origin: {office.x_origin}\") print(f\"Y origin: {office.y_origin}\")  # Modify a field office.x_origin = 5.0 print(f\"Updated X origin: {office.x_origin}\")  # Access via index (0 = name, 1+ = fields) print(f\"Name via index: {office[0]}\") <pre>X origin: 0.0\nY origin: 0.0\nUpdated X origin: 5.0\nName via index: Office_Zone\n</pre> In\u00a0[6]: Copied! <pre># Iterate over a collection\nfor zone in model[\"Zone\"]:\n    print(f\"  Zone: {zone.name} at ({zone.x_origin}, {zone.y_origin}, {zone.z_origin})\")\n\n# Iterate over all objects in the document\nprint(f\"\\nAll objects ({len(model)}):\")\nfor obj in model.all_objects:\n    print(f\"  {obj.obj_type}: {obj.name}\")\n</pre> # Iterate over a collection for zone in model[\"Zone\"]:     print(f\"  Zone: {zone.name} at ({zone.x_origin}, {zone.y_origin}, {zone.z_origin})\")  # Iterate over all objects in the document print(f\"\\nAll objects ({len(model)}):\") for obj in model.all_objects:     print(f\"  {obj.obj_type}: {obj.name}\") <pre>  Zone: Office_Zone at (5.0, 0.0, 0.0)\n  Zone: Corridor_Zone at (10.0, 0.0, 0.0)\n\nAll objects (3):\n  Building: My Office Building\n  Zone: Office_Zone\n  Zone: Corridor_Zone\n</pre> In\u00a0[7]: Copied! <pre>from idfkit import write_epjson, write_idf\n\n# Get IDF as a string\nidf_string = write_idf(model)\nprint(\"=== IDF output (first 600 chars) ===\")\nprint(idf_string[:600])\nprint(\"...\")\n</pre> from idfkit import write_epjson, write_idf  # Get IDF as a string idf_string = write_idf(model) print(\"=== IDF output (first 600 chars) ===\") print(idf_string[:600]) print(\"...\") <pre>=== IDF output (first 600 chars) ===\n!-Generator archetypal\n!-Option SortedOrder\n\nVersion,\n  24.1;                    !- Version Identifier\n\nBuilding,\n  My Office Building,         !- Name\n  0,                          !- North Axis\n  City,                       !- Terrain\n  0.04,                       !- Loads Convergence Tolerance Value\n  0.4,                        !- Temperature Convergence Tolerance Value\n  FullInteriorAndExterior,    !- Solar Distribution\n  25,                         !- Maximum Number Of Warmup Days\n  ;                           !- Minimum Number Of Warmup Days\n\nZone,\n  Office_Zone,                !- Name\n\n...\n</pre> In\u00a0[8]: Copied! <pre># Get epJSON as a string\nepjson_string = write_epjson(model)\nprint(\"=== epJSON output (first 600 chars) ===\")\nprint(epjson_string[:600])\nprint(\"...\")\n</pre> # Get epJSON as a string epjson_string = write_epjson(model) print(\"=== epJSON output (first 600 chars) ===\") print(epjson_string[:600]) print(\"...\") <pre>=== epJSON output (first 600 chars) ===\n{\n  \"Version\": {\n    \"Version 1\": {\n      \"version_identifier\": \"24.1\"\n    }\n  },\n  \"Building\": {\n    \"My Office Building\": {\n      \"north_axis\": 0,\n      \"terrain\": \"City\",\n      \"loads_convergence_tolerance_value\": 0.04,\n      \"temperature_convergence_tolerance_value\": 0.4,\n      \"solar_distribution\": \"FullInteriorAndExterior\",\n      \"maximum_number_of_warmup_days\": 25\n    }\n  },\n  \"Zone\": {\n    \"Office_Zone\": {\n      \"x_origin\": 5.0,\n      \"y_origin\": 0.0,\n      \"z_origin\": 0.0\n    },\n    \"Corridor_Zone\": {\n      \"x_origin\": 10.0,\n      \"y_origin\": 0.0,\n      \"z_origin\": 0.0\n    }\n  }\n}\n...\n</pre> In\u00a0[9]: Copied! <pre>import tempfile\nfrom pathlib import Path\n\nfrom idfkit import load_idf\n\n# Write to a temp file, then reload\nwith tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".idf\", delete=False) as f:\n    f.write(idf_string)\n    temp_path = Path(f.name)\n\nreloaded = load_idf(str(temp_path))\nprint(reloaded)\nprint(f\"Zones after reload: {len(reloaded['Zone'])}\")\n\ntemp_path.unlink()  # clean up\n</pre> import tempfile from pathlib import Path  from idfkit import load_idf  # Write to a temp file, then reload with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".idf\", delete=False) as f:     f.write(idf_string)     temp_path = Path(f.name)  reloaded = load_idf(str(temp_path)) print(reloaded) print(f\"Zones after reload: {len(reloaded['Zone'])}\")  temp_path.unlink()  # clean up <pre>IDFDocument(version=24.1.0, objects=3)\n\n  Building: 1 objects\n  Zone: 2 objects\nZones after reload: 2\n</pre> In\u00a0[10]: Copied! <pre># Add a new zone, then remove it\ntemp_zone = model.add(\"Zone\", \"Temporary_Zone\", x_origin=20.0, y_origin=0.0, z_origin=0.0)\nprint(f\"Added zone: {temp_zone.name}\")\nprint(f\"Total zones: {len(model['Zone'])}\")\n\n# Remove it\nmodel.removeidfobject(temp_zone)\nprint(f\"After removal: {len(model['Zone'])} zones\")\n</pre> # Add a new zone, then remove it temp_zone = model.add(\"Zone\", \"Temporary_Zone\", x_origin=20.0, y_origin=0.0, z_origin=0.0) print(f\"Added zone: {temp_zone.name}\") print(f\"Total zones: {len(model['Zone'])}\")  # Remove it model.removeidfobject(temp_zone) print(f\"After removal: {len(model['Zone'])} zones\") <pre>Added zone: Temporary_Zone\nTotal zones: 3\nAfter removal: 2 zones\n</pre> In\u00a0[\u00a0]: Copied! <pre>model = new_document(version=(24, 1, 0))\n\n# Building\nmodel.add(\"Building\", \"Demo Building\", north_axis=0, terrain=\"Suburbs\")\n\n# Global geometry rules\nmodel.add(\n    \"GlobalGeometryRules\",\n    \"\",\n    starting_vertex_position=\"UpperLeftCorner\",\n    vertex_entry_direction=\"Counterclockwise\",\n    coordinate_system=\"Relative\",\n)\n\n# Zone\nmodel.add(\"Zone\", \"Office\", x_origin=0, y_origin=0, z_origin=0, multiplier=1, type=1)\n\n# Material\nmodel.add(\n    \"Material\",\n    \"Concrete_200mm\",\n    roughness=\"MediumRough\",\n    thickness=0.2,\n    conductivity=1.73,\n    density=2243,\n    specific_heat=837,\n    thermal_absorptance=0.9,\n    solar_absorptance=0.65,\n    visible_absorptance=0.65,\n)\nmodel.add(\n    \"Material\",\n    \"Insulation_50mm\",\n    roughness=\"MediumSmooth\",\n    thickness=0.05,\n    conductivity=0.04,\n    density=32,\n    specific_heat=830,\n)\n\n# Construction referencing the materials\nmodel.add(\n    \"Construction\",\n    \"Exterior_Wall\",\n    outside_layer=\"Concrete_200mm\",\n    layer_2=\"Insulation_50mm\",\n)\nmodel.add(\"Construction\", \"Floor_Slab\", outside_layer=\"Concrete_200mm\")\nmodel.add(\"Construction\", \"Flat_Roof\", outside_layer=\"Concrete_200mm\")\n\n# Surfaces (simple 5m x 5m x 3m box)\nmodel.add(\n    \"BuildingSurface:Detailed\",\n    \"Wall_South\",\n    surface_type=\"Wall\",\n    construction_name=\"Exterior_Wall\",\n    zone_name=\"Office\",\n    outside_boundary_condition=\"Outdoors\",\n    sun_exposure=\"SunExposed\",\n    wind_exposure=\"WindExposed\",\n    number_of_vertices=4,\n    vertex_1_x_coordinate=0,\n    vertex_1_y_coordinate=0,\n    vertex_1_z_coordinate=3,\n    vertex_2_x_coordinate=0,\n    vertex_2_y_coordinate=0,\n    vertex_2_z_coordinate=0,\n    vertex_3_x_coordinate=5,\n    vertex_3_y_coordinate=0,\n    vertex_3_z_coordinate=0,\n    vertex_4_x_coordinate=5,\n    vertex_4_y_coordinate=0,\n    vertex_4_z_coordinate=3,\n)\nmodel.add(\n    \"BuildingSurface:Detailed\",\n    \"Wall_East\",\n    surface_type=\"Wall\",\n    construction_name=\"Exterior_Wall\",\n    zone_name=\"Office\",\n    outside_boundary_condition=\"Outdoors\",\n    sun_exposure=\"SunExposed\",\n    wind_exposure=\"WindExposed\",\n    number_of_vertices=4,\n    vertex_1_x_coordinate=5,\n    vertex_1_y_coordinate=0,\n    vertex_1_z_coordinate=3,\n    vertex_2_x_coordinate=5,\n    vertex_2_y_coordinate=0,\n    vertex_2_z_coordinate=0,\n    vertex_3_x_coordinate=5,\n    vertex_3_y_coordinate=5,\n    vertex_3_z_coordinate=0,\n    vertex_4_x_coordinate=5,\n    vertex_4_y_coordinate=5,\n    vertex_4_z_coordinate=3,\n)\nmodel.add(\n    \"BuildingSurface:Detailed\",\n    \"Wall_North\",\n    surface_type=\"Wall\",\n    construction_name=\"Exterior_Wall\",\n    zone_name=\"Office\",\n    outside_boundary_condition=\"Outdoors\",\n    sun_exposure=\"SunExposed\",\n    wind_exposure=\"WindExposed\",\n    number_of_vertices=4,\n    vertex_1_x_coordinate=5,\n    vertex_1_y_coordinate=5,\n    vertex_1_z_coordinate=3,\n    vertex_2_x_coordinate=5,\n    vertex_2_y_coordinate=5,\n    vertex_2_z_coordinate=0,\n    vertex_3_x_coordinate=0,\n    vertex_3_y_coordinate=5,\n    vertex_3_z_coordinate=0,\n    vertex_4_x_coordinate=0,\n    vertex_4_y_coordinate=5,\n    vertex_4_z_coordinate=3,\n)\nmodel.add(\n    \"BuildingSurface:Detailed\",\n    \"Wall_West\",\n    surface_type=\"Wall\",\n    construction_name=\"Exterior_Wall\",\n    zone_name=\"Office\",\n    outside_boundary_condition=\"Outdoors\",\n    sun_exposure=\"SunExposed\",\n    wind_exposure=\"WindExposed\",\n    number_of_vertices=4,\n    vertex_1_x_coordinate=0,\n    vertex_1_y_coordinate=5,\n    vertex_1_z_coordinate=3,\n    vertex_2_x_coordinate=0,\n    vertex_2_y_coordinate=5,\n    vertex_2_z_coordinate=0,\n    vertex_3_x_coordinate=0,\n    vertex_3_y_coordinate=0,\n    vertex_3_z_coordinate=0,\n    vertex_4_x_coordinate=0,\n    vertex_4_y_coordinate=0,\n    vertex_4_z_coordinate=3,\n)\nmodel.add(\n    \"BuildingSurface:Detailed\",\n    \"Floor\",\n    surface_type=\"Floor\",\n    construction_name=\"Floor_Slab\",\n    zone_name=\"Office\",\n    outside_boundary_condition=\"Ground\",\n    number_of_vertices=4,\n    vertex_1_x_coordinate=0,\n    vertex_1_y_coordinate=0,\n    vertex_1_z_coordinate=0,\n    vertex_2_x_coordinate=0,\n    vertex_2_y_coordinate=5,\n    vertex_2_z_coordinate=0,\n    vertex_3_x_coordinate=5,\n    vertex_3_y_coordinate=5,\n    vertex_3_z_coordinate=0,\n    vertex_4_x_coordinate=5,\n    vertex_4_y_coordinate=0,\n    vertex_4_z_coordinate=0,\n)\nmodel.add(\n    \"BuildingSurface:Detailed\",\n    \"Roof\",\n    surface_type=\"Roof\",\n    construction_name=\"Flat_Roof\",\n    zone_name=\"Office\",\n    outside_boundary_condition=\"Outdoors\",\n    sun_exposure=\"SunExposed\",\n    wind_exposure=\"WindExposed\",\n    number_of_vertices=4,\n    vertex_1_x_coordinate=0,\n    vertex_1_y_coordinate=0,\n    vertex_1_z_coordinate=3,\n    vertex_2_x_coordinate=5,\n    vertex_2_y_coordinate=0,\n    vertex_2_z_coordinate=3,\n    vertex_3_x_coordinate=5,\n    vertex_3_y_coordinate=5,\n    vertex_3_z_coordinate=3,\n    vertex_4_x_coordinate=0,\n    vertex_4_y_coordinate=5,\n    vertex_4_z_coordinate=3,\n)\n\n# Schedules\nmodel.add(\"ScheduleTypeLimits\", \"Fraction\", lower_limit_value=0, upper_limit_value=1, numeric_type=\"Continuous\")\nmodel.add(\n    \"ScheduleTypeLimits\", \"Activity_Level\", lower_limit_value=0, upper_limit_value=1000, numeric_type=\"Continuous\"\n)\nmodel.add(\"Schedule:Constant\", \"Always_On\", schedule_type_limits_name=\"Fraction\", hourly_value=1.0)\nmodel.add(\"Schedule:Constant\", \"Activity_Schedule\", schedule_type_limits_name=\"Activity_Level\", hourly_value=120.0)\n\n# People (references Zone and Schedule)\nmodel.add(\n    \"People\",\n    \"Office_People\",\n    zone_or_zonelist_or_space_or_spacelist_name=\"Office\",\n    number_of_people_schedule_name=\"Always_On\",\n    number_of_people_calculation_method=\"People\",\n    number_of_people=10,\n    activity_level_schedule_name=\"Activity_Schedule\",\n)\n\n# Lights\nmodel.add(\n    \"Lights\",\n    \"Office_Lights\",\n    zone_or_zonelist_or_space_or_spacelist_name=\"Office\",\n    schedule_name=\"Always_On\",\n    design_level_calculation_method=\"Watts/Area\",\n    watts_per_floor_area=10.0,\n)\n\nprint(model)\n</pre> model = new_document(version=(24, 1, 0))  # Building model.add(\"Building\", \"Demo Building\", north_axis=0, terrain=\"Suburbs\")  # Global geometry rules model.add(     \"GlobalGeometryRules\",     \"\",     starting_vertex_position=\"UpperLeftCorner\",     vertex_entry_direction=\"Counterclockwise\",     coordinate_system=\"Relative\", )  # Zone model.add(\"Zone\", \"Office\", x_origin=0, y_origin=0, z_origin=0, multiplier=1, type=1)  # Material model.add(     \"Material\",     \"Concrete_200mm\",     roughness=\"MediumRough\",     thickness=0.2,     conductivity=1.73,     density=2243,     specific_heat=837,     thermal_absorptance=0.9,     solar_absorptance=0.65,     visible_absorptance=0.65, ) model.add(     \"Material\",     \"Insulation_50mm\",     roughness=\"MediumSmooth\",     thickness=0.05,     conductivity=0.04,     density=32,     specific_heat=830, )  # Construction referencing the materials model.add(     \"Construction\",     \"Exterior_Wall\",     outside_layer=\"Concrete_200mm\",     layer_2=\"Insulation_50mm\", ) model.add(\"Construction\", \"Floor_Slab\", outside_layer=\"Concrete_200mm\") model.add(\"Construction\", \"Flat_Roof\", outside_layer=\"Concrete_200mm\")  # Surfaces (simple 5m x 5m x 3m box) model.add(     \"BuildingSurface:Detailed\",     \"Wall_South\",     surface_type=\"Wall\",     construction_name=\"Exterior_Wall\",     zone_name=\"Office\",     outside_boundary_condition=\"Outdoors\",     sun_exposure=\"SunExposed\",     wind_exposure=\"WindExposed\",     number_of_vertices=4,     vertex_1_x_coordinate=0,     vertex_1_y_coordinate=0,     vertex_1_z_coordinate=3,     vertex_2_x_coordinate=0,     vertex_2_y_coordinate=0,     vertex_2_z_coordinate=0,     vertex_3_x_coordinate=5,     vertex_3_y_coordinate=0,     vertex_3_z_coordinate=0,     vertex_4_x_coordinate=5,     vertex_4_y_coordinate=0,     vertex_4_z_coordinate=3, ) model.add(     \"BuildingSurface:Detailed\",     \"Wall_East\",     surface_type=\"Wall\",     construction_name=\"Exterior_Wall\",     zone_name=\"Office\",     outside_boundary_condition=\"Outdoors\",     sun_exposure=\"SunExposed\",     wind_exposure=\"WindExposed\",     number_of_vertices=4,     vertex_1_x_coordinate=5,     vertex_1_y_coordinate=0,     vertex_1_z_coordinate=3,     vertex_2_x_coordinate=5,     vertex_2_y_coordinate=0,     vertex_2_z_coordinate=0,     vertex_3_x_coordinate=5,     vertex_3_y_coordinate=5,     vertex_3_z_coordinate=0,     vertex_4_x_coordinate=5,     vertex_4_y_coordinate=5,     vertex_4_z_coordinate=3, ) model.add(     \"BuildingSurface:Detailed\",     \"Wall_North\",     surface_type=\"Wall\",     construction_name=\"Exterior_Wall\",     zone_name=\"Office\",     outside_boundary_condition=\"Outdoors\",     sun_exposure=\"SunExposed\",     wind_exposure=\"WindExposed\",     number_of_vertices=4,     vertex_1_x_coordinate=5,     vertex_1_y_coordinate=5,     vertex_1_z_coordinate=3,     vertex_2_x_coordinate=5,     vertex_2_y_coordinate=5,     vertex_2_z_coordinate=0,     vertex_3_x_coordinate=0,     vertex_3_y_coordinate=5,     vertex_3_z_coordinate=0,     vertex_4_x_coordinate=0,     vertex_4_y_coordinate=5,     vertex_4_z_coordinate=3, ) model.add(     \"BuildingSurface:Detailed\",     \"Wall_West\",     surface_type=\"Wall\",     construction_name=\"Exterior_Wall\",     zone_name=\"Office\",     outside_boundary_condition=\"Outdoors\",     sun_exposure=\"SunExposed\",     wind_exposure=\"WindExposed\",     number_of_vertices=4,     vertex_1_x_coordinate=0,     vertex_1_y_coordinate=5,     vertex_1_z_coordinate=3,     vertex_2_x_coordinate=0,     vertex_2_y_coordinate=5,     vertex_2_z_coordinate=0,     vertex_3_x_coordinate=0,     vertex_3_y_coordinate=0,     vertex_3_z_coordinate=0,     vertex_4_x_coordinate=0,     vertex_4_y_coordinate=0,     vertex_4_z_coordinate=3, ) model.add(     \"BuildingSurface:Detailed\",     \"Floor\",     surface_type=\"Floor\",     construction_name=\"Floor_Slab\",     zone_name=\"Office\",     outside_boundary_condition=\"Ground\",     number_of_vertices=4,     vertex_1_x_coordinate=0,     vertex_1_y_coordinate=0,     vertex_1_z_coordinate=0,     vertex_2_x_coordinate=0,     vertex_2_y_coordinate=5,     vertex_2_z_coordinate=0,     vertex_3_x_coordinate=5,     vertex_3_y_coordinate=5,     vertex_3_z_coordinate=0,     vertex_4_x_coordinate=5,     vertex_4_y_coordinate=0,     vertex_4_z_coordinate=0, ) model.add(     \"BuildingSurface:Detailed\",     \"Roof\",     surface_type=\"Roof\",     construction_name=\"Flat_Roof\",     zone_name=\"Office\",     outside_boundary_condition=\"Outdoors\",     sun_exposure=\"SunExposed\",     wind_exposure=\"WindExposed\",     number_of_vertices=4,     vertex_1_x_coordinate=0,     vertex_1_y_coordinate=0,     vertex_1_z_coordinate=3,     vertex_2_x_coordinate=5,     vertex_2_y_coordinate=0,     vertex_2_z_coordinate=3,     vertex_3_x_coordinate=5,     vertex_3_y_coordinate=5,     vertex_3_z_coordinate=3,     vertex_4_x_coordinate=0,     vertex_4_y_coordinate=5,     vertex_4_z_coordinate=3, )  # Schedules model.add(\"ScheduleTypeLimits\", \"Fraction\", lower_limit_value=0, upper_limit_value=1, numeric_type=\"Continuous\") model.add(     \"ScheduleTypeLimits\", \"Activity_Level\", lower_limit_value=0, upper_limit_value=1000, numeric_type=\"Continuous\" ) model.add(\"Schedule:Constant\", \"Always_On\", schedule_type_limits_name=\"Fraction\", hourly_value=1.0) model.add(\"Schedule:Constant\", \"Activity_Schedule\", schedule_type_limits_name=\"Activity_Level\", hourly_value=120.0)  # People (references Zone and Schedule) model.add(     \"People\",     \"Office_People\",     zone_or_zonelist_or_space_or_spacelist_name=\"Office\",     number_of_people_schedule_name=\"Always_On\",     number_of_people_calculation_method=\"People\",     number_of_people=10,     activity_level_schedule_name=\"Activity_Schedule\", )  # Lights model.add(     \"Lights\",     \"Office_Lights\",     zone_or_zonelist_or_space_or_spacelist_name=\"Office\",     schedule_name=\"Always_On\",     design_level_calculation_method=\"Watts/Area\",     watts_per_floor_area=10.0, )  print(model) In\u00a0[12]: Copied! <pre># What objects reference the \"Office\" zone?\nrefs = model.get_referencing(\"Office\")\nprint(f\"Objects referencing 'Office' ({len(refs)}):\")\nfor obj in sorted(refs, key=lambda o: o.obj_type):\n    print(f\"  {obj.obj_type}: {obj.name}\")\n</pre> # What objects reference the \"Office\" zone? refs = model.get_referencing(\"Office\") print(f\"Objects referencing 'Office' ({len(refs)}):\") for obj in sorted(refs, key=lambda o: o.obj_type):     print(f\"  {obj.obj_type}: {obj.name}\") <pre>Objects referencing 'Office' (8):\n  BuildingSurface:Detailed: Wall_North\n  BuildingSurface:Detailed: Floor\n  BuildingSurface:Detailed: Wall_East\n  BuildingSurface:Detailed: Wall_West\n  BuildingSurface:Detailed: Roof\n  BuildingSurface:Detailed: Wall_South\n  Lights: Office_Lights\n  People: Office_People\n</pre> In\u00a0[13]: Copied! <pre># What objects reference the \"Exterior_Wall\" construction?\nwall_refs = model.get_referencing(\"Exterior_Wall\")\nprint(f\"Surfaces using 'Exterior_Wall' construction ({len(wall_refs)}):\")\nfor obj in wall_refs:\n    print(f\"  {obj.name}\")\n</pre> # What objects reference the \"Exterior_Wall\" construction? wall_refs = model.get_referencing(\"Exterior_Wall\") print(f\"Surfaces using 'Exterior_Wall' construction ({len(wall_refs)}):\") for obj in wall_refs:     print(f\"  {obj.name}\") <pre>Surfaces using 'Exterior_Wall' construction (4):\n  Wall_North\n  Wall_East\n  Wall_West\n  Wall_South\n</pre> In\u00a0[14]: Copied! <pre># Reverse query: what does the People object reference?\npeople_obj = model[\"People\"][\"Office_People\"]\nreferenced_names = model.get_references(people_obj)\nprint(f\"'Office_People' references: {referenced_names}\")\n</pre> # Reverse query: what does the People object reference? people_obj = model[\"People\"][\"Office_People\"] referenced_names = model.get_references(people_obj) print(f\"'Office_People' references: {referenced_names}\") <pre>'Office_People' references: {'ALWAYS_ON', 'OFFICE'}\n</pre> In\u00a0[\u00a0]: Copied! <pre># Demonstrate rename on a separate model so the main model stays clean\nrename_demo = new_document()\nrename_demo.add(\"ScheduleTypeLimits\", \"Fraction\", lower_limit_value=0, upper_limit_value=1, numeric_type=\"Continuous\")\nrename_demo.add(\n    \"ScheduleTypeLimits\", \"Activity_Level\", lower_limit_value=0, upper_limit_value=1000, numeric_type=\"Continuous\"\n)\nrename_demo.add(\"Schedule:Constant\", \"Always_On\", schedule_type_limits_name=\"Fraction\", hourly_value=1.0)\nrename_demo.add(\n    \"Schedule:Constant\", \"Activity_Schedule\", schedule_type_limits_name=\"Activity_Level\", hourly_value=120.0\n)\nrename_demo.add(\"Zone\", \"Kitchen\")\nrename_demo.add(\n    \"People\",\n    \"Kitchen_Staff\",\n    zone_or_zonelist_or_space_or_spacelist_name=\"Kitchen\",\n    number_of_people_schedule_name=\"Always_On\",\n    number_of_people_calculation_method=\"People\",\n    number_of_people=3,\n    activity_level_schedule_name=\"Activity_Schedule\",\n)\nrename_demo.add(\n    \"Lights\",\n    \"Kitchen_Lights\",\n    zone_or_zonelist_or_space_or_spacelist_name=\"Kitchen\",\n    schedule_name=\"Always_On\",\n    design_level_calculation_method=\"Watts/Area\",\n    watts_per_floor_area=12.0,\n)\n\npeople = rename_demo[\"People\"][\"Kitchen_Staff\"]\nlights = rename_demo[\"Lights\"][\"Kitchen_Lights\"]\n\n# --- Method 1: Direct .name assignment ---\nzone = rename_demo[\"Zone\"][\"Kitchen\"]\nprint(\"Before rename:\")\nprint(f\"  People zone ref: {people.zone_or_zonelist_or_space_or_spacelist_name}\")\nprint(f\"  Lights zone ref: {lights.zone_or_zonelist_or_space_or_spacelist_name}\")\n\nzone.name = \"Staff_Kitchen\"\n\nprint(\"\\nAfter zone.name = 'Staff_Kitchen':\")\nprint(f\"  People zone ref: {people.zone_or_zonelist_or_space_or_spacelist_name}\")\nprint(f\"  Lights zone ref: {lights.zone_or_zonelist_or_space_or_spacelist_name}\")\nprint(f\"  Lookup by new name: {'Staff_Kitchen' in rename_demo['Zone']}\")\nprint(f\"  Lookup by old name: {'Kitchen' in rename_demo['Zone']}\")\n\n# --- Method 2: model.rename() (equivalent result) ---\nrename_demo.rename(\"Zone\", \"Staff_Kitchen\", \"Main_Kitchen\")\n\nprint(\"\\nAfter model.rename('Zone', 'Staff_Kitchen', 'Main_Kitchen'):\")\nprint(f\"  People zone ref: {people.zone_or_zonelist_or_space_or_spacelist_name}\")\nprint(f\"  Lights zone ref: {lights.zone_or_zonelist_or_space_or_spacelist_name}\")\n</pre> # Demonstrate rename on a separate model so the main model stays clean rename_demo = new_document() rename_demo.add(\"ScheduleTypeLimits\", \"Fraction\", lower_limit_value=0, upper_limit_value=1, numeric_type=\"Continuous\") rename_demo.add(     \"ScheduleTypeLimits\", \"Activity_Level\", lower_limit_value=0, upper_limit_value=1000, numeric_type=\"Continuous\" ) rename_demo.add(\"Schedule:Constant\", \"Always_On\", schedule_type_limits_name=\"Fraction\", hourly_value=1.0) rename_demo.add(     \"Schedule:Constant\", \"Activity_Schedule\", schedule_type_limits_name=\"Activity_Level\", hourly_value=120.0 ) rename_demo.add(\"Zone\", \"Kitchen\") rename_demo.add(     \"People\",     \"Kitchen_Staff\",     zone_or_zonelist_or_space_or_spacelist_name=\"Kitchen\",     number_of_people_schedule_name=\"Always_On\",     number_of_people_calculation_method=\"People\",     number_of_people=3,     activity_level_schedule_name=\"Activity_Schedule\", ) rename_demo.add(     \"Lights\",     \"Kitchen_Lights\",     zone_or_zonelist_or_space_or_spacelist_name=\"Kitchen\",     schedule_name=\"Always_On\",     design_level_calculation_method=\"Watts/Area\",     watts_per_floor_area=12.0, )  people = rename_demo[\"People\"][\"Kitchen_Staff\"] lights = rename_demo[\"Lights\"][\"Kitchen_Lights\"]  # --- Method 1: Direct .name assignment --- zone = rename_demo[\"Zone\"][\"Kitchen\"] print(\"Before rename:\") print(f\"  People zone ref: {people.zone_or_zonelist_or_space_or_spacelist_name}\") print(f\"  Lights zone ref: {lights.zone_or_zonelist_or_space_or_spacelist_name}\")  zone.name = \"Staff_Kitchen\"  print(\"\\nAfter zone.name = 'Staff_Kitchen':\") print(f\"  People zone ref: {people.zone_or_zonelist_or_space_or_spacelist_name}\") print(f\"  Lights zone ref: {lights.zone_or_zonelist_or_space_or_spacelist_name}\") print(f\"  Lookup by new name: {'Staff_Kitchen' in rename_demo['Zone']}\") print(f\"  Lookup by old name: {'Kitchen' in rename_demo['Zone']}\")  # --- Method 2: model.rename() (equivalent result) --- rename_demo.rename(\"Zone\", \"Staff_Kitchen\", \"Main_Kitchen\")  print(\"\\nAfter model.rename('Zone', 'Staff_Kitchen', 'Main_Kitchen'):\") print(f\"  People zone ref: {people.zone_or_zonelist_or_space_or_spacelist_name}\") print(f\"  Lights zone ref: {lights.zone_or_zonelist_or_space_or_spacelist_name}\") In\u00a0[16]: Copied! <pre># Filter walls only\nwalls = model[\"BuildingSurface:Detailed\"].filter(lambda s: getattr(s, \"surface_type\", \"\") == \"Wall\")\nprint(f\"Walls: {[w.name for w in walls]}\")\n\n# Built-in surface type filter\nroofs = model.getsurfaces(\"roof\")\nprint(f\"Roofs: {[r.name for r in roofs]}\")\n\n# Get all surfaces belonging to a zone via reference graph\nzone_surfaces = model.get_zone_surfaces(\"Office\")\nprint(f\"All objects referencing 'Office': {len(zone_surfaces)}\")\n</pre> # Filter walls only walls = model[\"BuildingSurface:Detailed\"].filter(lambda s: getattr(s, \"surface_type\", \"\") == \"Wall\") print(f\"Walls: {[w.name for w in walls]}\")  # Built-in surface type filter roofs = model.getsurfaces(\"roof\") print(f\"Roofs: {[r.name for r in roofs]}\")  # Get all surfaces belonging to a zone via reference graph zone_surfaces = model.get_zone_surfaces(\"Office\") print(f\"All objects referencing 'Office': {len(zone_surfaces)}\") <pre>Walls: ['Wall_South', 'Wall_East', 'Wall_North', 'Wall_West']\nRoofs: ['Roof']\nAll objects referencing 'Office': 8\n</pre> In\u00a0[17]: Copied! <pre># Look up a schedule by name\nsched = model.get_schedule(\"Always_On\")\nprint(f\"Schedule: {sched.name} (type: {sched.obj_type})\")\n\n# Add an unused schedule\nmodel.add(\"Schedule:Constant\", \"Unused_Schedule\", schedule_type_limits_name=\"Fraction\", hourly_value=0.0)\n\n# Detect which schedules are actually referenced\nused = model.get_used_schedules()\nprint(f\"Used schedules: {used}\")\n\nall_schedules = set(model.schedules_dict.keys())\nunused = all_schedules - used\nprint(f\"Unused schedules: {unused}\")\n</pre> # Look up a schedule by name sched = model.get_schedule(\"Always_On\") print(f\"Schedule: {sched.name} (type: {sched.obj_type})\")  # Add an unused schedule model.add(\"Schedule:Constant\", \"Unused_Schedule\", schedule_type_limits_name=\"Fraction\", hourly_value=0.0)  # Detect which schedules are actually referenced used = model.get_used_schedules() print(f\"Used schedules: {used}\")  all_schedules = set(model.schedules_dict.keys()) unused = all_schedules - used print(f\"Unused schedules: {unused}\") <pre>Schedule: Always_On (type: Schedule:Constant)\nUsed schedules: {'ALWAYS_ON'}\nUnused schedules: {'UNUSED_SCHEDULE'}\n</pre> In\u00a0[\u00a0]: Copied! <pre>from idfkit import validate_document\n\nresult = validate_document(model)\nprint(result)\nprint(f\"\\nValid: {result.is_valid}\")\nprint(f\"Total issues: {result.total_issues}\")\nprint(f\"Errors: {len(result.errors)}, Warnings: {len(result.warnings)}, Info: {len(result.info)}\")\n</pre> from idfkit import validate_document  result = validate_document(model) print(result) print(f\"\\nValid: {result.is_valid}\") print(f\"Total issues: {result.total_issues}\") print(f\"Errors: {len(result.errors)}, Warnings: {len(result.warnings)}, Info: {len(result.info)}\") In\u00a0[19]: Copied! <pre># Show first few issues if any exist\nfor issue in result.errors[:5]:\n    print(issue)\nfor issue in result.warnings[:5]:\n    print(issue)\n</pre> # Show first few issues if any exist for issue in result.errors[:5]:     print(issue) for issue in result.warnings[:5]:     print(issue) <pre>[ERROR] People:'Office_People'.activity_level_schedule_name: Required field 'activity_level_schedule_name' is missing\n</pre> In\u00a0[20]: Copied! <pre># Validate only specific object types\nzone_result = validate_document(model, object_types=[\"Zone\"])\nprint(f\"Zone validation: {zone_result.total_issues} issues\")\n</pre> # Validate only specific object types zone_result = validate_document(model, object_types=[\"Zone\"]) print(f\"Zone validation: {zone_result.total_issues} issues\") <pre>Zone validation: 0 issues\n</pre> In\u00a0[21]: Copied! <pre># Compare the two formats for the same Zone object\nidf_out = write_idf(model)\nepjson_out = write_epjson(model)\n\nprint(f\"IDF size  : {len(idf_out):,} characters\")\nprint(f\"epJSON size: {len(epjson_out):,} characters\")\n</pre> # Compare the two formats for the same Zone object idf_out = write_idf(model) epjson_out = write_epjson(model)  print(f\"IDF size  : {len(idf_out):,} characters\") print(f\"epJSON size: {len(epjson_out):,} characters\") <pre>IDF size  : 10,712 characters\nepJSON size: 6,183 characters\n</pre> In\u00a0[22]: Copied! <pre># Create a variant with thicker insulation\nvariant = model.copy()\n\ninsulation = variant[\"Material\"][\"Insulation_50mm\"]\ninsulation.thickness = 0.10  # 100mm instead of 50mm\n\n# Original is unchanged\noriginal_thickness = model[\"Material\"][\"Insulation_50mm\"].thickness\nvariant_thickness = variant[\"Material\"][\"Insulation_50mm\"].thickness\nprint(f\"Original thickness: {original_thickness} m\")\nprint(f\"Variant thickness : {variant_thickness} m\")\n</pre> # Create a variant with thicker insulation variant = model.copy()  insulation = variant[\"Material\"][\"Insulation_50mm\"] insulation.thickness = 0.10  # 100mm instead of 50mm  # Original is unchanged original_thickness = model[\"Material\"][\"Insulation_50mm\"].thickness variant_thickness = variant[\"Material\"][\"Insulation_50mm\"].thickness print(f\"Original thickness: {original_thickness} m\") print(f\"Variant thickness : {variant_thickness} m\") <pre>Original thickness: 0.05 m\nVariant thickness : 0.1 m\n</pre> In\u00a0[23]: Copied! <pre>from idfkit import Polygon3D, Vector3D\n\n# Vector arithmetic\nv1 = Vector3D(1.0, 0.0, 0.0)\nv2 = Vector3D(0.0, 1.0, 0.0)\n\nprint(f\"v1 + v2     = {v1 + v2}\")\nprint(f\"v1 x v2     = {v1.cross(v2)}\")\nprint(f\"v1 . v2     = {v1.dot(v2)}\")\nprint(f\"|v1 + v2|   = {(v1 + v2).length():.4f}\")\nprint(f\"normalize   = {(v1 + v2).normalize()}\")\nprint(f\"rotate 90\u00b0  = {v1.rotate_z(90)}\")\n</pre> from idfkit import Polygon3D, Vector3D  # Vector arithmetic v1 = Vector3D(1.0, 0.0, 0.0) v2 = Vector3D(0.0, 1.0, 0.0)  print(f\"v1 + v2     = {v1 + v2}\") print(f\"v1 x v2     = {v1.cross(v2)}\") print(f\"v1 . v2     = {v1.dot(v2)}\") print(f\"|v1 + v2|   = {(v1 + v2).length():.4f}\") print(f\"normalize   = {(v1 + v2).normalize()}\") print(f\"rotate 90\u00b0  = {v1.rotate_z(90)}\") <pre>v1 + v2     = Vector3D(x=1.0, y=1.0, z=0.0)\nv1 x v2     = Vector3D(x=0.0, y=0.0, z=1.0)\nv1 . v2     = 0.0\n|v1 + v2|   = 1.4142\nnormalize   = Vector3D(x=0.7071067811865475, y=0.7071067811865475, z=0.0)\nrotate 90\u00b0  = Vector3D(x=6.123233995736766e-17, y=1.0, z=0.0)\n</pre> In\u00a0[24]: Copied! <pre># Create a polygon from tuples\nfloor_polygon = Polygon3D.from_tuples([\n    (0, 0, 0),\n    (10, 0, 0),\n    (10, 8, 0),\n    (0, 8, 0),\n])\n\nprint(f\"Area       : {floor_polygon.area:.1f} m\u00b2\")\nprint(f\"Centroid   : {floor_polygon.centroid}\")\nprint(f\"Normal     : {floor_polygon.normal}\")\nprint(f\"Horizontal?: {floor_polygon.is_horizontal}\")\nprint(f\"Vertical?  : {floor_polygon.is_vertical}\")\n</pre> # Create a polygon from tuples floor_polygon = Polygon3D.from_tuples([     (0, 0, 0),     (10, 0, 0),     (10, 8, 0),     (0, 8, 0), ])  print(f\"Area       : {floor_polygon.area:.1f} m\u00b2\") print(f\"Centroid   : {floor_polygon.centroid}\") print(f\"Normal     : {floor_polygon.normal}\") print(f\"Horizontal?: {floor_polygon.is_horizontal}\") print(f\"Vertical?  : {floor_polygon.is_vertical}\") <pre>Area       : 80.0 m\u00b2\nCentroid   : Vector3D(x=5.0, y=4.0, z=0.0)\nNormal     : Vector3D(x=0.0, y=0.0, z=1.0)\nHorizontal?: True\nVertical?  : False\n</pre> In\u00a0[25]: Copied! <pre>from idfkit.geometry import (\n    calculate_surface_area,\n    calculate_zone_floor_area,\n    calculate_zone_volume,\n    get_surface_coords,\n)\n\nfor surface in model[\"BuildingSurface:Detailed\"]:\n    area = calculate_surface_area(surface)\n    coords = get_surface_coords(surface)\n    stype = surface.surface_type\n    print(f\"  {surface.name:12s}  type={stype:5s}  area={area:6.1f} m\u00b2  vertices={coords.num_vertices}\")\n</pre> from idfkit.geometry import (     calculate_surface_area,     calculate_zone_floor_area,     calculate_zone_volume,     get_surface_coords, )  for surface in model[\"BuildingSurface:Detailed\"]:     area = calculate_surface_area(surface)     coords = get_surface_coords(surface)     stype = surface.surface_type     print(f\"  {surface.name:12s}  type={stype:5s}  area={area:6.1f} m\u00b2  vertices={coords.num_vertices}\") <pre>  Wall_South    type=Wall   area=  15.0 m\u00b2  vertices=4\n  Wall_East     type=Wall   area=  15.0 m\u00b2  vertices=4\n  Wall_North    type=Wall   area=  15.0 m\u00b2  vertices=4\n  Wall_West     type=Wall   area=  15.0 m\u00b2  vertices=4\n  Floor         type=Floor  area=  25.0 m\u00b2  vertices=4\n  Roof          type=Roof   area=  25.0 m\u00b2  vertices=4\n</pre> In\u00a0[26]: Copied! <pre># Zone-level geometry aggregates\nfloor_area = calculate_zone_floor_area(model, \"Office\")\nvolume = calculate_zone_volume(model, \"Office\")\n\nprint(f\"Office zone floor area: {floor_area:.1f} m\u00b2\")\nprint(f\"Office zone volume    : {volume:.1f} m\u00b3\")\n</pre> # Zone-level geometry aggregates floor_area = calculate_zone_floor_area(model, \"Office\") volume = calculate_zone_volume(model, \"Office\")  print(f\"Office zone floor area: {floor_area:.1f} m\u00b2\") print(f\"Office zone volume    : {volume:.1f} m\u00b3\") <pre>Office zone floor area: 25.0 m\u00b2\nOffice zone volume    : 75.0 m\u00b3\n</pre> In\u00a0[27]: Copied! <pre>wall_south = model[\"BuildingSurface:Detailed\"][\"Wall_South\"]\npoly = get_surface_coords(wall_south)\n\nprint(f\"Original centroid: {poly.centroid}\")\n\n# Translate by (10, 20, 0)\nshifted = poly.translate(Vector3D(10, 20, 0))\nprint(f\"Shifted centroid : {shifted.centroid}\")\n\n# Rotate 45\u00b0 around the centroid\nrotated = poly.rotate_z(45)\nprint(f\"Rotated centroid : {rotated.centroid}\")\nprint(\"Rotated vertices :\")\nfor v in rotated.vertices:\n    print(f\"  ({v.x:.2f}, {v.y:.2f}, {v.z:.2f})\")\n</pre> wall_south = model[\"BuildingSurface:Detailed\"][\"Wall_South\"] poly = get_surface_coords(wall_south)  print(f\"Original centroid: {poly.centroid}\")  # Translate by (10, 20, 0) shifted = poly.translate(Vector3D(10, 20, 0)) print(f\"Shifted centroid : {shifted.centroid}\")  # Rotate 45\u00b0 around the centroid rotated = poly.rotate_z(45) print(f\"Rotated centroid : {rotated.centroid}\") print(\"Rotated vertices :\") for v in rotated.vertices:     print(f\"  ({v.x:.2f}, {v.y:.2f}, {v.z:.2f})\") <pre>Original centroid: Vector3D(x=2.5, y=0.0, z=1.5)\nShifted centroid : Vector3D(x=12.5, y=20.0, z=1.5)\nRotated centroid : Vector3D(x=2.5, y=0.0, z=1.5)\nRotated vertices :\n  (0.73, -1.77, 3.00)\n  (0.73, -1.77, 0.00)\n  (4.27, 1.77, 0.00)\n  (4.27, 1.77, 3.00)\n</pre> In\u00a0[28]: Copied! <pre>schema = model.schema\n\n# How many object types does EnergyPlus 24.1 define?\nprint(f\"Total object types in schema: {len(schema)}\")\nprint(f\"First 10: {schema.object_types[:10]}\")\n</pre> schema = model.schema  # How many object types does EnergyPlus 24.1 define? print(f\"Total object types in schema: {len(schema)}\") print(f\"First 10: {schema.object_types[:10]}\") <pre>Total object types in schema: 847</pre> <pre>\nFirst 10: ['Version', 'SimulationControl', 'PerformancePrecisionTradeoffs', 'Building', 'ShadowCalculation', 'SurfaceConvectionAlgorithm:Inside', 'SurfaceConvectionAlgorithm:Outside', 'HeatBalanceAlgorithm', 'HeatBalanceSettings:ConductionFiniteDifference', 'ZoneAirHeatBalanceAlgorithm']\n</pre> In\u00a0[29]: Copied! <pre># Inspect the Zone object type\nobj_type = \"Zone\"\nfield_names = schema.get_field_names(obj_type)\nrequired = schema.get_required_fields(obj_type)\nmemo = schema.get_object_memo(obj_type)\n\nprint(f\"--- {obj_type} ---\")\nprint(f\"Memo: {memo}\")\nprint(f\"Fields ({len(field_names)}):\")\nfor fname in field_names:\n    ftype = schema.get_field_type(obj_type, fname)\n    default = schema.get_field_default(obj_type, fname)\n    is_ref = schema.is_reference_field(obj_type, fname)\n    req = \"*\" if fname in required else \" \"\n    print(f\"  {req} {fname:40s} type={ftype!s:8s} default={default!s:12s} ref={is_ref}\")\n</pre> # Inspect the Zone object type obj_type = \"Zone\" field_names = schema.get_field_names(obj_type) required = schema.get_required_fields(obj_type) memo = schema.get_object_memo(obj_type)  print(f\"--- {obj_type} ---\") print(f\"Memo: {memo}\") print(f\"Fields ({len(field_names)}):\") for fname in field_names:     ftype = schema.get_field_type(obj_type, fname)     default = schema.get_field_default(obj_type, fname)     is_ref = schema.is_reference_field(obj_type, fname)     req = \"*\" if fname in required else \" \"     print(f\"  {req} {fname:40s} type={ftype!s:8s} default={default!s:12s} ref={is_ref}\") <pre>--- Zone ---\nMemo: Defines a thermal zone of the building. Every zone contains one or more Spaces. Space is an optional input. If a Zone has no Space(s) specified in input then a default Space named &lt;Zone Name&gt; will be created. If some surfaces in a Zone are assigned to a space and some are not, then a default Space named &lt;Zone Name&gt;-Remainder will be created. Input references to Space Names must have a matching Space object (default space names may not be referenced except in output variable keys).\nFields (12):\n    direction_of_relative_north              type=number   default=0.0          ref=False\n    x_origin                                 type=number   default=0.0          ref=False\n    y_origin                                 type=number   default=0.0          ref=False\n    z_origin                                 type=number   default=0.0          ref=False\n    type                                     type=integer  default=1            ref=False\n    multiplier                               type=integer  default=1            ref=False\n    ceiling_height                           type=number   default=Autocalculate ref=False\n    volume                                   type=number   default=Autocalculate ref=False\n    floor_area                               type=number   default=Autocalculate ref=False\n    zone_inside_convection_algorithm         type=string   default=None         ref=False\n    zone_outside_convection_algorithm        type=string   default=None         ref=False\n    part_of_total_floor_area                 type=string   default=Yes          ref=False\n</pre> In\u00a0[30]: Copied! <pre># Check extensible types (e.g., Construction has a variable number of layers)\nprint(f\"Construction extensible? {schema.is_extensible('Construction')}\")\nprint(f\"Construction extensible size: {schema.get_extensible_size('Construction')}\")\nprint(f\"Zone extensible? {schema.is_extensible('Zone')}\")\n</pre> # Check extensible types (e.g., Construction has a variable number of layers) print(f\"Construction extensible? {schema.is_extensible('Construction')}\") print(f\"Construction extensible size: {schema.get_extensible_size('Construction')}\") print(f\"Zone extensible? {schema.is_extensible('Zone')}\") <pre>Construction extensible? False\nConstruction extensible size: None\nZone extensible? False\n</pre> In\u00a0[31]: Copied! <pre># Reference graph stats\nstats = model.references.stats()\nfor key, value in stats.items():\n    print(f\"  {key}: {value}\")\n</pre> # Reference graph stats stats = model.references.stats() for key, value in stats.items():     print(f\"  {key}: {value}\") <pre>  total_references: 22\n  objects_with_references: 13\n  names_referenced: 8\n  object_lists: 0\n</pre> In\u00a0[\u00a0]: Copied! <pre># Introduce a dangling reference on purpose\nbad_model = model.copy()\nbad_model.add(\n    \"People\",\n    \"Ghost_People\",\n    zone_or_zonelist_or_space_or_spacelist_name=\"NonExistent_Zone\",\n    number_of_people_schedule_name=\"NonExistent_Schedule\",\n    number_of_people_calculation_method=\"People\",\n    number_of_people=5,\n    activity_level_schedule_name=\"NonExistent_Activity\",\n    validate=False,  # Skip validation on add so we can demonstrate it below\n)\n\n# Validate to find the dangling references\nbad_result = validate_document(bad_model)\nprint(f\"Valid? {bad_result.is_valid}\")\nfor err in bad_result.errors:\n    if \"non-existent\" in err.message.lower() or \"reference\" in err.message.lower():\n        print(f\"  {err}\")\n</pre> # Introduce a dangling reference on purpose bad_model = model.copy() bad_model.add(     \"People\",     \"Ghost_People\",     zone_or_zonelist_or_space_or_spacelist_name=\"NonExistent_Zone\",     number_of_people_schedule_name=\"NonExistent_Schedule\",     number_of_people_calculation_method=\"People\",     number_of_people=5,     activity_level_schedule_name=\"NonExistent_Activity\",     validate=False,  # Skip validation on add so we can demonstrate it below )  # Validate to find the dangling references bad_result = validate_document(bad_model) print(f\"Valid? {bad_result.is_valid}\") for err in bad_result.errors:     if \"non-existent\" in err.message.lower() or \"reference\" in err.message.lower():         print(f\"  {err}\") In\u00a0[33]: Copied! <pre># What object lists does the People \"zone_or_zonelist_or_space_or_spacelist_name\" field accept?\nobj_list = schema.get_field_object_list(\"People\", \"zone_or_zonelist_or_space_or_spacelist_name\")\nprint(f\"People zone field accepts: {obj_list}\")\n\n# What object types provide names for the first list?\nif obj_list:\n    providers = schema.get_types_providing_reference(obj_list[0])\n    print(f\"Types providing '{obj_list[0]}': {providers[:10]}\")\n</pre> # What object lists does the People \"zone_or_zonelist_or_space_or_spacelist_name\" field accept? obj_list = schema.get_field_object_list(\"People\", \"zone_or_zonelist_or_space_or_spacelist_name\") print(f\"People zone field accepts: {obj_list}\")  # What object types provide names for the first list? if obj_list:     providers = schema.get_types_providing_reference(obj_list[0])     print(f\"Types providing '{obj_list[0]}': {providers[:10]}\") <pre>People zone field accepts: ['SpaceAndSpaceListNames', 'ZoneAndZoneListNames']\nTypes providing 'SpaceAndSpaceListNames': ['Space', 'SpaceList']\n</pre> In\u00a0[34]: Copied! <pre># Single object to dict\nzone_dict = model[\"Zone\"][\"Office\"].to_dict()\nprint(\"Zone as dict:\")\nfor k, v in zone_dict.items():\n    print(f\"  {k}: {v}\")\n\n# Entire collection to list of dicts\nmaterials_list = model[\"Material\"].to_dict()\nprint(f\"\\nMaterials ({len(materials_list)} items):\")\nfor m in materials_list:\n    print(f\"  {m['name']} \u2014 thickness={m.get('thickness')} m\")\n</pre> # Single object to dict zone_dict = model[\"Zone\"][\"Office\"].to_dict() print(\"Zone as dict:\") for k, v in zone_dict.items():     print(f\"  {k}: {v}\")  # Entire collection to list of dicts materials_list = model[\"Material\"].to_dict() print(f\"\\nMaterials ({len(materials_list)} items):\") for m in materials_list:     print(f\"  {m['name']} \u2014 thickness={m.get('thickness')} m\") <pre>Zone as dict:</pre> <pre>\n  name: Office\n  x_origin: 0\n  y_origin: 0\n  z_origin: 0\n  multiplier: 1\n  type: 1\n\nMaterials (2 items):\n  Concrete_200mm \u2014 thickness=0.2 m\n  Insulation_50mm \u2014 thickness=0.05 m\n</pre> In\u00a0[35]: Copied! <pre>from idfkit import get_schema_manager\n\nmanager = get_schema_manager()\nversions = manager.get_available_versions()\nprint(f\"Available schema versions: {versions}\")\n</pre> from idfkit import get_schema_manager  manager = get_schema_manager() versions = manager.get_available_versions() print(f\"Available schema versions: {versions}\") <pre>Available schema versions: [(8, 9, 0), (9, 0, 1), (9, 1, 0), (9, 2, 0), (9, 3, 0), (9, 4, 0), (9, 5, 0), (9, 6, 0), (22, 1, 0), (22, 2, 0), (23, 1, 0), (23, 2, 0), (24, 1, 0), (24, 2, 0), (25, 1, 0), (25, 2, 0)]\n</pre>"},{"location":"getting-started/core-tutorial/#core-tutorial","title":"Core Tutorial\u00b6","text":"<p>This interactive notebook walks you through the core features of idfkit, a fast and modern Python toolkit for parsing, creating, and manipulating EnergyPlus IDF and epJSON files. It is organized into three parts:</p> Part What you will learn Basic Creating documents, adding objects, reading/writing files Advanced Reference tracking, validation, format conversion, schedule management Expert 3D geometry, schema introspection, coordinate transforms, reference graph analysis <p>For Weather and Simulation features, see:</p> <ul> <li>Weather Guide</li> <li>Simulation Guide</li> </ul>"},{"location":"getting-started/core-tutorial/#setup","title":"Setup\u00b6","text":"<p>Make sure idfkit is installed. If you are working inside the repository you can run:</p> <pre>uv sync\n</pre>"},{"location":"getting-started/core-tutorial/#part-1-basic-usage","title":"Part 1 \u2014 Basic Usage\u00b6","text":"<p>This section covers the everyday operations that every idfkit user needs to know: creating a new EnergyPlus document from scratch, adding objects, reading their fields, and writing the result to IDF or epJSON files.</p>"},{"location":"getting-started/core-tutorial/#11-creating-a-new-document","title":"1.1 Creating a new document\u00b6","text":"<p>Use <code>new_document()</code> to create an empty EnergyPlus model. You can specify the target EnergyPlus version as a tuple.</p>"},{"location":"getting-started/core-tutorial/#12-adding-objects","title":"1.2 Adding objects\u00b6","text":"<p>Every EnergyPlus object has a type (e.g. <code>\"Zone\"</code>) and a name. Field data can be passed as a dictionary, as keyword arguments, or both.</p>"},{"location":"getting-started/core-tutorial/#13-accessing-collections-and-objects","title":"1.3 Accessing collections and objects\u00b6","text":"<p>Objects are organized into collections by type. You can access them in two ways:</p> <ul> <li>Bracket notation: <code>model[\"Zone\"]</code> \u2014 uses the exact EnergyPlus type name.</li> <li>Attribute notation: <code>model.zones</code> \u2014 uses a Python-friendly alias.</li> </ul>"},{"location":"getting-started/core-tutorial/#14-reading-and-writing-object-fields","title":"1.4 Reading and writing object fields\u00b6","text":"<p>Fields are accessed as Python attributes using snake_case names.</p>"},{"location":"getting-started/core-tutorial/#15-iterating-over-objects","title":"1.5 Iterating over objects\u00b6","text":"<p>Collections are iterable. You can also iterate over the entire document.</p>"},{"location":"getting-started/core-tutorial/#16-writing-to-idf-and-epjson-formats","title":"1.6 Writing to IDF and epJSON formats\u00b6","text":"<p>Call <code>write_idf()</code> or <code>write_epjson()</code> with no file path to get a string, or pass a path to write directly to disk.</p>"},{"location":"getting-started/core-tutorial/#17-loading-an-existing-idf-file","title":"1.7 Loading an existing IDF file\u00b6","text":"<p>Use <code>load_idf()</code> to parse an existing <code>.idf</code> file. Here we write our model to a temporary file and then reload it to demonstrate the round-trip.</p>"},{"location":"getting-started/core-tutorial/#18-removing-and-copying-objects","title":"1.8 Removing and copying objects\u00b6","text":""},{"location":"getting-started/core-tutorial/#part-2-advanced-usage","title":"Part 2 \u2014 Advanced Usage\u00b6","text":"<p>This section covers reference tracking, document validation, format conversion, filtering, and schedule management.</p>"},{"location":"getting-started/core-tutorial/#21-building-a-realistic-model","title":"2.1 Building a realistic model\u00b6","text":"<p>Let's build a more complete model so we can demonstrate reference tracking and validation. We will add materials, constructions, surfaces, schedules, and internal loads.</p>"},{"location":"getting-started/core-tutorial/#22-reference-tracking","title":"2.2 Reference tracking\u00b6","text":"<p>idfkit automatically builds a reference graph as objects are added. This lets you instantly find every object that refers to a given name.</p>"},{"location":"getting-started/core-tutorial/#23-renaming-with-automatic-reference-updates","title":"2.3 Renaming with automatic reference updates\u00b6","text":"<p>When you rename an object, idfkit updates every field across the document that pointed to the old name. There are two ways to rename:</p> <ol> <li>Direct assignment \u2014 set <code>.name</code> on the object itself.</li> <li><code>model.rename()</code> \u2014 look up the object by type and old name, then rename it.</li> </ol> <p>Both approaches trigger the same update pipeline: the collection index, the reference graph, and every referencing field are kept in sync automatically.</p>"},{"location":"getting-started/core-tutorial/#24-filtering-collections","title":"2.4 Filtering collections\u00b6","text":"<p>Use the <code>filter()</code> method on a collection to select objects matching a predicate, or use <code>getsurfaces()</code> for built-in surface type filtering.</p>"},{"location":"getting-started/core-tutorial/#25-schedule-management","title":"2.5 Schedule management\u00b6","text":"<p>Schedules are central to EnergyPlus models. idfkit provides cached lookup and can detect which schedules are actually used in the model.</p>"},{"location":"getting-started/core-tutorial/#26-validation","title":"2.6 Validation\u00b6","text":"<p><code>validate_document()</code> checks the model against the EpJSON schema: required fields, field types, numeric ranges, and reference integrity.</p>"},{"location":"getting-started/core-tutorial/#27-format-conversion","title":"2.7 Format conversion\u00b6","text":"<p>idfkit can write the same document to both IDF (text) and epJSON (JSON) formats. Conversion helpers are also available for file-to-file transforms.</p>"},{"location":"getting-started/core-tutorial/#28-deep-copy-for-variant-analysis","title":"2.8 Deep copy for variant analysis\u00b6","text":"<p>Use <code>model.copy()</code> to create an independent clone. Changes to the copy will not affect the original.</p>"},{"location":"getting-started/core-tutorial/#part-3-expert-usage","title":"Part 3 \u2014 Expert Usage\u00b6","text":"<p>This section covers 3D geometry operations, schema introspection, coordinate transforms, and reference graph analysis.</p>"},{"location":"getting-started/core-tutorial/#31-3d-geometry-vectors-and-polygons","title":"3.1 3D geometry \u2014 vectors and polygons\u00b6","text":"<p>The <code>Vector3D</code> and <code>Polygon3D</code> classes provide basic 3D math without any external geometry dependency.</p>"},{"location":"getting-started/core-tutorial/#32-extracting-surface-geometry-from-the-model","title":"3.2 Extracting surface geometry from the model\u00b6","text":"<p>Use <code>get_surface_coords()</code> to read vertex data from any surface object and <code>calculate_surface_area()</code> to compute its area.</p>"},{"location":"getting-started/core-tutorial/#33-polygon-transforms","title":"3.3 Polygon transforms\u00b6","text":"<p>Polygons can be translated and rotated.</p>"},{"location":"getting-started/core-tutorial/#34-schema-introspection","title":"3.4 Schema introspection\u00b6","text":"<p>The <code>EpJSONSchema</code> class lets you programmatically discover what fields an object type supports, which ones are required, what their types and defaults are, and which fields are references to other objects.</p>"},{"location":"getting-started/core-tutorial/#35-reference-graph-statistics-and-dangling-reference-detection","title":"3.5 Reference graph statistics and dangling reference detection\u00b6","text":"<p>The <code>ReferenceGraph</code> powering <code>get_referencing()</code> and <code>get_references()</code> can also report statistics and detect broken references.</p>"},{"location":"getting-started/core-tutorial/#36-object-field-metadata-via-schema","title":"3.6 Object field metadata via schema\u00b6","text":"<p>You can retrieve schema-level metadata for individual fields of an object, which is useful for building editors or automated reporting.</p>"},{"location":"getting-started/core-tutorial/#37-working-with-to_dict-for-data-analysis","title":"3.7 Working with <code>to_dict()</code> for data analysis\u00b6","text":"<p>Convert objects or collections to plain dictionaries and lists for integration with pandas, JSON APIs, or any other tooling.</p>"},{"location":"getting-started/core-tutorial/#38-available-schema-versions","title":"3.8 Available schema versions\u00b6","text":"<p>The <code>SchemaManager</code> can detect which EnergyPlus schema versions are available \u2014 either bundled with idfkit or from local EnergyPlus installations.</p>"},{"location":"getting-started/core-tutorial/#summary","title":"Summary\u00b6","text":"Level Feature Key API Basic Create documents <code>new_document()</code> Basic Add / remove objects <code>model.add()</code>, <code>model.removeidfobject()</code> Basic Access fields <code>obj.field_name</code>, <code>obj[\"field\"]</code> Basic Read / write files <code>load_idf()</code>, <code>write_idf()</code>, <code>write_epjson()</code> Advanced Reference tracking <code>model.get_referencing()</code>, <code>model.get_references()</code> Advanced Rename with updates <code>obj.name = ...</code>, <code>model.rename()</code> Advanced Validation <code>validate_document()</code> Advanced Schedule management <code>model.get_schedule()</code>, <code>model.get_used_schedules()</code> Advanced Deep copy <code>model.copy()</code> Expert 3D geometry <code>Vector3D</code>, <code>Polygon3D</code> Expert Surface area / volume <code>calculate_surface_area()</code>, <code>calculate_zone_volume()</code> Expert Schema introspection <code>schema.get_field_names()</code>, <code>schema.get_field_type()</code> Expert Reference graph <code>model.references.stats()</code>, dangling ref detection"},{"location":"getting-started/core-tutorial/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Weather Guide \u2014 Weather station search and design days</li> <li>Simulation Guide \u2014 Running EnergyPlus simulations</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>idfkit is available on PyPI and can be installed with pip or uv.</p>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"pipuv <pre><code>pip install idfkit\n</code></pre> <pre><code>uv add idfkit\n</code></pre> <p>This installs the core package with support for:</p> <ul> <li>Loading and writing IDF/epJSON files</li> <li>O(1) object lookups and reference tracking</li> <li>Schema validation</li> <li>3D geometry calculations</li> <li>Running EnergyPlus simulations</li> </ul>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>idfkit provides optional extras for additional functionality:</p>"},{"location":"getting-started/installation/#weather-station-index","title":"Weather Station Index","text":"<p>Refresh the bundled weather station index from climate.onebuilding.org:</p> pipuv <pre><code>pip install idfkit[weather]\n</code></pre> <pre><code>uv add idfkit[weather]\n</code></pre> <p>Note</p> <p>The bundled station index works without this extra. Only install <code>[weather]</code> if you need to refresh the index with <code>StationIndex.refresh()</code>.</p>"},{"location":"getting-started/installation/#dataframe-support","title":"DataFrame Support","text":"<p>Convert simulation results to pandas DataFrames:</p> pipuv <pre><code>pip install idfkit[dataframes]\n</code></pre> <pre><code>uv add idfkit[dataframes]\n</code></pre>"},{"location":"getting-started/installation/#plotting","title":"Plotting","text":"<p>Visualize simulation results with matplotlib or plotly:</p> pipuv <pre><code># Matplotlib backend\npip install idfkit[plot]\n\n# Plotly backend\npip install idfkit[plotly]\n</code></pre> <pre><code># Matplotlib backend\nuv add idfkit[plot]\n\n# Plotly backend\nuv add idfkit[plotly]\n</code></pre>"},{"location":"getting-started/installation/#progress-bars","title":"Progress Bars","text":"<p>Show tqdm progress bars during batch simulations:</p> pipuv <pre><code>pip install idfkit[progress]\n</code></pre> <pre><code>uv add idfkit[progress]\n</code></pre>"},{"location":"getting-started/installation/#cloud-storage-s3","title":"Cloud Storage (S3)","text":"<p>Store simulation results in Amazon S3:</p> pipuv <pre><code>pip install idfkit[s3]\n</code></pre> <pre><code>uv add idfkit[s3]\n</code></pre>"},{"location":"getting-started/installation/#install-everything","title":"Install Everything","text":"<p>Install all optional dependencies at once:</p> pipuv <pre><code>pip install idfkit[all]\n</code></pre> <pre><code>uv add idfkit[all]\n</code></pre>"},{"location":"getting-started/installation/#energyplus-installation","title":"EnergyPlus Installation","text":"<p>To run simulations, you need EnergyPlus installed on your system.</p>"},{"location":"getting-started/installation/#automatic-discovery","title":"Automatic Discovery","text":"<p>idfkit automatically discovers EnergyPlus using this priority:</p> <ol> <li>Explicit path passed to <code>find_energyplus(path=...)</code></li> <li>Environment variable <code>ENERGYPLUS_DIR</code></li> <li>System PATH (looks for <code>energyplus</code> executable)</li> <li>Platform defaults:<ul> <li>macOS: <code>/Applications/EnergyPlus-*/</code></li> <li>Linux: <code>/usr/local/EnergyPlus-*/</code></li> <li>Windows: <code>C:\\EnergyPlusV*/</code></li> </ul> </li> </ol>"},{"location":"getting-started/installation/#download-energyplus","title":"Download EnergyPlus","text":"<p>Download from the official EnergyPlus website:</p> <ul> <li>EnergyPlus Downloads</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from idfkit.simulation import find_energyplus\n\nconfig = find_energyplus()\nprint(f\"EnergyPlus {config.version[0]}.{config.version[1]}.{config.version[2]}\")\nprint(f\"Executable: {config.executable}\")\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>To contribute to idfkit, clone the repository and install with development dependencies:</p> <pre><code>git clone https://github.com/samuelduchesne/idfkit.git\ncd idfkit\nuv sync\n</code></pre> <p>Run the test suite:</p> <pre><code>make test\n</code></pre> <p>Run all quality checks:</p> <pre><code>make check\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or later</li> <li>EnergyPlus 8.9 or later (for simulation features)</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get up and running in 5 minutes</li> <li>Core Tutorial - In-depth interactive tutorial</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get up and running with idfkit in 5 minutes. This guide covers the essential operations you'll use every day.</p>"},{"location":"getting-started/quick-start/#load-a-model","title":"Load a Model","text":"<pre><code>from idfkit import load_idf\n\n# Load an existing IDF file\nmodel = load_idf(\"building.idf\")\nprint(f\"Loaded {len(model)} objects\")\n</code></pre>"},{"location":"getting-started/quick-start/#query-objects","title":"Query Objects","text":"<p>Access objects with O(1) dictionary lookups:</p> <pre><code># Get all zones\nfor zone in model[\"Zone\"]:\n    print(f\"Zone: {zone.name}\")\n\n# Get a specific zone by name\noffice = model[\"Zone\"][\"Office\"]\nprint(f\"Origin: ({office.x_origin}, {office.y_origin}, {office.z_origin})\")\n</code></pre>"},{"location":"getting-started/quick-start/#modify-fields","title":"Modify Fields","text":"<p>Change field values with attribute assignment:</p> <pre><code># Update a field\noffice.x_origin = 10.0\n\n# See what references this zone\nfor obj in model.get_referencing(\"Office\"):\n    print(f\"  {obj.obj_type}: {obj.name}\")\n</code></pre>"},{"location":"getting-started/quick-start/#discover-available-fields","title":"Discover Available Fields","text":"<p>Not sure what fields an object type has? Use <code>describe()</code> to see all available fields:</p> <pre><code># See all fields for a Zone\nprint(model.describe(\"Zone\"))\n# === Zone ===\n# ...\n# Fields (9):\n#   direction_of_relative_north (number) [deg] default=0\n#   x_origin (number) [m] default=0\n#   ...\n\n# See required fields for a Material\ndesc = model.describe(\"Material\")\nprint(f\"Required: {desc.required_fields}\")\n# Required: ['roughness', 'thickness', 'conductivity', 'density', 'specific_heat']\n</code></pre> <p>In REPL/Jupyter, use tab completion to explore object fields:</p> <pre><code>zone = model[\"Zone\"][\"Office\"]\nzone.&lt;TAB&gt;  # Shows: x_origin, y_origin, z_origin, multiplier, ...\n</code></pre> <p>Validation is enabled by default, so typos are caught immediately:</p> <pre><code>model.add(\"Zone\", \"Office\", x_orgin=0)  # Raises: unknown field 'x_orgin'\n\n# Disable validation for bulk operations where performance matters\nmodel.add(\"Zone\", \"Office\", x_origin=0, validate=False)\n</code></pre>"},{"location":"getting-started/quick-start/#create-a-new-model","title":"Create a New Model","text":"<pre><code>from idfkit import new_document\n\n# Create an empty model for EnergyPlus 24.1\nmodel = new_document(version=(24, 1, 0))\n\n# Add named objects\nmodel.add(\"Building\", \"My Building\", north_axis=0, terrain=\"City\")\nmodel.add(\"Zone\", \"Office\", x_origin=0, y_origin=0, z_origin=0)\n\n# Add unnamed objects (name parameter is optional)\nmodel.add(\"Timestep\", number_of_timesteps_per_hour=4)\nmodel.add(\n    \"GlobalGeometryRules\",\n    starting_vertex_position=\"UpperLeftCorner\",\n    vertex_entry_direction=\"Counterclockwise\",\n    coordinate_system=\"Relative\",\n)\n</code></pre>"},{"location":"getting-started/quick-start/#write-output","title":"Write Output","text":"<pre><code>from idfkit import write_idf, write_epjson\n\n# Write to IDF format\nwrite_idf(model, \"output.idf\")\n\n# Or write to epJSON format\nwrite_epjson(model, \"output.epJSON\")\n\n# Get as string (no file path)\nidf_string = write_idf(model)\n</code></pre>"},{"location":"getting-started/quick-start/#run-a-simulation","title":"Run a Simulation","text":"<pre><code>from idfkit.simulation import simulate\n\nresult = simulate(\n    model,\n    weather=\"weather.epw\",\n    design_day=True,  # Fast design-day run\n)\n\nprint(f\"Success: {result.success}\")\nprint(f\"Runtime: {result.runtime_seconds:.1f}s\")\n\n# Check for errors\nif result.errors.has_fatal:\n    for err in result.errors.fatal:\n        print(f\"Error: {err.message}\")\n</code></pre>"},{"location":"getting-started/quick-start/#query-results","title":"Query Results","text":"<pre><code># Get time-series data from SQLite output\nts = result.sql.get_timeseries(\n    variable_name=\"Zone Mean Air Temperature\",\n    key_value=\"Office\",\n)\nprint(f\"Variable: {ts.variable_name}\")\nprint(f\"Temperature range: {min(ts.values):.1f}\u00b0C to {max(ts.values):.1f}\u00b0C\")\n\n# Filter by environment if needed\nts_sizing = result.sql.get_timeseries(\n    variable_name=\"Zone Mean Air Temperature\",\n    key_value=\"Office\",\n    environment=\"sizing\",  # Design day data only\n)\n\n# Get tabular data\ntables = result.sql.get_tabular_data(report_name=\"AnnualBuildingUtilityPerformanceSummary\")\n</code></pre>"},{"location":"getting-started/quick-start/#find-weather-stations","title":"Find Weather Stations","text":"<pre><code>from idfkit.weather import StationIndex, geocode\n\n# Load the station index (instant, no network needed)\nindex = StationIndex.load()\n\n# Search by name\nresults = index.search(\"chicago ohare\")\nprint(results[0].station.display_name)\n\n# Find nearest station to an address\nresults = index.nearest(*geocode(\"Willis Tower, Chicago, IL\"))\nstation = results[0].station\nprint(f\"{station.display_name}: {results[0].distance_km:.0f} km away\")\n</code></pre>"},{"location":"getting-started/quick-start/#apply-design-days","title":"Apply Design Days","text":"<pre><code>from idfkit.weather import DesignDayManager\n\n# Parse a DDY file and apply design days to your model\nddm = DesignDayManager(\"weather.ddy\")\nadded = ddm.apply_to_model(\n    model,\n    heating=\"99.6%\",\n    cooling=\"1%\",\n    update_location=True,\n)\nprint(f\"Added {len(added)} design days\")\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Core Tutorial - Complete interactive walkthrough</li> <li>Simulation Guide - Deep dive into simulation features</li> <li>Weather Guide - Weather station search and design days</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"schedules/","title":"Schedules Overview","text":"<p>The schedules module lets you evaluate EnergyPlus schedules without running a simulation. This is useful for previewing schedule profiles, validating inputs, and understanding building operation patterns.</p>"},{"location":"schedules/#quick-start","title":"Quick Start","text":"<pre><code>from datetime import datetime\nfrom idfkit import load_idf\nfrom idfkit.schedules import evaluate, values\n\n# Load a model\ndoc = load_idf(\"building.idf\")\n\n# Get a schedule by name\nschedule = doc[\"Schedule:Compact\"][\"Office Occupancy\"]\n\n# Evaluate at a specific time\nvalue = evaluate(schedule, datetime(2024, 1, 8, 10, 0))\nprint(f\"Value at Monday 10am: {value}\")\n\n# Get hourly values for a full year\nhourly = values(schedule, year=2024)\nprint(f\"Annual hours: {len(hourly)}\")  # 8784 for leap year\n</code></pre>"},{"location":"schedules/#supported-schedule-types","title":"Supported Schedule Types","text":"Schedule Type Description <code>Schedule:Compact</code> DSL-based schedules with Through/For/Until syntax <code>Schedule:Year</code> References week schedules for date ranges <code>Schedule:Week:Daily</code> References day schedules for each weekday <code>Schedule:Week:Compact</code> Compact syntax for week schedules <code>Schedule:Day:Hourly</code> 24 hourly values <code>Schedule:Day:Interval</code> Time/value pairs <code>Schedule:Day:List</code> Values at fixed intervals <code>Schedule:Constant</code> Single constant value <code>Schedule:File</code> Values from external CSV file"},{"location":"schedules/#key-features","title":"Key Features","text":""},{"location":"schedules/#design-day-evaluation","title":"Design Day Evaluation","text":"<p>For sizing calculations, override the day type to use design day schedules:</p> <pre><code>from idfkit.schedules import evaluate\n\n# Summer design day (typically peak cooling)\nvalue = evaluate(\n    schedule,\n    datetime(2024, 7, 15, 14, 0),\n    day_type=\"summer\",\n)\n\n# Winter design day (typically peak heating)\nvalue = evaluate(\n    schedule,\n    datetime(2024, 1, 15, 6, 0),\n    day_type=\"winter\",\n)\n</code></pre> <p>Valid <code>day_type</code> values: <code>\"normal\"</code>, <code>\"summer\"</code>, <code>\"winter\"</code>, <code>\"holiday\"</code>, <code>\"customday1\"</code>, <code>\"customday2\"</code></p>"},{"location":"schedules/#holiday-support","title":"Holiday Support","text":"<p>Holidays are automatically extracted from <code>RunPeriodControl:SpecialDays</code> objects in your model:</p> <pre><code>from idfkit.schedules import get_holidays, evaluate\n\n# See what holidays are defined\nholidays = get_holidays(doc, year=2024)\nprint(f\"Holidays: {holidays}\")\n\n# Evaluation automatically uses holiday schedules on those dates\nchristmas = datetime(2024, 12, 25, 10, 0)\nvalue = evaluate(schedule, christmas)\n</code></pre>"},{"location":"schedules/#sub-hourly-timesteps","title":"Sub-Hourly Timesteps","text":"<p>Generate values at any timestep (values per hour):</p> <pre><code>from idfkit.schedules import values\n\n# 15-minute intervals (4 per hour)\nquarter_hourly = values(schedule, year=2024, timestep=4)\nprint(f\"Values: {len(quarter_hourly)}\")  # 35136 for leap year\n\n# 1-minute intervals\nminute_values = values(schedule, year=2024, timestep=60)\n</code></pre>"},{"location":"schedules/#interpolation","title":"Interpolation","text":"<p>Control how values are interpolated between defined points:</p> <pre><code>from idfkit.schedules import values\n\n# Step function (default) - value changes at each Until time\nstep_values = values(schedule, timestep=4, interpolation=\"no\")\n\n# Linear interpolation between values\nsmooth_values = values(schedule, timestep=4, interpolation=\"average\")\n</code></pre> <p>Valid <code>interpolation</code> values: <code>\"no\"</code> (or <code>\"step\"</code>), <code>\"average\"</code> (or <code>\"linear\"</code>)</p>"},{"location":"schedules/#schedulefile-with-remote-storage","title":"Schedule:File with Remote Storage","text":"<p>Read CSV files from any storage backend using the FileSystem interface:</p> <pre><code>from idfkit.simulation.fs import S3FileSystem\nfrom idfkit.schedules import evaluate\n\n# Configure S3 storage\nfs = S3FileSystem(bucket=\"my-bucket\", prefix=\"schedules/\")\n\n# Evaluate Schedule:File reading from S3\nvalue = evaluate(schedule, dt, fs=fs, base_path=\"\")\n</code></pre>"},{"location":"schedules/#pandas-integration","title":"Pandas Integration","text":"<p>Convert schedules to pandas Series for analysis and plotting:</p> <pre><code>from idfkit.schedules import to_series, plot_schedule\n\n# Convert to pandas Series with datetime index\nseries = to_series(schedule, year=2024)\nprint(series.describe())\n\n# Quick visualization\nplot_schedule(schedule, year=2024)\n</code></pre>"},{"location":"schedules/#example-analyze-office-occupancy","title":"Example: Analyze Office Occupancy","text":"<pre><code>from idfkit import load_idf\nfrom idfkit.schedules import values, to_series\n\ndoc = load_idf(\"office.idf\")\noccupancy = doc[\"Schedule:Compact\"][\"BLDG_OCC_SCH\"]\n\n# Get annual values\nannual = values(occupancy, year=2024)\n\n# Basic statistics\ntotal_hours = len([v for v in annual if v &gt; 0])\nprint(f\"Occupied hours: {total_hours}\")\n\n# Peak analysis with pandas\nseries = to_series(occupancy, year=2024)\nprint(f\"Peak occupancy: {series.max()}\")\nprint(f\"Average (occupied): {series[series &gt; 0].mean():.2f}\")\n\n# Weekly pattern\nweekly = series.groupby(series.index.dayofweek).mean()\nprint(\"Average by day of week:\")\nprint(weekly)\n</code></pre>"},{"location":"schedules/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Full API documentation</li> <li>Design Document - Implementation details</li> </ul>"},{"location":"simulation/","title":"Simulation Overview","text":"<p>The simulation module provides subprocess-based EnergyPlus execution with structured result parsing, batch processing, and content-addressed caching.</p>"},{"location":"simulation/#quick-start","title":"Quick Start","text":"<pre><code>from idfkit import load_idf\nfrom idfkit.simulation import simulate\n\n# Load a model and run a simulation\nmodel = load_idf(\"building.idf\")\nresult = simulate(model, \"weather.epw\", design_day=True)\n\n# Check results\nprint(f\"Success: {result.success}\")\nprint(f\"Runtime: {result.runtime_seconds:.1f}s\")\n\n# Query time-series data\nts = result.sql.get_timeseries(\n    variable_name=\"Zone Mean Air Temperature\",\n    key_value=\"THERMAL ZONE 1\",\n)\nprint(f\"Max temp: {max(ts.values):.1f}\u00b0C\")\n</code></pre>"},{"location":"simulation/#requirements","title":"Requirements","text":"<ul> <li>EnergyPlus 8.9+ installed on your system</li> <li>idfkit auto-discovers EnergyPlus installations</li> </ul> <p>Check your installation:</p> <pre><code>from idfkit.simulation import find_energyplus\n\nconfig = find_energyplus()\nprint(f\"Found EnergyPlus {config.version[0]}.{config.version[1]}\")\n</code></pre>"},{"location":"simulation/#module-components","title":"Module Components","text":"Component Description <code>simulate()</code> Run a single simulation <code>SimulationResult</code> Access output files and parsed data <code>SQLResult</code> Query time-series and tabular data <code>simulate_batch()</code> Run multiple simulations in parallel <code>async_simulate()</code> Non-blocking single simulation <code>async_simulate_batch()</code> Non-blocking parallel simulations <code>async_simulate_batch_stream()</code> Streaming progress via async generator <code>SimulationProgress</code> Real-time progress tracking via callbacks <code>SimulationCache</code> Content-addressed result caching <code>OutputVariableIndex</code> Discover available output variables <code>ErrorReport</code> Parse error and warning messages Plotting Visualize results with matplotlib/plotly"},{"location":"simulation/#key-features","title":"Key Features","text":""},{"location":"simulation/#automatic-sqlite-output","title":"Automatic SQLite Output","text":"<p>The module automatically injects <code>Output:SQLite</code> into your model, ensuring reliable access to all simulation data through a single queryable file.</p>"},{"location":"simulation/#lazy-loading","title":"Lazy Loading","text":"<p>Output files are only parsed when accessed, keeping memory usage low:</p> <pre><code>result = simulate(model, weather)  # Just runs EnergyPlus\nresult.errors  # Parses ERR file on first access\nresult.sql  # Opens SQLite database on first access\n</code></pre>"},{"location":"simulation/#model-safety","title":"Model Safety","text":"<p>Your original model is never mutated \u2014 simulations work on a copy:</p> <pre><code>result = simulate(model, weather)\nassert \"Output:SQLite\" not in model  # Original unchanged\n</code></pre>"},{"location":"simulation/#parallel-execution","title":"Parallel Execution","text":"<p>Run parametric studies efficiently with batch processing:</p> <pre><code>from idfkit.simulation import simulate_batch, SimulationJob\n\njobs = [SimulationJob(model=variant, weather=\"weather.epw\", label=f\"case-{i}\") for i, variant in enumerate(variants)]\nbatch = simulate_batch(jobs, max_workers=4)\n</code></pre>"},{"location":"simulation/#async-execution","title":"Async Execution","text":"<p>For async applications, non-blocking variants are available:</p> <pre><code>from idfkit.simulation import async_simulate, async_simulate_batch_stream\n\n# Single simulation\nresult = await async_simulate(model, \"weather.epw\")\n\n# Streaming progress\nasync for event in async_simulate_batch_stream(jobs, max_concurrent=4):\n    print(f\"[{event.completed}/{event.total}] {event.label}\")\n</code></pre>"},{"location":"simulation/#installation","title":"Installation","text":"<p>The simulation module requires no extra dependencies for basic operation.</p> <p>For optional features:</p> <pre><code># DataFrame conversion\npip install idfkit[dataframes]\n\n# Plotting\npip install idfkit[plot]      # matplotlib\npip install idfkit[plotly]    # plotly\n\n# Cloud storage\npip install idfkit[s3]\n</code></pre>"},{"location":"simulation/#next-steps","title":"Next Steps","text":"<ul> <li>Running Simulations \u2014 Detailed guide to <code>simulate()</code></li> <li>Async Simulation \u2014 Non-blocking execution with <code>asyncio</code></li> <li>Parsing Results \u2014 Working with <code>SimulationResult</code></li> <li>Simulation Architecture \u2014 Design decisions</li> </ul>"},{"location":"simulation/async/","title":"Async Simulation","text":"<p>The async simulation API provides non-blocking counterparts to <code>simulate()</code> and <code>simulate_batch()</code>, built on Python's <code>asyncio</code> module.  Use these when you need to run EnergyPlus simulations inside an async application (FastAPI, Jupyter async, event-driven orchestrators) or when you want streaming progress without callbacks.</p>"},{"location":"simulation/async/#when-to-use-async-vs-sync","title":"When to Use Async vs Sync","text":"Scenario Recommended Scripts, CLI tools <code>simulate()</code> / <code>simulate_batch()</code> Async web servers (FastAPI, aiohttp) <code>async_simulate()</code> / <code>async_simulate_batch()</code> Real-time progress UI without callbacks <code>async_simulate_batch_stream()</code> Jupyter with <code>await</code> support Either \u2014 async avoids blocking the notebook Mixing simulations with other async I/O <code>async_simulate()</code>"},{"location":"simulation/async/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom idfkit import load_idf\nfrom idfkit.simulation import async_simulate\n\n\nasync def main():\n    model = load_idf(\"building.idf\")\n    result = await async_simulate(model, \"weather.epw\", design_day=True)\n\n    print(f\"Success: {result.success}\")\n    print(f\"Runtime: {result.runtime_seconds:.1f}s\")\n\n\nasyncio.run(main())\n</code></pre> <p><code>async_simulate()</code> accepts exactly the same parameters as <code>simulate()</code> and returns the same <code>SimulationResult</code>.  The only difference is that EnergyPlus runs as an <code>asyncio</code> subprocess, so the event loop is free to do other work while waiting.</p>"},{"location":"simulation/async/#async-batch-processing","title":"Async Batch Processing","text":"<p><code>async_simulate_batch()</code> mirrors <code>simulate_batch()</code> but uses an <code>asyncio.Semaphore</code> for concurrency control instead of a thread pool:</p> <pre><code>import asyncio\nfrom idfkit.simulation import async_simulate_batch, SimulationJob\n\n\nasync def main():\n    jobs = [\n        SimulationJob(model=model1, weather=\"weather.epw\", label=\"baseline\"),\n        SimulationJob(model=model2, weather=\"weather.epw\", label=\"improved\"),\n    ]\n\n    batch = await async_simulate_batch(jobs, max_concurrent=4)\n\n    print(f\"Completed: {len(batch.succeeded)}/{len(batch)}\")\n    for i, result in enumerate(batch):\n        print(f\"  Job {i}: {'Success' if result.success else 'Failed'}\")\n\n\nasyncio.run(main())\n</code></pre> <p>Results are returned in the same order as the input jobs, identical to <code>simulate_batch()</code>.</p>"},{"location":"simulation/async/#concurrency","title":"Concurrency","text":"<p>Control how many simulations run at once:</p> <pre><code># Use all CPUs (default)\nbatch = await async_simulate_batch(jobs)\n\n# Limit to 4 concurrent simulations\nbatch = await async_simulate_batch(jobs, max_concurrent=4)\n\n# Sequential (useful for debugging)\nbatch = await async_simulate_batch(jobs, max_concurrent=1)\n</code></pre> <p>Default: <code>min(len(jobs), os.cpu_count())</code></p>"},{"location":"simulation/async/#streaming-progress","title":"Streaming Progress","text":"<p><code>async_simulate_batch_stream()</code> is an async generator that yields <code>SimulationEvent</code> objects as each simulation completes \u2014 no callbacks needed:</p> <pre><code>import asyncio\nfrom idfkit.simulation import async_simulate_batch_stream, SimulationJob\n\n\nasync def main():\n    jobs = [\n        SimulationJob(model=variant, weather=\"weather.epw\", label=f\"case-{i}\") for i, variant in enumerate(variants)\n    ]\n\n    async for event in async_simulate_batch_stream(jobs, max_concurrent=4):\n        status = \"OK\" if event.result.success else \"FAIL\"\n        print(f\"[{event.completed}/{event.total}] {event.label}: {status}\")\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"simulation/async/#simulationevent","title":"SimulationEvent","text":"<p>Each event contains:</p> Attribute Type Description <code>index</code> <code>int</code> Position of this job in the original sequence <code>label</code> <code>str</code> Human-readable label from <code>SimulationJob</code> <code>result</code> <code>SimulationResult</code> The simulation result <code>completed</code> <code>int</code> Number of jobs completed so far <code>total</code> <code>int</code> Total number of jobs <p>Events arrive in completion order, not submission order.  Use <code>event.index</code> to map back to the original job.</p>"},{"location":"simulation/async/#early-termination","title":"Early Termination","text":"<p>Breaking out of the stream cancels remaining simulations:</p> <pre><code>async for event in async_simulate_batch_stream(jobs, max_concurrent=4):\n    if not event.result.success:\n        print(f\"Job {event.label} failed \u2014 aborting remaining\")\n        break  # Remaining tasks are cancelled automatically\n</code></pre>"},{"location":"simulation/async/#cancellation","title":"Cancellation","text":"<p>Async tasks support cancellation natively.  Cancelling a task kills the underlying EnergyPlus subprocess:</p> <pre><code>async def run_with_timeout():\n    task = asyncio.create_task(async_simulate(model, \"weather.epw\"))\n\n    try:\n        result = await asyncio.wait_for(task, timeout=120)\n    except asyncio.TimeoutError:\n        print(\"Simulation cancelled after 120s\")\n</code></pre>"},{"location":"simulation/async/#parametric-study","title":"Parametric Study","text":"<p>Create model variants and analyze results \u2014 the async equivalent of the pattern shown in Batch Processing:</p> <pre><code>import asyncio\nfrom idfkit.simulation import async_simulate_batch, SimulationJob\n\n\nasync def main():\n    # Create variants\n    jobs = []\n    for insulation in [0.05, 0.10, 0.15, 0.20]:\n        variant = model.copy()\n        variant[\"Material\"][\"Insulation\"].thickness = insulation\n        jobs.append(\n            SimulationJob(\n                model=variant,\n                weather=\"weather.epw\",\n                label=f\"insulation-{insulation}m\",\n                design_day=True,\n            )\n        )\n\n    # Run all variants\n    batch = await async_simulate_batch(jobs, max_concurrent=4)\n\n    # Analyze results\n    for job, result in zip(jobs, batch):\n        if result.success:\n            ts = result.sql.get_timeseries(\n                \"Zone Mean Air Temperature\",\n                \"ZONE 1\",\n            )\n            print(f\"{job.label}: Max temp {max(ts.values):.1f}\u00b0C\")\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"simulation/async/#running-simulations-alongside-other-async-work","title":"Running Simulations Alongside Other Async Work","text":"<p>A key benefit of the async API is running simulations concurrently with other I/O-bound tasks \u2014 database queries, HTTP requests, file uploads \u2014 without blocking:</p> <pre><code>import asyncio\nfrom idfkit.simulation import async_simulate\n\n\nasync def fetch_weather_data(station_id: str) -&gt; dict:\n    \"\"\"Fetch weather metadata from a remote API.\"\"\"\n    ...\n\n\nasync def main():\n    # Run a simulation and an API call concurrently\n    sim_task = async_simulate(model, \"weather.epw\", design_day=True)\n    api_task = fetch_weather_data(\"725300\")\n\n    result, weather_meta = await asyncio.gather(sim_task, api_task)\n\n    print(f\"Simulation: {result.runtime_seconds:.1f}s\")\n    print(f\"Weather station: {weather_meta}\")\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"simulation/async/#collecting-streaming-results","title":"Collecting Streaming Results","text":"<p>The streaming API yields events in completion order.  To collect and reorder results for analysis:</p> <pre><code>import asyncio\nfrom idfkit.simulation import async_simulate_batch_stream, SimulationJob\n\n\nasync def main():\n    jobs = [\n        SimulationJob(model=variant, weather=\"weather.epw\", label=f\"case-{i}\") for i, variant in enumerate(variants)\n    ]\n\n    # Collect events and reorder by original index\n    results = [None] * len(jobs)\n    async for event in async_simulate_batch_stream(jobs, max_concurrent=4):\n        results[event.index] = event.result\n        pct = event.completed / event.total * 100\n        print(f\"[{pct:3.0f}%] {event.label}: {'OK' if event.result.success else 'FAIL'}\")\n\n    # Results are now in submission order\n    for i, result in enumerate(results):\n        if result.success:\n            ts = result.sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\n            print(f\"Case {i}: max temp {max(ts.values):.1f}\u00b0C\")\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"simulation/async/#caching-and-cloud-storage","title":"Caching and Cloud Storage","text":"<p>All async functions accept the same <code>cache</code> and <code>fs</code> parameters as their sync counterparts.  For best results in async code, use an <code>AsyncFileSystem</code> to avoid blocking the event loop during file uploads and result reads:</p> <pre><code>from idfkit.simulation import SimulationCache, AsyncLocalFileSystem\n\ncache = SimulationCache()\nfs = AsyncLocalFileSystem()\n\nresult = await async_simulate(\n    model, \"weather.epw\",\n    cache=cache,\n    output_dir=\"run-001\",\n    fs=fs,\n)\n\n# Use async accessors to read results without blocking\nerrors = await result.async_errors()\nsql = await result.async_sql()\n</code></pre> <p>For S3 storage, use <code>AsyncS3FileSystem</code> (requires <code>pip install idfkit[async-s3]</code>):</p> <pre><code>from idfkit.simulation import AsyncS3FileSystem, SimulationCache\n\ncache = SimulationCache()\n\nasync with AsyncS3FileSystem(bucket=\"my-bucket\", prefix=\"study/\") as fs:\n    result = await async_simulate(\n        model, \"weather.epw\",\n        cache=cache,\n        output_dir=\"run-001\",\n        fs=fs,\n    )\n</code></pre> <p>A sync <code>FileSystem</code> (e.g., <code>S3FileSystem</code>) is still accepted \u2014 the upload step is automatically wrapped in <code>asyncio.to_thread()</code> so it doesn't block the event loop.  However, using <code>AsyncFileSystem</code> is recommended because it provides true non-blocking I/O for both uploads and result reads (via the <code>async_errors()</code>, <code>async_sql()</code>, etc. accessors), and avoids thread-pool overhead.</p>"},{"location":"simulation/async/#fastapi-integration","title":"FastAPI Integration","text":"<p>A minimal example of an async simulation endpoint:</p> <pre><code>from fastapi import FastAPI\nfrom idfkit import load_idf\nfrom idfkit.simulation import async_simulate\n\napp = FastAPI()\n\n\n@app.post(\"/simulate\")\nasync def run_simulation(idf_path: str, weather_path: str):\n    model = load_idf(idf_path)\n    result = await async_simulate(model, weather_path, design_day=True)\n\n    return {\n        \"success\": result.success,\n        \"runtime\": result.runtime_seconds,\n        \"errors\": result.errors.summary(),\n    }\n</code></pre> <p>Because <code>async_simulate</code> doesn't block the event loop, the server remains responsive to other requests while EnergyPlus runs.</p>"},{"location":"simulation/async/#preprocessing","title":"Preprocessing","text":"<p>When <code>expand_objects=True</code> (the default), the Slab and Basement preprocessors run in a background thread via <code>asyncio.to_thread()</code> so they don't block the event loop.  This is transparent \u2014 no user action required.</p>"},{"location":"simulation/async/#error-handling","title":"Error Handling","text":"<p>Error handling is identical to the sync API:</p> <pre><code>from idfkit.exceptions import SimulationError\n\ntry:\n    result = await async_simulate(model, weather, timeout=60)\nexcept SimulationError as e:\n    if e.exit_code is None:\n        print(\"Simulation timed out\")\n    else:\n        print(f\"Failed: {e}\")\n</code></pre> <p>In batch mode, individual failures are captured in the results \u2014 the batch never raises due to a single job failing.</p>"},{"location":"simulation/async/#see-also","title":"See Also","text":"<ul> <li>Running Simulations \u2014 Sync simulation guide</li> <li>Batch Processing \u2014 Sync batch guide</li> <li>Simulation Architecture \u2014 Design decisions</li> </ul>"},{"location":"simulation/batch/","title":"Batch Processing","text":"<p>The <code>simulate_batch()</code> function runs multiple EnergyPlus simulations in parallel using a thread pool, ideal for parametric studies and sensitivity analyses.</p>"},{"location":"simulation/batch/#basic-usage","title":"Basic Usage","text":"<pre><code>from idfkit.simulation import simulate_batch, SimulationJob\n\n# Create jobs\njobs = [\n    SimulationJob(model=model1, weather=\"weather.epw\", label=\"baseline\"),\n    SimulationJob(model=model2, weather=\"weather.epw\", label=\"improved\"),\n]\n\n# Run in parallel\nbatch = simulate_batch(jobs, max_workers=4)\n\nprint(f\"Completed: {len(batch.succeeded)}/{len(batch)}\")\nfor i, result in enumerate(batch):\n    print(f\"  Job {i}: {'Success' if result.success else 'Failed'}\")\n</code></pre>"},{"location":"simulation/batch/#simulationjob","title":"SimulationJob","text":"<p>Define individual simulations with <code>SimulationJob</code>:</p> <pre><code>from idfkit.simulation import SimulationJob\n\njob = SimulationJob(\n    model=my_model,  # Required: IDFDocument\n    weather=\"weather.epw\",  # Required: Path to weather file\n    label=\"case-001\",  # Optional: Label for progress reporting\n    output_dir=\"./output/case1\",  # Optional: Output directory\n    design_day=True,  # Optional: Design-day-only\n    annual=False,  # Optional: Annual simulation\n    timeout=3600.0,  # Optional: Max runtime in seconds\n)\n</code></pre>"},{"location":"simulation/batch/#simulationjob-attributes","title":"SimulationJob Attributes","text":"Attribute Type Default Description <code>model</code> <code>IDFDocument</code> Required EnergyPlus model <code>weather</code> <code>str | Path</code> Required Weather file path <code>label</code> <code>str</code> <code>\"\"</code> Human-readable label <code>output_dir</code> <code>str | Path | None</code> <code>None</code> Output directory <code>expand_objects</code> <code>bool</code> <code>True</code> Run ExpandObjects <code>annual</code> <code>bool</code> <code>False</code> Annual simulation <code>design_day</code> <code>bool</code> <code>False</code> Design-day-only <code>output_prefix</code> <code>str</code> <code>\"eplus\"</code> Output file prefix <code>output_suffix</code> <code>\"C\" | \"L\" | \"D\"</code> <code>\"C\"</code> Output naming style <code>readvars</code> <code>bool</code> <code>False</code> Run ReadVarsESO <code>timeout</code> <code>float</code> <code>3600.0</code> Max runtime (seconds) <code>extra_args</code> <code>tuple[str, ...] | None</code> <code>None</code> Extra CLI args"},{"location":"simulation/batch/#parametric-studies","title":"Parametric Studies","text":"<p>Create model variants for parametric analysis:</p> <pre><code>from idfkit.simulation import simulate_batch, SimulationJob\n\n# Create variants\njobs = []\nfor insulation in [0.05, 0.10, 0.15, 0.20]:\n    variant = model.copy()\n    variant[\"Material\"][\"Insulation\"].thickness = insulation\n    jobs.append(\n        SimulationJob(\n            model=variant,\n            weather=\"weather.epw\",\n            label=f\"insulation-{insulation}m\",\n            design_day=True,\n        )\n    )\n\n# Run all variants\nbatch = simulate_batch(jobs, max_workers=4)\n\n# Analyze results\nfor job, result in zip(jobs, batch):\n    if result.success:\n        ts = result.sql.get_timeseries(\n            \"Zone Mean Air Temperature\",\n            \"ZONE 1\",\n        )\n        print(f\"{job.label}: Max temp {max(ts.values):.1f}\u00b0C\")\n</code></pre>"},{"location":"simulation/batch/#batchresult","title":"BatchResult","text":"<p>The <code>BatchResult</code> class aggregates results:</p> <pre><code>batch = simulate_batch(jobs)\n\n# Access results\nbatch.results  # All results as tuple\nbatch[0]  # First result (by index)\nlen(batch)  # Number of jobs\n\n# Filter by success\nbatch.succeeded  # Only successful results\nbatch.failed  # Only failed results\nbatch.all_succeeded  # True if all succeeded\n\n# Timing\nprint(f\"Total time: {batch.total_runtime_seconds:.1f}s\")\n</code></pre>"},{"location":"simulation/batch/#progress-callbacks","title":"Progress Callbacks","text":"<p>Monitor progress with a callback function:</p> <pre><code>def on_progress(completed, total, label, success):\n    status = \"OK\" if success else \"FAIL\"\n    print(f\"[{completed}/{total}] {label}: {status}\")\n\n\nbatch = simulate_batch(jobs, progress=on_progress)\n</code></pre> <p>The callback receives:</p> Parameter Type Description <code>completed</code> <code>int</code> Number of completed jobs <code>total</code> <code>int</code> Total number of jobs <code>label</code> <code>str</code> Label of the just-completed job <code>success</code> <code>bool</code> Whether the job succeeded"},{"location":"simulation/batch/#rich-progress-bar","title":"Rich Progress Bar","text":"<pre><code>from rich.progress import Progress\n\nwith Progress() as progress:\n    task = progress.add_task(\"Simulating...\", total=len(jobs))\n\n    def callback(completed, total, label, success):\n        progress.update(task, completed=completed)\n\n    batch = simulate_batch(jobs, progress=callback)\n</code></pre>"},{"location":"simulation/batch/#parallelism","title":"Parallelism","text":""},{"location":"simulation/batch/#worker-count","title":"Worker Count","text":"<p>Control concurrency with <code>max_workers</code>:</p> <pre><code># Use all CPUs\nbatch = simulate_batch(jobs, max_workers=None)  # Default\n\n# Limit to 4 concurrent simulations\nbatch = simulate_batch(jobs, max_workers=4)\n\n# Sequential (useful for debugging)\nbatch = simulate_batch(jobs, max_workers=1)\n</code></pre> <p>Default: <code>min(len(jobs), os.cpu_count())</code></p>"},{"location":"simulation/batch/#thread-vs-process","title":"Thread vs Process","text":"<p><code>simulate_batch</code> uses threads (not processes) because:</p> <ul> <li>EnergyPlus runs as a subprocess (releases GIL)</li> <li>Lower memory overhead than multiprocessing</li> <li>Simpler error handling</li> </ul>"},{"location":"simulation/batch/#error-handling","title":"Error Handling","text":"<p>Failed simulations don't stop the batch:</p> <pre><code>batch = simulate_batch(jobs)\n\nfor i, result in enumerate(batch):\n    if not result.success:\n        print(f\"Job {i} failed:\")\n        print(f\"  Exit code: {result.exit_code}\")\n        print(f\"  Stderr: {result.stderr}\")\n        for err in result.errors.fatal:\n            print(f\"  Error: {err.message}\")\n</code></pre>"},{"location":"simulation/batch/#partial-failures","title":"Partial Failures","text":"<pre><code>if not batch.all_succeeded:\n    failed_count = len(batch.failed)\n    print(f\"{failed_count} jobs failed\")\n\n    # Process only successful results\n    for result in batch.succeeded:\n        # ... analyze results\n</code></pre>"},{"location":"simulation/batch/#caching","title":"Caching","text":"<p>Share a cache across batch jobs:</p> <pre><code>from idfkit.simulation import SimulationCache\n\ncache = SimulationCache()\n\n# All jobs share the same cache\nbatch = simulate_batch(jobs, cache=cache)\n\n# Re-running is instant for unchanged models\nbatch2 = simulate_batch(jobs, cache=cache)  # Cache hits\n</code></pre>"},{"location":"simulation/batch/#cloud-storage","title":"Cloud Storage","text":"<p>Store results in S3:</p> <pre><code>from idfkit.simulation import S3FileSystem\n\nfs = S3FileSystem(bucket=\"my-bucket\", prefix=\"study-001/\")\n\n# Each job needs an explicit output_dir\njobs = [\n    SimulationJob(\n        model=variant,\n        weather=\"weather.epw\",\n        label=f\"case-{i}\",\n        output_dir=f\"case-{i}\",  # Required with fs\n    )\n    for i, variant in enumerate(variants)\n]\n\nbatch = simulate_batch(jobs, fs=fs)\n</code></pre>"},{"location":"simulation/batch/#best-practices","title":"Best Practices","text":"<ol> <li>Use labels \u2014 Makes progress tracking and debugging easier</li> <li>Set timeouts \u2014 Prevent runaway simulations from blocking</li> <li>Share caches \u2014 Avoid redundant work across similar models</li> <li>Handle failures gracefully \u2014 Check <code>result.success</code> before accessing outputs</li> <li>Start small \u2014 Test with a few jobs before running thousands</li> </ol>"},{"location":"simulation/batch/#see-also","title":"See Also","text":"<ul> <li>Running Simulations \u2014 Single simulation guide</li> <li>Caching \u2014 Content-addressed caching</li> <li>Examples: Parametric Study \u2014 Complete example</li> </ul>"},{"location":"simulation/caching/","title":"Simulation Caching","text":"<p>The <code>SimulationCache</code> provides content-addressed caching to avoid redundant simulations. Cache keys are computed from model content, weather data, and simulation flags.</p>"},{"location":"simulation/caching/#basic-usage","title":"Basic Usage","text":"<pre><code>from idfkit.simulation import simulate, SimulationCache\n\ncache = SimulationCache()\n\n# First run: executes EnergyPlus\nresult1 = simulate(model, \"weather.epw\", cache=cache)\nprint(f\"Runtime: {result1.runtime_seconds:.1f}s\")\n\n# Second run: instant cache hit\nresult2 = simulate(model, \"weather.epw\", cache=cache)\nprint(f\"Runtime: {result2.runtime_seconds:.1f}s\")  # Near zero\n</code></pre>"},{"location":"simulation/caching/#how-it-works","title":"How It Works","text":""},{"location":"simulation/caching/#cache-key-computation","title":"Cache Key Computation","text":"<p>The cache key is a SHA-256 digest of:</p> <ol> <li>Normalized IDF content \u2014 Model text with <code>Output:SQLite</code> ensured</li> <li>Weather file bytes \u2014 Complete weather file content</li> <li>Simulation flags \u2014 <code>annual</code>, <code>design_day</code>, <code>expand_objects</code>, etc.</li> </ol> <pre><code>key = cache.compute_key(\n    model,\n    \"weather.epw\",\n    design_day=True,\n    annual=False,\n)\nprint(f\"Cache key: {key.hex_digest[:16]}...\")\n</code></pre>"},{"location":"simulation/caching/#what-gets-cached","title":"What Gets Cached","text":"<p>The entire run directory is copied into the cache:</p> <ul> <li>SQLite output database (<code>.sql</code>)</li> <li>Error report (<code>.err</code>)</li> <li>Variable files (<code>.rdd</code>, <code>.mdd</code>)</li> <li>All other output files</li> </ul> <p>Cached results have full access to all outputs, identical to a fresh run.</p>"},{"location":"simulation/caching/#cache-location","title":"Cache Location","text":"<p>Default locations by platform:</p> Platform Default Path Linux <code>~/.cache/idfkit/simulation/</code> macOS <code>~/Library/Caches/idfkit/simulation/</code> Windows <code>%LOCALAPPDATA%\\idfkit\\cache\\simulation\\</code>"},{"location":"simulation/caching/#custom-location","title":"Custom Location","text":"<pre><code>from pathlib import Path\n\ncache = SimulationCache(cache_dir=Path(\"/data/sim_cache\"))\n</code></pre>"},{"location":"simulation/caching/#cache-operations","title":"Cache Operations","text":""},{"location":"simulation/caching/#check-for-hit","title":"Check for Hit","text":"<pre><code>key = cache.compute_key(model, weather, design_day=True)\n\nif cache.contains(key):\n    print(\"Would be a cache hit\")\nelse:\n    print(\"Would be a cache miss\")\n</code></pre>"},{"location":"simulation/caching/#manual-getput","title":"Manual Get/Put","text":"<pre><code># Compute key\nkey = cache.compute_key(model, weather)\n\n# Check cache\ncached_result = cache.get(key)\nif cached_result is not None:\n    print(\"Cache hit!\")\nelse:\n    # Run simulation\n    result = simulate(model, weather)\n\n    # Store in cache (only successful results)\n    cache.put(key, result)\n</code></pre>"},{"location":"simulation/caching/#clear-cache","title":"Clear Cache","text":"<pre><code># Remove all cached entries\ncache.clear()\n</code></pre>"},{"location":"simulation/caching/#batch-processing","title":"Batch Processing","text":"<p>Share a cache across batch simulations:</p> <pre><code>from idfkit.simulation import simulate_batch, SimulationCache\n\ncache = SimulationCache()\n\n# All jobs share the same cache\nbatch1 = simulate_batch(jobs, cache=cache)\n\n# Re-running unchanged jobs hits cache\nbatch2 = simulate_batch(jobs, cache=cache)  # Instant for unchanged\n</code></pre>"},{"location":"simulation/caching/#cache-invalidation","title":"Cache Invalidation","text":"<p>Content-addressed caching means automatic invalidation:</p> Change Effect Modify model Different key \u2192 fresh simulation Change weather file Different key \u2192 fresh simulation Change flags Different key \u2192 fresh simulation Same inputs Same key \u2192 cache hit <p>No manual invalidation is needed.</p>"},{"location":"simulation/caching/#cache-key-details","title":"Cache Key Details","text":""},{"location":"simulation/caching/#model-normalization","title":"Model Normalization","text":"<p>Before hashing, the model is:</p> <ol> <li>Copied to avoid mutation</li> <li>Has <code>Output:SQLite</code> added if missing</li> <li>Written to IDF text format</li> </ol> <p>This ensures models differing only in <code>Output:SQLite</code> produce the same key.</p>"},{"location":"simulation/caching/#flag-influence","title":"Flag Influence","text":"<p>Flags that affect the cache key:</p> <ul> <li><code>expand_objects</code></li> <li><code>annual</code></li> <li><code>design_day</code></li> <li><code>output_suffix</code></li> <li><code>extra_args</code></li> </ul> <p>Flags that don't affect the key:</p> <ul> <li><code>output_dir</code> (just affects where results go)</li> <li><code>timeout</code> (affects execution, not results)</li> <li><code>readvars</code> (post-processing only)</li> </ul>"},{"location":"simulation/caching/#thread-and-process-safety","title":"Thread and Process Safety","text":"<p>The cache is safe for concurrent access:</p> <ul> <li>Atomic writes \u2014 Uses temp directory + rename</li> <li>Thread-safe \u2014 Safe for <code>simulate_batch()</code> with shared cache</li> <li>Process-safe \u2014 Multiple Python processes can share the cache</li> </ul> <pre><code># Safe: concurrent access from multiple workers\nbatch = simulate_batch(jobs, max_workers=8, cache=cache)\n</code></pre>"},{"location":"simulation/caching/#storage-considerations","title":"Storage Considerations","text":""},{"location":"simulation/caching/#disk-space","title":"Disk Space","text":"<p>Each cached entry is a full copy of the run directory. Monitor usage:</p> <pre><code>du -sh ~/.cache/idfkit/simulation/\n</code></pre>"},{"location":"simulation/caching/#cleanup","title":"Cleanup","text":"<pre><code># Clear everything\ncache.clear()\n\n# Or manually delete specific entries\nimport shutil\n\nshutil.rmtree(cache.cache_dir / \"abc123...\")\n</code></pre>"},{"location":"simulation/caching/#disabling-caching","title":"Disabling Caching","text":"<p>Pass <code>cache=None</code> (the default) to skip caching:</p> <pre><code># No caching\nresult = simulate(model, weather)\n\n# With caching\nresult = simulate(model, weather, cache=SimulationCache())\n</code></pre>"},{"location":"simulation/caching/#best-practices","title":"Best Practices","text":"<ol> <li>Use for development \u2014 Cache during iterative testing</li> <li>Clear for production \u2014 Start fresh for final runs</li> <li>Share across batch \u2014 Pass same cache to <code>simulate_batch()</code></li> <li>Monitor disk usage \u2014 Large studies can fill disk</li> <li>Custom location \u2014 Use fast SSD for better performance</li> </ol>"},{"location":"simulation/caching/#see-also","title":"See Also","text":"<ul> <li>Caching Strategy \u2014 Design concepts</li> <li>Running Simulations \u2014 Basic simulation guide</li> <li>Batch Processing \u2014 Parallel execution</li> </ul>"},{"location":"simulation/errors/","title":"Error Handling","text":"<p>This page covers error handling in the simulation module, including parsing EnergyPlus error reports and handling simulation failures.</p>"},{"location":"simulation/errors/#error-report","title":"Error Report","text":"<p>The <code>ErrorReport</code> class parses the <code>.err</code> file produced by EnergyPlus:</p> <pre><code>from idfkit.simulation import simulate\n\nresult = simulate(model, weather)\n\n# Access the error report\nerrors = result.errors\n\n# Check for problems\nif errors.has_fatal:\n    print(\"Simulation had fatal errors\")\nif errors.has_severe:\n    print(\"Simulation had severe errors\")\nif errors.warning_count &gt; 0:\n    print(f\"Simulation had {errors.warning_count} warnings\")\n</code></pre>"},{"location":"simulation/errors/#error-severity-levels","title":"Error Severity Levels","text":"<p>EnergyPlus uses several error severity levels:</p> Level Description Effect Fatal Unrecoverable error Simulation stops immediately Severe Serious problem May cause incorrect results Warning Potential issue Simulation continues Info Informational No action needed"},{"location":"simulation/errors/#querying-errors","title":"Querying Errors","text":""},{"location":"simulation/errors/#by-severity","title":"By Severity","text":"<pre><code>errors = result.errors\n\n# Fatal errors (simulation stopped)\nfor err in errors.fatal:\n    print(f\"FATAL: {err.message}\")\n\n# Severe errors (may cause incorrect results)\nfor err in errors.severe:\n    print(f\"SEVERE: {err.message}\")\n\n# Warnings\nfor warn in errors.warnings:\n    print(f\"Warning: {warn.message}\")\n</code></pre>"},{"location":"simulation/errors/#error-counts","title":"Error Counts","text":"<pre><code>print(f\"Fatal: {errors.fatal_count}\")\nprint(f\"Severe: {errors.severe_count}\")\nprint(f\"Warnings: {errors.warning_count}\")\n</code></pre>"},{"location":"simulation/errors/#summary","title":"Summary","text":"<pre><code># Get a formatted summary string\nprint(errors.summary())\n# Output: \"0 Fatal, 2 Severe, 15 Warnings\"\n</code></pre>"},{"location":"simulation/errors/#errormessage-attributes","title":"ErrorMessage Attributes","text":"<p>Each error/warning is an <code>ErrorMessage</code> object:</p> Attribute Type Description <code>severity</code> <code>str</code> \"Fatal\", \"Severe\", \"Warning\", etc. <code>message</code> <code>str</code> The error message text"},{"location":"simulation/errors/#simulation-exceptions","title":"Simulation Exceptions","text":"<p>The <code>simulate()</code> function raises <code>SimulationError</code> for certain failures:</p> <pre><code>from idfkit.exceptions import SimulationError\n\ntry:\n    result = simulate(model, weather)\nexcept SimulationError as e:\n    print(f\"Simulation failed: {e}\")\n    print(f\"Exit code: {e.exit_code}\")\n    print(f\"Stderr: {e.stderr}\")\n</code></pre>"},{"location":"simulation/errors/#exception-cases","title":"Exception Cases","text":"Situation Exception Weather file not found <code>SimulationError</code> EnergyPlus not found <code>EnergyPlusNotFoundError</code> Timeout exceeded <code>SimulationError</code> (exit_code=None) OS error starting process <code>SimulationError</code>"},{"location":"simulation/errors/#timeout-handling","title":"Timeout Handling","text":"<pre><code>try:\n    result = simulate(model, weather, timeout=60.0)\nexcept SimulationError as e:\n    if e.exit_code is None:\n        print(\"Simulation timed out\")\n    else:\n        print(f\"Simulation failed with exit code {e.exit_code}\")\n</code></pre>"},{"location":"simulation/errors/#non-exception-failures","title":"Non-Exception Failures","text":"<p>Some simulation failures don't raise exceptions but return a result with <code>success=False</code>:</p> <pre><code>result = simulate(model, weather)\n\nif not result.success:\n    print(f\"Exit code: {result.exit_code}\")\n\n    # Check errors\n    if result.errors.has_fatal:\n        for err in result.errors.fatal:\n            print(f\"Fatal: {err.message}\")\n</code></pre>"},{"location":"simulation/errors/#batch-error-handling","title":"Batch Error Handling","text":"<p>In batch processing, individual failures don't stop the batch:</p> <pre><code>from idfkit.simulation import simulate_batch\n\nbatch = simulate_batch(jobs)\n\n# Check overall success\nif not batch.all_succeeded:\n    print(f\"{len(batch.failed)} jobs failed\")\n\n# Handle failures individually\nfor i, result in enumerate(batch):\n    if not result.success:\n        print(f\"Job {i} failed:\")\n        for err in result.errors.fatal:\n            print(f\"  {err.message}\")\n</code></pre>"},{"location":"simulation/errors/#common-energyplus-errors","title":"Common EnergyPlus Errors","text":""},{"location":"simulation/errors/#missing-required-input","title":"Missing Required Input","text":"<pre><code>** Severe  ** GetSurfaceData: BuildingSurface:Detailed=\"WALL_1\", Construction=\"EXTERIOR_WALL\" was not found.\n</code></pre> <p>Solution: Ensure all referenced objects exist in the model.</p>"},{"location":"simulation/errors/#invalid-weather-data","title":"Invalid Weather Data","text":"<pre><code>** Fatal  ** GetWeatherDataPeriods: Weather file has no data for requested period\n</code></pre> <p>Solution: Check that run period dates match weather file coverage.</p>"},{"location":"simulation/errors/#geometry-errors","title":"Geometry Errors","text":"<pre><code>** Severe  ** GetSurfaceData: Surface=\"WALL_1\" has zero or negative area\n</code></pre> <p>Solution: Fix surface vertex coordinates.</p>"},{"location":"simulation/errors/#schedule-errors","title":"Schedule Errors","text":"<pre><code>** Severe  ** GetSchedule: Schedule=\"OCCUPANCY\" was not found\n</code></pre> <p>Solution: Add the missing schedule or fix the reference.</p>"},{"location":"simulation/errors/#hvac-sizing","title":"HVAC Sizing","text":"<pre><code>** Severe  ** Sizing:Zone: Zone=\"ZONE_1\" has zero volume\n</code></pre> <p>Solution: Ensure zone geometry is properly enclosed.</p>"},{"location":"simulation/errors/#debugging-tips","title":"Debugging Tips","text":""},{"location":"simulation/errors/#1-check-the-error-report-first","title":"1. Check the Error Report First","text":"<pre><code>if not result.success:\n    print(errors.summary())\n    for err in errors.fatal + errors.severe:\n        print(err.message)\n</code></pre>"},{"location":"simulation/errors/#2-examine-raw-output","title":"2. Examine Raw Output","text":"<pre><code># Check stderr\nprint(result.stderr)\n\n# Check the run directory\nprint(f\"Outputs in: {result.run_dir}\")\n</code></pre>"},{"location":"simulation/errors/#3-run-design-day-first","title":"3. Run Design-Day First","text":"<p>Design-day simulations are faster and catch most errors:</p> <pre><code># Quick validation\nresult = simulate(model, weather, design_day=True)\nif result.success:\n    # Then run full annual\n    result = simulate(model, weather, annual=True)\n</code></pre>"},{"location":"simulation/errors/#4-validate-before-simulation","title":"4. Validate Before Simulation","text":"<pre><code>from idfkit import validate_document\n\nvalidation = validate_document(model)\nif not validation.is_valid:\n    for err in validation.errors:\n        print(err)\n</code></pre>"},{"location":"simulation/errors/#error-report-from-file","title":"Error Report from File","text":"<p>Parse an error file directly:</p> <pre><code>from idfkit.simulation import ErrorReport\n\nerrors = ErrorReport.from_file(\"/path/to/eplusout.err\")\nprint(errors.summary())\n</code></pre> <p>Or from string:</p> <pre><code>err_text = Path(\"eplusout.err\").read_text()\nerrors = ErrorReport.from_string(err_text)\n</code></pre>"},{"location":"simulation/errors/#see-also","title":"See Also","text":"<ul> <li>Running Simulations \u2014 Basic simulation guide</li> <li>Parsing Results \u2014 Working with SimulationResult</li> <li>Troubleshooting \u2014 Common error solutions</li> </ul>"},{"location":"simulation/output-discovery/","title":"Output Discovery","text":"<p>The <code>OutputVariableIndex</code> helps you discover available output variables and meters from EnergyPlus, then add them to your model for future simulations.</p>"},{"location":"simulation/output-discovery/#basic-usage","title":"Basic Usage","text":"<pre><code>from idfkit.simulation import simulate\n\nresult = simulate(model, weather)\n\nvariables = result.variables\nif variables is not None:\n    # Search for temperature-related outputs\n    matches = variables.search(\"Temperature\")\n    for var in matches[:10]:\n        print(f\"{var.name} [{var.units}]\")\n</code></pre>"},{"location":"simulation/output-discovery/#understanding-rdd-and-mdd-files","title":"Understanding RDD and MDD Files","text":"<p>EnergyPlus generates these files to describe available outputs:</p> File Contents <code>.rdd</code> Output variables (zone temps, surface temps, etc.) <code>.mdd</code> Output meters (energy consumption, etc.) <p>These files are only generated after a simulation runs \u2014 they describe what outputs could be requested, not what was actually recorded.</p>"},{"location":"simulation/output-discovery/#outputvariableindex","title":"OutputVariableIndex","text":""},{"location":"simulation/output-discovery/#creating-an-index","title":"Creating an Index","text":"<p>From simulation results:</p> <pre><code>variables = result.variables\n</code></pre> <p>From files directly:</p> <pre><code>from idfkit.simulation import OutputVariableIndex\n\nindex = OutputVariableIndex.from_files(\n    rdd_path=\"/path/to/eplusout.rdd\",\n    mdd_path=\"/path/to/eplusout.mdd\",\n)\n</code></pre>"},{"location":"simulation/output-discovery/#search-variables","title":"Search Variables","text":"<pre><code># Search by name pattern\nmatches = variables.search(\"Zone Mean Air Temperature\")\n\n# Search with regex\nmatches = variables.search(r\"Zone.*Temperature\")\n\n# Case-insensitive\nmatches = variables.search(\"temperature\")  # Finds all temperature vars\n</code></pre>"},{"location":"simulation/output-discovery/#filter-by-units","title":"Filter by Units","text":"<pre><code># Get all temperature variables (\u00b0C)\ntemp_vars = variables.filter_by_units(\"C\")\n\n# Get all energy variables\nenergy_vars = variables.filter_by_units(\"J\")\n</code></pre>"},{"location":"simulation/output-discovery/#list-all-variables","title":"List All Variables","text":"<pre><code># All output variables\nfor var in variables.variables:\n    print(f\"Variable: {var.name} [{var.units}]\")\n\n# All meters\nfor meter in variables.meters:\n    print(f\"Meter: {meter.name} [{meter.units}]\")\n</code></pre>"},{"location":"simulation/output-discovery/#outputvariable-and-outputmeter","title":"OutputVariable and OutputMeter","text":""},{"location":"simulation/output-discovery/#outputvariable-attributes","title":"OutputVariable Attributes","text":"Attribute Type Description <code>name</code> <code>str</code> Variable name <code>units</code> <code>str</code> Variable units <code>key_options</code> <code>str</code> Key types (e.g., \"Zone\", \"*\")"},{"location":"simulation/output-discovery/#outputmeter-attributes","title":"OutputMeter Attributes","text":"Attribute Type Description <code>name</code> <code>str</code> Meter name <code>units</code> <code>str</code> Meter units <code>resource_type</code> <code>str</code> Resource being measured <code>end_use</code> <code>str</code> End use category"},{"location":"simulation/output-discovery/#adding-outputs-to-model","title":"Adding Outputs to Model","text":""},{"location":"simulation/output-discovery/#add-all-matching","title":"Add All Matching","text":"<pre><code># Add all temperature outputs\ncount = variables.add_all_to_model(\n    model,\n    filter_pattern=\"Zone.*Temperature\",\n)\nprint(f\"Added {count} output requests\")\n</code></pre>"},{"location":"simulation/output-discovery/#selective-addition","title":"Selective Addition","text":"<pre><code># Search first, review, then add selectively\nmatches = variables.search(\"Heating\")\n\n# Filter to specific ones\nselected = [v for v in matches if \"Coil\" in v.name]\n\n# Add to model (name is optional for Output:Variable)\nfor var in selected:\n    model.add(\n        \"Output:Variable\",\n        key_value=\"*\",\n        variable_name=var.name,\n        reporting_frequency=\"Timestep\",\n    )\n</code></pre>"},{"location":"simulation/output-discovery/#reporting-frequencies","title":"Reporting Frequencies","text":"Frequency Description <code>\"Detailed\"</code> Every zone timestep <code>\"Timestep\"</code> Every zone timestep <code>\"Hourly\"</code> Once per hour <code>\"Daily\"</code> Once per day <code>\"Monthly\"</code> Once per month <code>\"RunPeriod\"</code> Once per run period <code>\"Environment\"</code> Once per environment <pre><code>variables.add_all_to_model(\n    model,\n    filter_pattern=\"Temperature\",\n    reporting_frequency=\"Hourly\",\n)\n</code></pre>"},{"location":"simulation/output-discovery/#workflow-discover-then-request","title":"Workflow: Discover Then Request","text":"<p>A common pattern is to run a \"discovery\" simulation to find available outputs, then run a second simulation with those outputs requested:</p> <pre><code>from idfkit.simulation import simulate\n\n# Step 1: Discovery run\nresult = simulate(model, weather, design_day=True)\n\n# Step 2: Find interesting outputs\nmatches = result.variables.search(\"Zone Mean Air Temperature\")\nprint(f\"Found {len(matches)} matching variables\")\n\n# Step 3: Add outputs to model\nresult.variables.add_all_to_model(\n    model,\n    filter_pattern=\"Zone Mean Air Temperature\",\n    reporting_frequency=\"Hourly\",\n)\n\n# Step 4: Full run with outputs\nresult = simulate(model, weather, annual=True)\n\n# Step 5: Query the data\nfor zone in [\"ZONE 1\", \"ZONE 2\"]:\n    ts = result.sql.get_timeseries(\n        \"Zone Mean Air Temperature\",\n        zone,\n    )\n    print(f\"{zone}: avg {sum(ts.values) / len(ts.values):.1f}\u00b0C\")\n</code></pre>"},{"location":"simulation/output-discovery/#common-output-variables","title":"Common Output Variables","text":""},{"location":"simulation/output-discovery/#zone-level","title":"Zone-Level","text":"Variable Description <code>Zone Mean Air Temperature</code> Average zone air temperature <code>Zone Air Relative Humidity</code> Zone relative humidity <code>Zone Air System Sensible Cooling Energy</code> Cooling energy delivered <code>Zone Air System Sensible Heating Energy</code> Heating energy delivered <code>Zone People Total Heating Energy</code> Heat from occupants <code>Zone Lights Total Heating Energy</code> Heat from lights <code>Zone Electric Equipment Total Heating Energy</code> Heat from equipment"},{"location":"simulation/output-discovery/#surface-level","title":"Surface-Level","text":"Variable Description <code>Surface Inside Face Temperature</code> Interior surface temperature <code>Surface Outside Face Temperature</code> Exterior surface temperature <code>Surface Inside Face Convection Heat Transfer Coefficient</code> Interior convection <code>Surface Outside Face Convection Heat Transfer Coefficient</code> Exterior convection"},{"location":"simulation/output-discovery/#hvac","title":"HVAC","text":"Variable Description <code>Zone Ideal Loads Supply Air Total Cooling Energy</code> Ideal loads cooling <code>Zone Ideal Loads Supply Air Total Heating Energy</code> Ideal loads heating <code>Facility Total Electric Demand Power</code> Total electric load"},{"location":"simulation/output-discovery/#common-meters","title":"Common Meters","text":"Meter Description <code>Electricity:Facility</code> Total facility electricity <code>Gas:Facility</code> Total facility gas <code>Heating:Electricity</code> Heating electricity <code>Cooling:Electricity</code> Cooling electricity <code>InteriorLights:Electricity</code> Interior lighting electricity <code>InteriorEquipment:Electricity</code> Interior equipment electricity"},{"location":"simulation/output-discovery/#see-also","title":"See Also","text":"<ul> <li>SQL Output Queries \u2014 Querying recorded data</li> <li>Parsing Results \u2014 Working with SimulationResult</li> <li>Running Simulations \u2014 Basic simulation guide</li> </ul>"},{"location":"simulation/plotting/","title":"Plotting","text":"<p>The simulation module provides pluggable plotting backends for visualizing results with matplotlib or plotly.</p>"},{"location":"simulation/plotting/#quick-start","title":"Quick Start","text":"<pre><code>from idfkit.simulation import simulate\n\nresult = simulate(model, weather)\n\n# Plot time series\nts = result.sql.get_timeseries(\n    \"Zone Mean Air Temperature\",\n    \"ZONE 1\",\n)\nfig = ts.plot()  # Auto-detects available backend\n</code></pre>"},{"location":"simulation/plotting/#installation","title":"Installation","text":"<p>Install a plotting backend:</p> <pre><code># Matplotlib (recommended for static plots)\npip install idfkit[plot]\n\n# Plotly (for interactive plots)\npip install idfkit[plotly]\n\n# Both\npip install idfkit[plot,plotly]\n</code></pre>"},{"location":"simulation/plotting/#built-in-visualizations","title":"Built-in Visualizations","text":""},{"location":"simulation/plotting/#temperature-profile","title":"Temperature Profile","text":"<pre><code>from idfkit.simulation import plot_temperature_profile\n\nfig = plot_temperature_profile(\n    result,\n    zone_name=\"THERMAL ZONE 1\",\n    title=\"Zone Temperatures\",\n)\n</code></pre>"},{"location":"simulation/plotting/#energy-balance","title":"Energy Balance","text":"<pre><code>from idfkit.simulation import plot_energy_balance\n\nfig = plot_energy_balance(\n    result,\n    title=\"Annual Energy Balance\",\n)\n</code></pre>"},{"location":"simulation/plotting/#comfort-hours","title":"Comfort Hours","text":"<pre><code>from idfkit.simulation import plot_comfort_hours\n\nfig = plot_comfort_hours(\n    result,\n    zone_name=\"THERMAL ZONE 1\",\n    title=\"Thermal Comfort Analysis\",\n)\n</code></pre>"},{"location":"simulation/plotting/#time-series-plotting","title":"Time Series Plotting","text":"<p><code>TimeSeriesResult</code> has a built-in <code>plot()</code> method:</p> <pre><code>ts = result.sql.get_timeseries(\n    \"Zone Mean Air Temperature\",\n    \"ZONE 1\",\n)\n\n# Default plot\nfig = ts.plot()\n\n# Custom title\nfig = ts.plot(title=\"My Custom Title\")\n\n# Explicit backend\nfrom idfkit.simulation import MatplotlibBackend\n\nfig = ts.plot(backend=MatplotlibBackend())\n</code></pre>"},{"location":"simulation/plotting/#backend-selection","title":"Backend Selection","text":""},{"location":"simulation/plotting/#auto-detection","title":"Auto-Detection","text":"<p>By default, the first available backend is used:</p> <pre><code>from idfkit.simulation import get_default_backend\n\nbackend = get_default_backend()\nprint(type(backend).__name__)  # MatplotlibBackend or PlotlyBackend\n</code></pre> <p>Priority: matplotlib \u2192 plotly</p>"},{"location":"simulation/plotting/#explicit-backend","title":"Explicit Backend","text":"<pre><code>from idfkit.simulation import MatplotlibBackend, PlotlyBackend\n\n# Force matplotlib\nfig = ts.plot(backend=MatplotlibBackend())\n\n# Force plotly\nfig = ts.plot(backend=PlotlyBackend())\n</code></pre>"},{"location":"simulation/plotting/#plotbackend-protocol","title":"PlotBackend Protocol","text":"<p>Create custom backends by implementing the <code>PlotBackend</code> protocol:</p> <pre><code>from idfkit.simulation import PlotBackend\n\n\nclass MyBackend(PlotBackend):\n    def line(\n        self,\n        x: list,\n        y: list,\n        *,\n        title: str = \"\",\n        xlabel: str = \"\",\n        ylabel: str = \"\",\n        label: str | None = None,\n    ):\n        # Return a figure object\n        ...\n\n    def bar(\n        self,\n        categories: list[str],\n        values: list[float],\n        *,\n        title: str = \"\",\n        xlabel: str = \"\",\n        ylabel: str = \"\",\n    ):\n        # Return a figure object\n        ...\n</code></pre>"},{"location":"simulation/plotting/#matplotlib-backend","title":"Matplotlib Backend","text":""},{"location":"simulation/plotting/#basic-usage","title":"Basic Usage","text":"<pre><code>from idfkit.simulation import MatplotlibBackend\n\nbackend = MatplotlibBackend()\nfig = backend.line(\n    x=list(ts.timestamps),\n    y=list(ts.values),\n    title=\"Zone Temperature\",\n    xlabel=\"Time\",\n    ylabel=\"Temperature (\u00b0C)\",\n)\n\n# Save to file\nfig.savefig(\"temperature.png\")\n</code></pre>"},{"location":"simulation/plotting/#customization","title":"Customization","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Create custom figure\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot multiple series\nfor zone_name in zone_names:\n    ts = result.sql.get_timeseries(\n        \"Zone Mean Air Temperature\",\n        zone_name,\n    )\n    ax.plot(ts.timestamps, ts.values, label=zone_name)\n\nax.legend()\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"Temperature (\u00b0C)\")\nplt.show()\n</code></pre>"},{"location":"simulation/plotting/#plotly-backend","title":"Plotly Backend","text":""},{"location":"simulation/plotting/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from idfkit.simulation import PlotlyBackend\n\nbackend = PlotlyBackend()\nfig = backend.line(\n    x=list(ts.timestamps),\n    y=list(ts.values),\n    title=\"Zone Temperature\",\n)\n\n# Interactive display\nfig.show()\n\n# Save to HTML\nfig.write_html(\"temperature.html\")\n</code></pre>"},{"location":"simulation/plotting/#customization_1","title":"Customization","text":"<pre><code>import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfor zone_name in zone_names:\n    ts = result.sql.get_timeseries(\n        \"Zone Mean Air Temperature\",\n        zone_name,\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=list(ts.timestamps),\n            y=list(ts.values),\n            name=zone_name,\n        )\n    )\n\nfig.update_layout(\n    title=\"Zone Temperatures\",\n    xaxis_title=\"Time\",\n    yaxis_title=\"Temperature (\u00b0C)\",\n)\nfig.show()\n</code></pre>"},{"location":"simulation/plotting/#dataframe-integration","title":"DataFrame Integration","text":"<p>Convert to pandas and use native plotting:</p> <pre><code># Get DataFrame\ndf = ts.to_dataframe()\n\n# Matplotlib via pandas\ndf.plot(figsize=(12, 6))\n\n# Plotly via pandas\nimport plotly.express as px\n\nfig = px.line(df.reset_index(), x=\"timestamp\", y=ts.variable_name)\n</code></pre>"},{"location":"simulation/plotting/#multiple-time-series","title":"Multiple Time Series","text":""},{"location":"simulation/plotting/#same-variable-multiple-keys","title":"Same Variable, Multiple Keys","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nfor zone_name in [\"ZONE 1\", \"ZONE 2\", \"ZONE 3\"]:\n    ts = result.sql.get_timeseries(\n        \"Zone Mean Air Temperature\",\n        zone_name,\n    )\n    ax.plot(ts.timestamps, ts.values, label=zone_name)\n\nax.legend()\nplt.show()\n</code></pre>"},{"location":"simulation/plotting/#different-variables","title":"Different Variables","text":"<pre><code>fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n\n# Temperature\nts_temp = result.sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\naxes[0].plot(ts_temp.timestamps, ts_temp.values)\naxes[0].set_ylabel(\"Temperature (\u00b0C)\")\n\n# Humidity\nts_rh = result.sql.get_timeseries(\"Zone Air Relative Humidity\", \"ZONE 1\")\naxes[1].plot(ts_rh.timestamps, ts_rh.values)\naxes[1].set_ylabel(\"Relative Humidity (%)\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"simulation/plotting/#saving-figures","title":"Saving Figures","text":""},{"location":"simulation/plotting/#matplotlib","title":"Matplotlib","text":"<pre><code>fig.savefig(\"plot.png\", dpi=300, bbox_inches=\"tight\")\nfig.savefig(\"plot.pdf\")\nfig.savefig(\"plot.svg\")\n</code></pre>"},{"location":"simulation/plotting/#plotly","title":"Plotly","text":"<pre><code>fig.write_html(\"plot.html\")\nfig.write_image(\"plot.png\")  # Requires kaleido\nfig.write_image(\"plot.pdf\")\n</code></pre>"},{"location":"simulation/plotting/#see-also","title":"See Also","text":"<ul> <li>SQL Output Queries \u2014 Getting time series data</li> <li>Parsing Results \u2014 Working with SimulationResult</li> <li>Examples: Parametric Study \u2014 Visualization examples</li> </ul>"},{"location":"simulation/progress/","title":"Simulation Progress Tracking","text":"<p>The <code>on_progress</code> callback provides real-time visibility into what EnergyPlus is doing during a simulation.  It fires for warmup iterations, simulation day changes, post-processing steps, and completion -- enabling progress bars, live logs, and remote monitoring.</p>"},{"location":"simulation/progress/#quick-start","title":"Quick Start","text":"<p>The fastest way to get a progress bar is the built-in tqdm integration:</p> <pre><code>pip install idfkit[progress]    # installs tqdm\n</code></pre> <pre><code>from idfkit import load_idf\nfrom idfkit.simulation import simulate\n\nmodel = load_idf(\"building.idf\")\nresult = simulate(model, \"weather.epw\", annual=True, on_progress=\"tqdm\")\n</code></pre> <p>That's it.  A tqdm progress bar appears in your terminal (or Jupyter notebook) and is automatically closed when the simulation finishes -- even on error.</p> <p>For full control, pass any callable instead:</p> <pre><code>from idfkit.simulation import simulate, SimulationProgress\n\n\ndef on_progress(event: SimulationProgress) -&gt; None:\n    if event.percent is not None:\n        print(f\"[{event.percent:5.1f}%] {event.phase}: {event.message}\")\n    else:\n        print(f\"[  ?  ] {event.phase}: {event.message}\")\n\n\nresult = simulate(model, \"weather.epw\", annual=True, on_progress=on_progress)\n</code></pre> <p>Output:</p> <pre><code>[  ?  ] initializing: Initializing New Environment Parameters\n[  ?  ] warmup: Warming up {1}\n[  ?  ] warmup: Warming up {2}\n[  ?  ] warmup: Warmup Complete\n[  0.0%] simulating: Starting Simulation at 01/01/2017 for AnnualRun from 01/01/2017 to 12/31/2017\n[  8.5%] simulating: Continuing Simulation at 02/01/2017 for AnnualRun\n[ 16.2%] simulating: Continuing Simulation at 03/01/2017 for AnnualRun\n...\n[ 91.5%] simulating: Continuing Simulation at 12/01/2017 for AnnualRun\n[  ?  ] postprocessing: Writing tabular output file results using comma format.\n[100.0%] complete: EnergyPlus Completed Successfully.\n</code></pre>"},{"location":"simulation/progress/#on_progress-parameter","title":"<code>on_progress</code> Parameter","text":"<p>All simulation functions accept <code>on_progress</code>:</p> Value Behavior <code>None</code> (default) No progress tracking.  Zero overhead -- uses the original <code>subprocess.run()</code> / <code>communicate()</code> code path. <code>\"tqdm\"</code> Built-in tqdm progress bar.  Auto-detects terminal vs Jupyter.  Requires <code>pip install idfkit[progress]</code>.  Supported by <code>simulate()</code> and <code>async_simulate()</code> only -- batch runners require a custom callback (see Batch Progress). Any <code>Callable[[SimulationProgress], None]</code> Your custom callback, called once per progress line. Any <code>async Callable</code> (async runner only) Async callback, awaited by the runner."},{"location":"simulation/progress/#simulationprogress","title":"SimulationProgress","text":"<p>Each callback invocation receives a <code>SimulationProgress</code> event:</p> Field Type Description <code>phase</code> <code>str</code> <code>\"initializing\"</code>, <code>\"warmup\"</code>, <code>\"simulating\"</code>, <code>\"postprocessing\"</code>, or <code>\"complete\"</code> <code>message</code> <code>str</code> Raw EnergyPlus stdout line (stripped) <code>percent</code> <code>float | None</code> Estimated 0-100 completion, or <code>None</code> when indeterminate <code>environment</code> <code>str | None</code> Current simulation environment name <code>warmup_day</code> <code>int | None</code> Current warmup iteration (1-based) <code>sim_day</code> <code>int | None</code> Current day-of-year (1-based) <code>sim_total_days</code> <code>int | None</code> Total simulation days when known <code>job_index</code> <code>int | None</code> Batch job index (only set in batch mode) <code>job_label</code> <code>str | None</code> Batch job label (only set in batch mode)"},{"location":"simulation/progress/#simulation-phases","title":"Simulation Phases","text":"Phase When <code>percent</code> <code>initializing</code> EnergyPlus starts a new environment <code>None</code> <code>warmup</code> Iterating warmup days until convergence <code>None</code> <code>simulating</code> Stepping through the simulation period <code>float</code> when period is known <code>postprocessing</code> Writing output files <code>None</code> <code>complete</code> Simulation finished successfully <code>100.0</code>"},{"location":"simulation/progress/#percentage-estimation","title":"Percentage Estimation","text":"<p>The <code>percent</code> field is estimated from the current simulation date relative to the run period.  It is only available during the <code>simulating</code> phase when EnergyPlus reports the simulation period (e.g. annual runs).</p> <p>When the period cannot be determined (design-day runs, custom periods without date ranges), <code>percent</code> is <code>None</code>.  Your progress indicator should handle this with a spinner or indeterminate bar.</p>"},{"location":"simulation/progress/#built-in-tqdm-progress-bar","title":"Built-in tqdm Progress Bar","text":""},{"location":"simulation/progress/#one-liner","title":"One-liner","text":"<pre><code>result = simulate(model, \"weather.epw\", annual=True, on_progress=\"tqdm\")\n</code></pre> <p>The <code>\"tqdm\"</code> shorthand:</p> <ul> <li>Creates a tqdm bar with sensible defaults (percentage, elapsed, ETA)</li> <li>Uses <code>tqdm.auto</code> so it works in terminals, Jupyter notebooks, and IPython</li> <li>Automatically closes the bar when the simulation finishes (including on error)</li> <li>Requires <code>pip install idfkit[progress]</code> -- raises a clear <code>ImportError</code> if missing</li> </ul>"},{"location":"simulation/progress/#customising-the-tqdm-bar","title":"Customising the tqdm bar","text":"<p>For more control over the bar appearance, use the <code>tqdm_progress()</code> context manager directly:</p> <pre><code>from idfkit.simulation import simulate\nfrom idfkit.simulation.progress_bars import tqdm_progress\n\nwith tqdm_progress(\n    desc=\"Annual run\",\n    bar_format=\"{l_bar}{bar:30}| {n:.0f}% [{elapsed}&lt;{remaining}]\",\n    leave=False,  # Remove bar after completion\n    position=1,  # For nested bars\n) as cb:\n    result = simulate(model, \"weather.epw\", annual=True, on_progress=cb)\n</code></pre> <p><code>tqdm_progress()</code> is a context manager that yields a callback.  The bar is automatically closed when the <code>with</code> block exits (even on exception).  All keyword arguments are forwarded to <code>tqdm.tqdm</code>, so you have full control over colours, file output, miniters, etc.</p>"},{"location":"simulation/progress/#building-your-own-progress-indicator","title":"Building Your Own Progress Indicator","text":"<p>The examples below show how to build custom <code>on_progress</code> callbacks for different use cases.  Each example is a self-contained recipe you can adapt.</p>"},{"location":"simulation/progress/#rich-console","title":"rich (Console)","text":"<p>rich provides beautiful terminal output with spinners, colours, and multi-column layouts.</p> <pre><code>from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn\nfrom idfkit.simulation import simulate, SimulationProgress\n\nwith Progress(\n    SpinnerColumn(),\n    TextColumn(\"[bold blue]{task.description}\"),\n    BarColumn(),\n    TextColumn(\"{task.percentage:&gt;3.0f}%\"),\n    TextColumn(\"[dim]{task.fields[phase]}\"),\n) as progress:\n    task = progress.add_task(\"Simulating\", total=100, phase=\"starting\")\n\n    def on_progress(event: SimulationProgress) -&gt; None:\n        if event.percent is not None:\n            progress.update(task, completed=event.percent, phase=event.phase)\n        else:\n            progress.update(task, phase=event.phase)\n\n    result = simulate(model, \"weather.epw\", annual=True, on_progress=on_progress)\n</code></pre> <p>Batch with rich -- multiple bars, one per concurrent job:</p> <pre><code>from rich.progress import Progress\nfrom idfkit.simulation import simulate_batch, SimulationProgress\nimport threading\n\nlock = threading.Lock()\n\nwith Progress() as progress:\n    tasks = {}  # job_index -&gt; task_id\n\n    def on_progress(event: SimulationProgress) -&gt; None:\n        with lock:\n            if event.job_index not in tasks:\n                tasks[event.job_index] = progress.add_task(\n                    event.job_label or f\"Job {event.job_index}\",\n                    total=100,\n                )\n            task_id = tasks[event.job_index]\n        if event.percent is not None:\n            progress.update(task_id, completed=event.percent)\n        progress.update(task_id, description=f\"{event.job_label}: {event.phase}\")\n\n    batch = simulate_batch(jobs, on_progress=on_progress, max_workers=4)\n</code></pre>"},{"location":"simulation/progress/#jupyter-ipywidgets","title":"Jupyter (ipywidgets)","text":"<pre><code>import ipywidgets as widgets\nfrom IPython.display import display\nfrom idfkit.simulation import simulate, SimulationProgress\n\nbar = widgets.FloatProgress(min=0, max=100, description=\"Simulating:\")\nlabel = widgets.Label(value=\"Starting...\")\ndisplay(widgets.HBox([bar, label]))\n\n\ndef on_progress(event: SimulationProgress) -&gt; None:\n    if event.percent is not None:\n        bar.value = event.percent\n    label.value = f\"{event.phase}: {event.message[:60]}\"\n\n\nresult = simulate(model, \"weather.epw\", annual=True, on_progress=on_progress)\nbar.value = 100\nlabel.value = \"Done!\"\n</code></pre> <p>Tip</p> <p>The <code>\"tqdm\"</code> shorthand also works in Jupyter -- <code>tqdm.auto</code> renders as a native Jupyter widget automatically.</p>"},{"location":"simulation/progress/#structured-logging","title":"Structured Logging","text":"<p>Emit structured log entries for observability platforms (Datadog, ELK, etc.):</p> <pre><code>import logging\nfrom idfkit.simulation import simulate, SimulationProgress\n\nlogger = logging.getLogger(\"simulation\")\n\n\ndef on_progress(event: SimulationProgress) -&gt; None:\n    logger.info(\n        \"simulation_progress\",\n        extra={\n            \"phase\": event.phase,\n            \"percent\": event.percent,\n            \"environment\": event.environment,\n            \"message\": event.message,\n        },\n    )\n\n\nresult = simulate(model, \"weather.epw\", on_progress=on_progress)\n</code></pre>"},{"location":"simulation/progress/#simple-console-log","title":"Simple Console Log","text":"<pre><code>from idfkit.simulation import simulate, SimulationProgress\n\n\ndef on_progress(event: SimulationProgress) -&gt; None:\n    match event.phase:\n        case \"warmup\":\n            print(f\"  Warmup iteration {event.warmup_day}\")\n        case \"simulating\":\n            pct = f\"{event.percent:.0f}%\" if event.percent else \"?\"\n            print(f\"  [{pct}] Simulating {event.environment}\")\n        case \"complete\":\n            print(\"  Simulation complete!\")\n\n\nresult = simulate(model, \"weather.epw\", on_progress=on_progress)\n</code></pre>"},{"location":"simulation/progress/#websocket-forwarding","title":"WebSocket Forwarding","text":"<p>Forward progress events to a web client for real-time dashboards. Use an async callback so WebSocket sends don't block the event loop:</p> <pre><code>import json\nfrom idfkit.simulation import async_simulate, SimulationProgress\n\n\nasync def run_with_websocket(model, weather, websocket):\n    \"\"\"Run a simulation and forward progress over WebSocket.\"\"\"\n\n    async def on_progress(event: SimulationProgress) -&gt; None:\n        await websocket.send_text(\n            json.dumps({\n                \"type\": \"simulation_progress\",\n                \"phase\": event.phase,\n                \"percent\": event.percent,\n                \"message\": event.message,\n                \"environment\": event.environment,\n            })\n        )\n\n    result = await async_simulate(model, weather, on_progress=on_progress)\n    await websocket.send_text(\n        json.dumps({\n            \"type\": \"simulation_complete\",\n            \"success\": result.success,\n            \"runtime\": result.runtime_seconds,\n        })\n    )\n    return result\n</code></pre>"},{"location":"simulation/progress/#fastapi-websocket","title":"FastAPI + WebSocket","text":"<p>A complete FastAPI endpoint that streams progress to a browser:</p> <pre><code>from fastapi import FastAPI, WebSocket\nfrom idfkit import load_idf\nfrom idfkit.simulation import async_simulate, SimulationProgress\n\napp = FastAPI()\n\n\n@app.websocket(\"/ws/simulate\")\nasync def simulate_ws(websocket: WebSocket):\n    await websocket.accept()\n    data = await websocket.receive_json()\n\n    model = load_idf(data[\"idf_path\"])\n\n    async def on_progress(event: SimulationProgress) -&gt; None:\n        await websocket.send_json({\n            \"phase\": event.phase,\n            \"percent\": event.percent,\n            \"message\": event.message,\n        })\n\n    result = await async_simulate(\n        model,\n        data[\"weather_path\"],\n        on_progress=on_progress,\n    )\n\n    await websocket.send_json({\n        \"phase\": \"done\",\n        \"success\": result.success,\n        \"runtime\": result.runtime_seconds,\n    })\n    await websocket.close()\n</code></pre> <p>JavaScript client:</p> <pre><code>const ws = new WebSocket(\"ws://localhost:8000/ws/simulate\");\nws.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n    if (data.phase === \"done\") {\n        console.log(`Simulation ${data.success ? \"succeeded\" : \"failed\"}`);\n    } else {\n        updateProgressBar(data.percent);\n        updateStatusText(`${data.phase}: ${data.message}`);\n    }\n};\nws.send(JSON.stringify({ idf_path: \"building.idf\", weather_path: \"weather.epw\" }));\n</code></pre>"},{"location":"simulation/progress/#server-sent-events-sse","title":"Server-Sent Events (SSE)","text":"<p>For one-way streaming without WebSocket overhead (ideal for dashboards):</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom idfkit import load_idf\nfrom idfkit.simulation import async_simulate, SimulationProgress\nimport asyncio\nimport json\n\napp = FastAPI()\n\n\n@app.get(\"/api/simulate/stream\")\nasync def simulate_stream(idf_path: str, weather_path: str):\n    queue: asyncio.Queue[str] = asyncio.Queue()\n\n    async def on_progress(event: SimulationProgress) -&gt; None:\n        data = json.dumps({\n            \"phase\": event.phase,\n            \"percent\": event.percent,\n            \"message\": event.message,\n        })\n        await queue.put(f\"data: {data}\\n\\n\")\n\n    async def generate():\n        model = load_idf(idf_path)\n        task = asyncio.create_task(async_simulate(model, weather_path, on_progress=on_progress))\n        while not task.done():\n            try:\n                chunk = await asyncio.wait_for(queue.get(), timeout=0.5)\n                yield chunk\n            except asyncio.TimeoutError:\n                yield \": keepalive\\n\\n\"\n        result = await task\n        yield f\"data: {json.dumps({'phase': 'done', 'success': result.success})}\\n\\n\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre>"},{"location":"simulation/progress/#cloud-logging-aws-cloudwatch-gcp-cloud-logging","title":"Cloud Logging (AWS CloudWatch / GCP Cloud Logging)","text":"<p>For cloud-deployed simulations, forward events to your cloud logging service:</p> <pre><code>import json\nimport logging\nfrom dataclasses import asdict\nfrom idfkit.simulation import simulate, SimulationProgress\n\n# Configure for JSON-structured cloud logging\nlogger = logging.getLogger(\"energyplus.progress\")\n\n\ndef on_progress(event: SimulationProgress) -&gt; None:\n    # asdict() makes SimulationProgress JSON-serializable\n    logger.info(json.dumps(asdict(event)))\n\n\nresult = simulate(model, \"weather.epw\", on_progress=on_progress)\n</code></pre> <p>With a message queue (Redis, RabbitMQ, SQS):</p> <pre><code>from dataclasses import asdict\nimport json\nfrom idfkit.simulation import simulate, SimulationProgress\n\n\ndef make_queue_callback(queue_client, channel: str):\n    \"\"\"Create a callback that publishes events to a message queue.\"\"\"\n\n    def on_progress(event: SimulationProgress) -&gt; None:\n        queue_client.publish(channel, json.dumps(asdict(event)))\n\n    return on_progress\n\n\ncb = make_queue_callback(redis_client, \"sim:progress:run-001\")\nresult = simulate(model, \"weather.epw\", on_progress=cb)\n</code></pre>"},{"location":"simulation/progress/#async-callbacks","title":"Async Callbacks","text":"<p><code>async_simulate()</code> accepts both sync and async callables:</p> <pre><code>from idfkit.simulation import async_simulate, SimulationProgress\n\n\nasync def on_progress(event: SimulationProgress) -&gt; None:\n    \"\"\"Async callback -- awaited by the runner.\"\"\"\n    await websocket.send_json({\n        \"phase\": event.phase,\n        \"percent\": event.percent,\n        \"message\": event.message,\n    })\n\n\nasync def main():\n    result = await async_simulate(model, \"weather.epw\", on_progress=on_progress)\n</code></pre> <p>Synchronous callbacks also work in the async runner and are called directly without awaiting:</p> <pre><code># This works too -- no need to make it async for simple logging\nresult = await async_simulate(model, \"weather.epw\", on_progress=lambda e: print(e.phase))\n</code></pre>"},{"location":"simulation/progress/#batch-progress","title":"Batch Progress","text":"<p>In batch mode, <code>on_progress</code> fires for every simulation in the batch. Events include <code>job_index</code> and <code>job_label</code> to identify which job they belong to.</p>"},{"location":"simulation/progress/#dual-progress-tracking","title":"Dual Progress Tracking","text":"<p>Use <code>on_progress</code> for intra-simulation progress and <code>progress</code> for job-level completion -- they are independent and complementary:</p> <pre><code>from idfkit.simulation import simulate_batch, SimulationJob, SimulationProgress\n\njobs = [SimulationJob(model=variant, weather=\"weather.epw\", label=f\"case-{i}\") for i, variant in enumerate(variants)]\n\n\ndef on_sim_progress(event: SimulationProgress) -&gt; None:\n    \"\"\"Fires during each simulation (warmup, simulating, etc.).\"\"\"\n    if event.percent is not None:\n        print(f\"  Job {event.job_index} ({event.job_label}): {event.percent:.0f}%\")\n\n\ndef on_job_complete(completed, total, label, success):\n    \"\"\"Fires when each job finishes.\"\"\"\n    status = \"OK\" if success else \"FAIL\"\n    print(f\"[{completed}/{total}] {label}: {status}\")\n\n\nbatch = simulate_batch(\n    jobs,\n    on_progress=on_sim_progress,\n    progress=on_job_complete,\n    max_workers=4,\n)\n</code></pre>"},{"location":"simulation/progress/#batch-progress-bar-with-tqdm","title":"Batch Progress Bar with tqdm","text":"<p>For batch simulations, the <code>\"tqdm\"</code> shorthand is not supported because a single progress bar cannot meaningfully represent multiple concurrent jobs. Instead, build per-job bars manually:</p> <pre><code>from tqdm import tqdm\nfrom idfkit.simulation import simulate_batch, SimulationProgress\n\njobs = [...]\n\n# Job-level progress bar\noverall = tqdm(total=len(jobs), desc=\"Batch\", position=0)\n\n# Sim-level progress bar (resets per job)\ncurrent = tqdm(total=100, desc=\"Current\", position=1, leave=False)\n\n\ndef on_progress(event: SimulationProgress) -&gt; None:\n    if event.percent is not None:\n        current.n = event.percent\n        current.refresh()\n    current.set_postfix_str(event.job_label or \"\")\n\n\ndef on_job_complete(completed, total, label, success):\n    overall.update(1)\n    current.n = 0\n    current.refresh()\n\n\nbatch = simulate_batch(\n    jobs,\n    on_progress=on_progress,\n    progress=on_job_complete,\n    max_workers=4,\n)\n\noverall.close()\ncurrent.close()\n</code></pre>"},{"location":"simulation/progress/#async-batch-with-stream-progress","title":"Async Batch with Stream + Progress","text":"<p>Combine <code>async_simulate_batch_stream</code> (job-level events) with <code>on_progress</code> (intra-simulation events):</p> <pre><code>import asyncio\nfrom idfkit.simulation import (\n    async_simulate_batch_stream,\n    SimulationJob,\n    SimulationProgress,\n)\n\n\nasync def main():\n    jobs = [\n        SimulationJob(model=variant, weather=\"weather.epw\", label=f\"case-{i}\") for i, variant in enumerate(variants)\n    ]\n\n    def on_sim_progress(event: SimulationProgress) -&gt; None:\n        if event.percent is not None:\n            print(f\"  [{event.job_label}] {event.percent:.0f}%\")\n\n    async for event in async_simulate_batch_stream(\n        jobs,\n        max_concurrent=4,\n        on_progress=on_sim_progress,\n    ):\n        status = \"OK\" if event.result.success else \"FAIL\"\n        print(f\"[{event.completed}/{event.total}] {event.label}: {status}\")\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"simulation/progress/#using-progressparser-directly","title":"Using ProgressParser Directly","text":"<p>The <code>ProgressParser</code> class can be used independently to parse EnergyPlus stdout output -- useful for custom integrations or when processing log files from previous simulation runs:</p> <pre><code>from idfkit.simulation import ProgressParser\n\nparser = ProgressParser()\n\n# Parse a log file\nwith open(\"energyplus_stdout.log\") as f:\n    for line in f:\n        event = parser.parse_line(line)\n        if event is not None:\n            print(f\"{event.phase}: {event.message}\")\n</code></pre> <p>The parser is stateful (it tracks environment transitions and warmup counters), so use a fresh instance for each simulation. Non-progress lines return <code>None</code> and never raise exceptions.</p>"},{"location":"simulation/progress/#cloud-execution","title":"Cloud Execution","text":"<p>When using the <code>fs</code> parameter for remote storage, progress callbacks fire during the local EnergyPlus execution -- before results are uploaded.  This works identically to local execution:</p> <pre><code>from idfkit.simulation import simulate, S3FileSystem, SimulationProgress\n\nfs = S3FileSystem(bucket=\"my-bucket\", prefix=\"runs/\")\n\n\ndef on_progress(event: SimulationProgress) -&gt; None:\n    # This fires during local execution, before upload\n    print(f\"{event.phase}: {event.percent}\")\n\n\nresult = simulate(\n    model,\n    \"weather.epw\",\n    output_dir=\"run-001\",\n    fs=fs,\n    on_progress=on_progress,\n)\n</code></pre> <p>For remote execution scenarios (where EnergyPlus runs on a different machine), use the async callback to forward events over a transport layer (WebSocket, SSE, message queue). The <code>SimulationProgress</code> dataclass is JSON-serializable via <code>dataclasses.asdict()</code>:</p> <pre><code>from dataclasses import asdict\nimport json\n\n\ndef on_progress(event: SimulationProgress) -&gt; None:\n    message_queue.publish(json.dumps(asdict(event)))\n</code></pre>"},{"location":"simulation/progress/#behavior-notes","title":"Behavior Notes","text":"<ul> <li> <p>No callback, no overhead: When <code>on_progress</code> is not provided, the   original <code>subprocess.run()</code> / <code>proc.communicate()</code> code paths are used   with no performance impact.</p> </li> <li> <p>Automatic cleanup: When using <code>on_progress=\"tqdm\"</code>, the progress bar   is always closed -- even if the simulation raises an exception.  On error,   the bar preserves its last reported value instead of jumping to 100%.</p> </li> <li> <p>Callback exceptions: If your callback raises an exception, the   simulation is killed and <code>SimulationError</code> is raised.</p> </li> <li> <p>Thread safety (batch): In <code>simulate_batch()</code>, the <code>on_progress</code>   callback may be called from multiple threads concurrently. If your   callback writes to shared state, ensure it is thread-safe (e.g. use a   lock or thread-safe data structures).</p> </li> <li> <p>Indeterminate phases: During warmup and post-processing, <code>percent</code>   is <code>None</code>. Your progress indicator should handle this gracefully --   show a spinner or simply log the phase name.</p> </li> </ul>"},{"location":"simulation/progress/#api-reference","title":"API Reference","text":""},{"location":"simulation/progress/#functions","title":"Functions","text":"Function <code>on_progress</code> Support <code>simulate()</code> <code>\"tqdm\"</code>, sync callback, or <code>None</code> <code>async_simulate()</code> <code>\"tqdm\"</code>, sync/async callback, or <code>None</code> <code>simulate_batch()</code> Sync callback or <code>None</code> (events include <code>job_index</code>/<code>job_label</code>) <code>async_simulate_batch()</code> Sync/async callback or <code>None</code> (events include <code>job_index</code>/<code>job_label</code>) <code>async_simulate_batch_stream()</code> Sync/async callback or <code>None</code> (events include <code>job_index</code>/<code>job_label</code>)"},{"location":"simulation/progress/#classes-factories","title":"Classes / Factories","text":"Name Description <code>SimulationProgress</code> Frozen dataclass for progress events <code>ProgressParser</code> Stateful EnergyPlus stdout line parser <code>tqdm_progress()</code> Context manager yielding a callback for customised tqdm bars"},{"location":"simulation/progress/#see-also","title":"See Also","text":"<ul> <li>Running Simulations -- Full <code>simulate()</code> parameter reference</li> <li>Async Simulation -- Non-blocking execution guide</li> <li>Batch Processing -- Parallel execution guide</li> </ul>"},{"location":"simulation/results/","title":"Parsing Results","text":"<p>The <code>SimulationResult</code> class provides structured access to all EnergyPlus output files with lazy loading for efficient memory usage.</p>"},{"location":"simulation/results/#simulationresult-overview","title":"SimulationResult Overview","text":"<pre><code>from idfkit.simulation import simulate\n\nresult = simulate(model, weather)\n\n# Basic info\nprint(f\"Success: {result.success}\")\nprint(f\"Exit code: {result.exit_code}\")\nprint(f\"Runtime: {result.runtime_seconds:.1f}s\")\nprint(f\"Output dir: {result.run_dir}\")\n\n# Parsed outputs (lazy-loaded)\nresult.errors  # ErrorReport from .err file\nresult.sql  # SQLResult from .sql database\nresult.variables  # OutputVariableIndex from .rdd/.mdd\nresult.csv  # CSVResult from .csv file\nresult.html  # HTMLResult from HTML tabular output\n</code></pre>"},{"location":"simulation/results/#output-file-paths","title":"Output File Paths","text":"<p>Access paths to specific output files:</p> <pre><code>result.sql_path  # Path to .sql database\nresult.err_path  # Path to .err file\nresult.eso_path  # Path to .eso file\nresult.csv_path  # Path to .csv file\nresult.html_path  # Path to HTML table file\nresult.rdd_path  # Path to .rdd file\nresult.mdd_path  # Path to .mdd file\n</code></pre> <p>Each returns <code>None</code> if the file wasn't produced.</p>"},{"location":"simulation/results/#error-report","title":"Error Report","text":"<p>Parse warnings and errors from the <code>.err</code> file:</p> <pre><code>errors = result.errors\n\n# Summary\nprint(errors.summary())\n\n# Check for fatal errors\nif errors.has_fatal:\n    for err in errors.fatal:\n        print(f\"FATAL: {err.message}\")\n\n# Check for severe errors\nif errors.has_severe:\n    for err in errors.severe:\n        print(f\"SEVERE: {err.message}\")\n\n# All warnings\nfor warn in errors.warnings:\n    print(f\"Warning: {warn.message}\")\n\n# Counts\nprint(f\"Fatal: {errors.fatal_count}\")\nprint(f\"Severe: {errors.severe_count}\")\nprint(f\"Warnings: {errors.warning_count}\")\n</code></pre> <p>See Error Handling for detailed error parsing.</p>"},{"location":"simulation/results/#sql-database","title":"SQL Database","text":"<p>Query time-series and tabular data from the SQLite output:</p> <pre><code>sql = result.sql\nif sql is not None:\n    # Time-series data\n    ts = sql.get_timeseries(\n        variable_name=\"Zone Mean Air Temperature\",\n        key_value=\"THERMAL ZONE 1\",\n    )\n    print(f\"Max: {max(ts.values):.1f}\u00b0C\")\n\n    # Tabular reports\n    rows = sql.get_tabular_data(report_name=\"AnnualBuildingUtilityPerformanceSummary\")\n</code></pre> <p>See SQL Output Queries for detailed SQL parsing.</p>"},{"location":"simulation/results/#output-variables","title":"Output Variables","text":"<p>Discover available output variables from <code>.rdd</code>/<code>.mdd</code> files:</p> <pre><code>variables = result.variables\nif variables is not None:\n    # Search for variables\n    matches = variables.search(\"Temperature\")\n    for var in matches:\n        print(f\"{var.name} [{var.units}]\")\n\n    # Add outputs to model for next run\n    variables.add_all_to_model(model, filter_pattern=\"Zone.*Temperature\")\n</code></pre> <p>See Output Discovery for variable discovery.</p>"},{"location":"simulation/results/#csv-output","title":"CSV Output","text":"<p>Parse CSV time-series output:</p> <pre><code>csv_result = result.csv\nif csv_result is not None:\n    # List all columns\n    for col in csv_result.columns:\n        print(f\"{col.variable_name} ({col.key_value}) [{col.units}]\")\n\n    # Get data for a specific column\n    values = csv_result.get_column_values(\"Zone Mean Air Temperature\")\n</code></pre>"},{"location":"simulation/results/#html-tabular-output","title":"HTML Tabular Output","text":"<p>Parse the HTML tabular summary file (<code>eplustbl.htm</code>) that EnergyPlus produces alongside every simulation:</p> <pre><code>html = result.html\nif html is not None:\n    # Iterate all tables\n    for table in html:\n        print(f\"{table.title}: {len(table.rows)} rows\")\n\n    # eppy-compatible (title, rows) pairs\n    for title, rows in html.titletable():\n        print(title)\n\n    # Look up a table by title (case-insensitive substring match)\n    table = html.tablebyname(\"Site and Source Energy\")\n    if table:\n        data = table.to_dict()  # {row_key: {col_header: value}}\n        print(data)\n\n    # Get all tables from a specific report\n    annual = html.tablesbyreport(\"Annual Building Utility Performance Summary\")\n\n    # Access by index\n    first = html.tablebyindex(0)\n</code></pre> <p>Each <code>HTMLTable</code> has these attributes:</p> Attribute Type Description <code>title</code> <code>str</code> Bold title preceding the table <code>header</code> <code>list[str]</code> Column headers <code>rows</code> <code>list[list[str]]</code> Data rows <code>report_name</code> <code>str</code> Parent report name <code>for_string</code> <code>str</code> The \"For:\" qualifier (e.g. \"Entire Facility\") <p>You can also parse a standalone HTML file without a full simulation:</p> <pre><code>from idfkit.simulation.parsers.html import HTMLResult\n\nhtml = HTMLResult.from_file(\"eplustbl.htm\")\nhtml = HTMLResult.from_string(html_string)\n</code></pre> <p>This replaces eppy's <code>readhtml</code> module.</p>"},{"location":"simulation/results/#lazy-loading","title":"Lazy Loading","text":"<p>Output files are parsed only when accessed:</p> <pre><code>result = simulate(model, weather)\n\n# Nothing parsed yet - only metadata stored\n\nresult.errors  # NOW parses .err file\nresult.sql  # NOW opens SQLite database\nresult.variables  # NOW parses .rdd/.mdd files\nresult.html  # NOW parses HTML tabular output\n</code></pre> <p>This keeps memory usage low, especially for batch simulations where you might only need specific outputs.</p>"},{"location":"simulation/results/#reconstructing-from-directory","title":"Reconstructing from Directory","text":"<p>Inspect results from a previous simulation:</p> <pre><code>from idfkit.simulation import SimulationResult\n\n# From a local directory\nresult = SimulationResult.from_directory(\"/path/to/sim_output\")\n\n# From a cloud storage location\nfrom idfkit.simulation import S3FileSystem\n\nfs = S3FileSystem(bucket=\"my-bucket\")\nresult = SimulationResult.from_directory(\"runs/run-001\", fs=fs)\n\n# Query data\nts = result.sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\n</code></pre>"},{"location":"simulation/results/#attributes-reference","title":"Attributes Reference","text":"Attribute Type Description <code>run_dir</code> <code>Path</code> Directory containing output files <code>success</code> <code>bool</code> Whether simulation succeeded <code>exit_code</code> <code>int | None</code> Process exit code (None if timed out) <code>stdout</code> <code>str</code> Captured standard output <code>stderr</code> <code>str</code> Captured standard error <code>runtime_seconds</code> <code>float</code> Wall-clock execution time <code>output_prefix</code> <code>str</code> Output file prefix (default \"eplus\")"},{"location":"simulation/results/#properties-reference","title":"Properties Reference","text":"Property Type Description <code>errors</code> <code>ErrorReport</code> Parsed error/warning report <code>sql</code> <code>SQLResult | None</code> SQL database accessor <code>variables</code> <code>OutputVariableIndex | None</code> Variable discovery <code>csv</code> <code>CSVResult | None</code> CSV output parser <code>html</code> <code>HTMLResult | None</code> HTML tabular output parser <code>sql_path</code> <code>Path | None</code> Path to .sql file <code>err_path</code> <code>Path | None</code> Path to .err file <code>eso_path</code> <code>Path | None</code> Path to .eso file <code>csv_path</code> <code>Path | None</code> Path to .csv file <code>html_path</code> <code>Path | None</code> Path to HTML file <code>rdd_path</code> <code>Path | None</code> Path to .rdd file <code>mdd_path</code> <code>Path | None</code> Path to .mdd file"},{"location":"simulation/results/#see-also","title":"See Also","text":"<ul> <li>SQL Output Queries \u2014 Detailed SQL database access</li> <li>Output Discovery \u2014 Finding available variables</li> <li>Error Handling \u2014 Parsing error reports</li> </ul>"},{"location":"simulation/running/","title":"Running Simulations","text":"<p>The <code>simulate()</code> function executes EnergyPlus as a subprocess and returns a structured <code>SimulationResult</code> with access to all output files.</p>"},{"location":"simulation/running/#basic-usage","title":"Basic Usage","text":"<pre><code>from idfkit import load_idf\nfrom idfkit.simulation import simulate\n\nmodel = load_idf(\"building.idf\")\nresult = simulate(model, \"weather.epw\")\n\nprint(f\"Success: {result.success}\")\nprint(f\"Runtime: {result.runtime_seconds:.1f}s\")\nprint(f\"Output directory: {result.run_dir}\")\n</code></pre>"},{"location":"simulation/running/#simulation-modes","title":"Simulation Modes","text":""},{"location":"simulation/running/#design-day-only","title":"Design-Day Only","text":"<p>Fast simulation using only design day conditions:</p> <pre><code>result = simulate(model, weather, design_day=True)\n</code></pre>"},{"location":"simulation/running/#annual-simulation","title":"Annual Simulation","text":"<p>Full-year simulation:</p> <pre><code>result = simulate(model, weather, annual=True)\n</code></pre>"},{"location":"simulation/running/#default-mode","title":"Default Mode","text":"<p>Without flags, EnergyPlus uses whatever run periods are defined in the model:</p> <pre><code>result = simulate(model, weather)  # Uses model's RunPeriod objects\n</code></pre>"},{"location":"simulation/running/#preprocessing","title":"Preprocessing","text":"<p>Some EnergyPlus models contain high-level template objects that must be expanded into their low-level equivalents before simulation.  idfkit provides standalone preprocessing functions for this.</p>"},{"location":"simulation/running/#expanding-hvac-templates","title":"Expanding HVAC Templates","text":"<p><code>HVACTemplate:*</code> objects are shorthand for complex HVAC systems. <code>expand_objects()</code> converts them into their fully specified equivalents:</p> <pre><code>from idfkit.simulation import expand_objects\n\nexpanded = expand_objects(model)\n# HVACTemplate:Zone:IdealLoadsAirSystem \u2192 ZoneHVAC:IdealLoadsAirSystem + ...\n</code></pre> <p>Note</p> <p><code>simulate()</code> runs ExpandObjects automatically when <code>expand_objects=True</code> (the default).  Call <code>expand_objects()</code> directly only when you need to inspect or modify the expanded model before simulation.</p>"},{"location":"simulation/running/#ground-heat-transfer-slab-basement","title":"Ground Heat Transfer (Slab &amp; Basement)","text":"<p>Models with <code>GroundHeatTransfer:Slab:*</code> or <code>GroundHeatTransfer:Basement:*</code> objects need the Slab or Basement preprocessor to compute ground temperatures.</p> <p>Note</p> <p><code>simulate()</code> automatically runs the Slab and/or Basement preprocessors when <code>expand_objects=True</code> (the default) and the model contains the corresponding ground heat-transfer objects.  In most cases you do not need to call these functions yourself.</p> <p>For cases where you need to inspect or modify the preprocessed model before simulation, standalone functions are available:</p> <pre><code>from idfkit.simulation import run_slab_preprocessor, run_basement_preprocessor\n\n# Slab-on-grade foundation\nexpanded = run_slab_preprocessor(model, weather=\"weather.epw\")\n\n# Basement walls and floors\nexpanded = run_basement_preprocessor(model, weather=\"weather.epw\")\n</code></pre> <p>Each function runs ExpandObjects first (to extract the ground heat-transfer input), then the Fortran solver, and returns a new <code>IDFDocument</code> with the computed temperature schedules appended.</p> <p>All preprocessing functions raise <code>ExpandObjectsError</code> on failure, with structured <code>preprocessor</code>, <code>exit_code</code>, and <code>stderr</code> fields for programmatic error handling.</p> <p>See the Preprocessing API reference for full details.</p>"},{"location":"simulation/running/#function-signature","title":"Function Signature","text":"<pre><code>def simulate(\n    model: IDFDocument,\n    weather: str | Path,\n    *,\n    output_dir: str | Path | None = None,\n    energyplus: EnergyPlusConfig | None = None,\n    expand_objects: bool = True,\n    annual: bool = False,\n    design_day: bool = False,\n    output_prefix: str = \"eplus\",\n    output_suffix: Literal[\"C\", \"L\", \"D\"] = \"C\",\n    readvars: bool = False,\n    timeout: float = 3600.0,\n    extra_args: list[str] | None = None,\n    cache: SimulationCache | None = None,\n    fs: FileSystem | None = None,\n    on_progress: Callable[[SimulationProgress], Any] | Literal[\"tqdm\"] | None = None,\n) -&gt; SimulationResult:\n</code></pre>"},{"location":"simulation/running/#parameters","title":"Parameters","text":""},{"location":"simulation/running/#required","title":"Required","text":"Parameter Description <code>model</code> The EnergyPlus model to simulate <code>weather</code> Path to the weather file (.epw)"},{"location":"simulation/running/#optional","title":"Optional","text":"Parameter Default Description <code>output_dir</code> Auto temp Directory for output files <code>energyplus</code> Auto-detect Pre-configured EnergyPlus installation <code>expand_objects</code> <code>True</code> Run ExpandObjects (and Slab/Basement if needed) before simulation <code>annual</code> <code>False</code> Run annual simulation (<code>-a</code> flag) <code>design_day</code> <code>False</code> Run design-day-only (<code>-D</code> flag) <code>output_prefix</code> <code>\"eplus\"</code> Prefix for output files <code>output_suffix</code> <code>\"C\"</code> Output naming style (C/L/D) <code>readvars</code> <code>False</code> Run ReadVarsESO after simulation <code>timeout</code> <code>3600.0</code> Maximum runtime in seconds <code>extra_args</code> <code>None</code> Additional command-line arguments <code>cache</code> <code>None</code> Simulation cache for result reuse <code>fs</code> <code>None</code> File system backend for cloud storage <code>on_progress</code> <code>None</code> Callback or <code>\"tqdm\"</code> for real-time progress updates"},{"location":"simulation/running/#energyplus-discovery","title":"EnergyPlus Discovery","text":"<p>By default, <code>simulate()</code> auto-discovers EnergyPlus:</p> <pre><code># Auto-discovery\nresult = simulate(model, weather)\n\n# Explicit path\nfrom idfkit.simulation import find_energyplus\n\nconfig = find_energyplus(\"/custom/path/EnergyPlus-24-1-0\")\nresult = simulate(model, weather, energyplus=config)\n\n# Environment variable\n# Set ENERGYPLUS_DIR=/path/to/EnergyPlus before running\nresult = simulate(model, weather)\n</code></pre> <p>Discovery priority:</p> <ol> <li>Explicit <code>energyplus</code> parameter</li> <li><code>ENERGYPLUS_DIR</code> environment variable</li> <li>System PATH</li> <li>Platform default directories</li> </ol>"},{"location":"simulation/running/#output-directory","title":"Output Directory","text":""},{"location":"simulation/running/#automatic-temporary-directory","title":"Automatic Temporary Directory","text":"<p>By default, outputs go to an auto-generated temp directory:</p> <pre><code>result = simulate(model, weather)\nprint(result.run_dir)  # e.g., /tmp/idfkit_sim_abc123/\n</code></pre>"},{"location":"simulation/running/#explicit-directory","title":"Explicit Directory","text":"<p>Specify where to store outputs:</p> <pre><code>result = simulate(model, weather, output_dir=\"./sim_output\")\n</code></pre> <p>The directory is created if it doesn't exist.</p>"},{"location":"simulation/running/#error-handling","title":"Error Handling","text":""},{"location":"simulation/running/#simulation-errors","title":"Simulation Errors","text":"<pre><code>from idfkit.exceptions import SimulationError\n\ntry:\n    result = simulate(model, weather)\nexcept SimulationError as e:\n    print(f\"Simulation failed: {e}\")\n    print(f\"Exit code: {e.exit_code}\")\n    print(f\"Stderr: {e.stderr}\")\n</code></pre>"},{"location":"simulation/running/#timeout","title":"Timeout","text":"<pre><code>try:\n    result = simulate(model, weather, timeout=60.0)  # 1 minute max\nexcept SimulationError as e:\n    if e.exit_code is None:\n        print(\"Simulation timed out\")\n</code></pre>"},{"location":"simulation/running/#checking-success","title":"Checking Success","text":"<pre><code>result = simulate(model, weather)\n\nif not result.success:\n    print(f\"Exit code: {result.exit_code}\")\n    print(f\"Stderr: {result.stderr}\")\n    for err in result.errors.fatal:\n        print(f\"Error: {err.message}\")\n</code></pre>"},{"location":"simulation/running/#model-safety","title":"Model Safety","text":"<p><code>simulate()</code> copies your model before running \u2014 the original is never modified:</p> <pre><code>model = load_idf(\"building.idf\")\noriginal_count = len(model)\n\nresult = simulate(model, weather)\n\n# Model unchanged\nassert len(model) == original_count\nassert \"Output:SQLite\" not in model\n</code></pre>"},{"location":"simulation/running/#command-line-options","title":"Command-Line Options","text":""},{"location":"simulation/running/#output-suffix-modes","title":"Output Suffix Modes","text":"Value Description <code>\"C\"</code> Combined table files (default) <code>\"L\"</code> Legacy separate table files <code>\"D\"</code> Timestamped separate files <pre><code>result = simulate(model, weather, output_suffix=\"L\")\n</code></pre>"},{"location":"simulation/running/#extra-arguments","title":"Extra Arguments","text":"<p>Pass additional EnergyPlus flags:</p> <pre><code>result = simulate(\n    model,\n    weather,\n    extra_args=[\"--convert-only\"],  # Just convert, don't simulate\n)\n</code></pre>"},{"location":"simulation/running/#cloud-storage","title":"Cloud Storage","text":"<p>For remote storage backends (S3, etc.):</p> <pre><code>from idfkit.simulation import S3FileSystem\n\nfs = S3FileSystem(bucket=\"my-bucket\", prefix=\"runs/\")\nresult = simulate(\n    model,\n    weather,\n    output_dir=\"run-001\",  # Required with fs\n    fs=fs,\n)\n</code></pre> <p>See Cloud &amp; Remote Storage for details.</p>"},{"location":"simulation/running/#caching","title":"Caching","text":"<p>Enable content-addressed caching to avoid redundant simulations:</p> <pre><code>from idfkit.simulation import SimulationCache\n\ncache = SimulationCache()\n\n# First run: executes simulation\nresult1 = simulate(model, weather, cache=cache)\n\n# Second run: instant cache hit\nresult2 = simulate(model, weather, cache=cache)\n</code></pre> <p>See Caching for details.</p>"},{"location":"simulation/running/#see-also","title":"See Also","text":"<ul> <li>Progress Tracking \u2014 Real-time progress with <code>on_progress</code></li> <li>Parsing Results \u2014 Working with <code>SimulationResult</code></li> <li>Batch Processing \u2014 Running multiple simulations</li> <li>Error Handling \u2014 Understanding error reports</li> </ul>"},{"location":"simulation/sql-queries/","title":"SQL Output Queries","text":"<p>The <code>SQLResult</code> class provides structured access to EnergyPlus's SQLite output database, containing time-series data, tabular reports, and metadata.</p>"},{"location":"simulation/sql-queries/#opening-the-database","title":"Opening the Database","text":"<pre><code>from idfkit.simulation import simulate\n\nresult = simulate(model, weather)\n\nsql = result.sql\nif sql is not None:\n    # Query data...\n</code></pre> <p>Or open directly:</p> <pre><code>from idfkit.simulation import SQLResult\n\nsql = SQLResult(\"/path/to/eplusout.sql\")\n</code></pre>"},{"location":"simulation/sql-queries/#time-series-data","title":"Time-Series Data","text":""},{"location":"simulation/sql-queries/#basic-query","title":"Basic Query","text":"<pre><code>ts = sql.get_timeseries(\n    variable_name=\"Zone Mean Air Temperature\",\n    key_value=\"THERMAL ZONE 1\",\n)\n\nprint(f\"Variable: {ts.variable_name}\")\nprint(f\"Key: {ts.key_value}\")\nprint(f\"Units: {ts.units}\")\nprint(f\"Frequency: {ts.frequency}\")\nprint(f\"Data points: {len(ts.values)}\")\nprint(f\"Min: {min(ts.values):.1f}, Max: {max(ts.values):.1f}\")\n</code></pre>"},{"location":"simulation/sql-queries/#timeseriesresult-attributes","title":"TimeSeriesResult Attributes","text":"Attribute Type Description <code>variable_name</code> <code>str</code> Output variable name <code>key_value</code> <code>str</code> Key (zone, surface, etc.) <code>units</code> <code>str</code> Variable units <code>frequency</code> <code>str</code> Reporting frequency <code>timestamps</code> <code>tuple[datetime, ...]</code> Timestamps for each point <code>values</code> <code>tuple[float, ...]</code> Numeric values"},{"location":"simulation/sql-queries/#filtering-by-environment","title":"Filtering by Environment","text":"<p>Specify which simulation environment to query:</p> <pre><code># Design day results only (use for design_day=True simulations)\nts = sql.get_timeseries(\n    \"Zone Mean Air Temperature\",\n    \"ZONE 1\",\n    environment=\"sizing\",\n)\n\n# Annual/run period results only (default)\nts = sql.get_timeseries(\n    \"Zone Mean Air Temperature\",\n    \"ZONE 1\",\n    environment=\"annual\",\n)\n\n# All environments (design days + run periods)\nts = sql.get_timeseries(\n    \"Zone Mean Air Temperature\",\n    \"ZONE 1\",\n    environment=None,\n)\n</code></pre> <p>The <code>environment</code> parameter accepts:</p> Value Description <code>None</code> All data from all environments (default) <code>\"annual\"</code> Weather-file run period data only <code>\"sizing\"</code> Design day data only"},{"location":"simulation/sql-queries/#converting-to-dataframe","title":"Converting to DataFrame","text":"<pre><code>df = ts.to_dataframe()\nprint(df.head())\n#                              Zone Mean Air Temperature\n# timestamp\n# 2017-01-01 01:00:00                               21.2\n# 2017-01-01 02:00:00                               21.1\n# ...\n</code></pre> <p>Requires pandas: <code>pip install idfkit[dataframes]</code></p>"},{"location":"simulation/sql-queries/#plotting-time-series","title":"Plotting Time Series","text":"<pre><code>fig = ts.plot()  # Auto-detects matplotlib/plotly\n</code></pre> <p>Requires matplotlib or plotly: <code>pip install idfkit[plot]</code></p>"},{"location":"simulation/sql-queries/#tabular-data","title":"Tabular Data","text":""},{"location":"simulation/sql-queries/#query-tabular-reports","title":"Query Tabular Reports","text":"<pre><code>rows = sql.get_tabular_data(report_name=\"AnnualBuildingUtilityPerformanceSummary\")\n\nfor row in rows[:5]:\n    print(f\"{row.table_name} | {row.row_name} | {row.column_name}: {row.value}\")\n</code></pre>"},{"location":"simulation/sql-queries/#tabularrow-attributes","title":"TabularRow Attributes","text":"Attribute Type Description <code>report_name</code> <code>str</code> Report name <code>report_for</code> <code>str</code> Report scope (e.g., \"Entire Facility\") <code>table_name</code> <code>str</code> Table name within report <code>row_name</code> <code>str</code> Row label <code>column_name</code> <code>str</code> Column label <code>units</code> <code>str</code> Value units <code>value</code> <code>str</code> Cell value as string"},{"location":"simulation/sql-queries/#filter-by-table","title":"Filter by Table","text":"<pre><code>rows = sql.get_tabular_data(\n    report_name=\"AnnualBuildingUtilityPerformanceSummary\",\n    table_name=\"Site and Source Energy\",\n)\n</code></pre>"},{"location":"simulation/sql-queries/#common-reports","title":"Common Reports","text":"Report Name Description <code>AnnualBuildingUtilityPerformanceSummary</code> Energy use summary <code>InputVerificationandResultsSummary</code> Model summary <code>EnvelopeSummary</code> Building envelope details <code>LightingSummary</code> Lighting power densities <code>EquipmentSummary</code> Equipment capacities <code>HVACSizingSummary</code> HVAC sizing results <code>ZoneComponentLoadSummary</code> Zone load components"},{"location":"simulation/sql-queries/#variable-metadata","title":"Variable Metadata","text":""},{"location":"simulation/sql-queries/#list-available-variables","title":"List Available Variables","text":"<pre><code>variables = sql.list_variables()\n\nfor var in variables[:10]:\n    print(f\"{var.name} ({var.key_value}) [{var.units}] - {var.frequency}\")\n</code></pre>"},{"location":"simulation/sql-queries/#variableinfo-attributes","title":"VariableInfo Attributes","text":"Attribute Type Description <code>name</code> <code>str</code> Variable name <code>key_value</code> <code>str</code> Key value <code>frequency</code> <code>str</code> Reporting frequency <code>units</code> <code>str</code> Variable units <code>is_meter</code> <code>bool</code> Whether this is a meter <code>variable_type</code> <code>str</code> Variable type (Zone, HVAC, etc.)"},{"location":"simulation/sql-queries/#search-variables","title":"Search Variables","text":"<pre><code># By name pattern\ntemp_vars = [v for v in variables if \"Temperature\" in v.name]\n\n# By key\nzone1_vars = [v for v in variables if v.key_value == \"ZONE 1\"]\n</code></pre>"},{"location":"simulation/sql-queries/#environment-metadata","title":"Environment Metadata","text":""},{"location":"simulation/sql-queries/#list-environments","title":"List Environments","text":"<pre><code>environments = sql.get_environments()\n\nfor env in environments:\n    print(f\"{env.index}: {env.name} (type={env.environment_type})\")\n</code></pre>"},{"location":"simulation/sql-queries/#environment-types","title":"Environment Types","text":"Type Value Description Design Day 1 <code>SizingPeriod:DesignDay</code> simulation Design Run Period 2 <code>SizingPeriod:WeatherFileDays</code> Weather File Run Period 3 Regular <code>RunPeriod</code> simulation"},{"location":"simulation/sql-queries/#environmentinfo-attributes","title":"EnvironmentInfo Attributes","text":"Attribute Type Description <code>index</code> <code>int</code> Environment period index <code>name</code> <code>str</code> Environment name <code>environment_type</code> <code>int</code> Type code (1, 2, or 3)"},{"location":"simulation/sql-queries/#timestamps","title":"Timestamps","text":"<p>EnergyPlus uses a fixed reference year (2017) for timestamps. The SQLResult automatically converts database timestamps to Python <code>datetime</code> objects.</p>"},{"location":"simulation/sql-queries/#energyplus-time-convention","title":"EnergyPlus Time Convention","text":"<ul> <li>Hour 24 in the database \u2192 midnight of the next day</li> <li>Warmup days are filtered out automatically</li> </ul> <pre><code>ts = sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\n\n# Timestamps are proper Python datetime objects\nfirst = ts.timestamps[0]\nprint(f\"Year: {first.year}\")  # 2017 (reference year)\nprint(f\"Month: {first.month}\")\nprint(f\"Day: {first.day}\")\nprint(f\"Hour: {first.hour}\")\n</code></pre>"},{"location":"simulation/sql-queries/#context-manager","title":"Context Manager","text":"<p><code>SQLResult</code> is a context manager for clean database cleanup:</p> <pre><code>with SQLResult(\"/path/to/eplusout.sql\") as sql:\n    ts = sql.get_timeseries(\"Zone Mean Air Temperature\", \"ZONE 1\")\n    # Connection automatically closed on exit\n</code></pre>"},{"location":"simulation/sql-queries/#error-handling","title":"Error Handling","text":"<pre><code>sql = result.sql\nif sql is None:\n    print(\"No SQL output - was Output:SQLite in the model?\")\n    return\n\n# Get time series (raises KeyError if not found)\ntry:\n    ts = sql.get_timeseries(\"Nonexistent Variable\", \"ZONE 1\")\nexcept KeyError as e:\n    print(f\"Variable not found: {e}\")\n</code></pre>"},{"location":"simulation/sql-queries/#performance-tips","title":"Performance Tips","text":"<ol> <li>Filter early \u2014 Use the <code>environment</code> parameter to reduce data size</li> <li>Query once \u2014 Store results in variables rather than re-querying</li> <li>Use lazy loading \u2014 Don't access <code>result.sql</code> if you don't need it</li> </ol>"},{"location":"simulation/sql-queries/#see-also","title":"See Also","text":"<ul> <li>Parsing Results \u2014 Overview of result parsing</li> <li>Plotting \u2014 Visualizing query results</li> <li>Output Discovery \u2014 Finding available variables</li> </ul>"},{"location":"troubleshooting/energyplus/","title":"EnergyPlus Issues","text":"<p>This page covers common EnergyPlus-related issues and their solutions.</p>"},{"location":"troubleshooting/energyplus/#installation-problems","title":"Installation Problems","text":""},{"location":"troubleshooting/energyplus/#energyplus-not-found","title":"EnergyPlus Not Found","text":"<p>idfkit searches for EnergyPlus in this order:</p> <ol> <li>Explicit path in code</li> <li><code>ENERGYPLUS_DIR</code> environment variable</li> <li>System PATH</li> <li>Platform default locations:<ul> <li>macOS: <code>/Applications/EnergyPlus-*/</code></li> <li>Linux: <code>/usr/local/EnergyPlus-*/</code></li> <li>Windows: <code>C:\\EnergyPlusV*/</code></li> </ul> </li> </ol> <p>Verify installation:</p> <pre><code>from idfkit.simulation import find_energyplus\n\ntry:\n    config = find_energyplus()\n    print(f\"Found: {config.executable}\")\nexcept Exception as e:\n    print(f\"Not found: {e}\")\n</code></pre> <p>Specify path manually:</p> <pre><code>config = find_energyplus(\"/path/to/EnergyPlus-24-1-0\")\nresult = simulate(model, weather, energyplus=config)\n</code></pre>"},{"location":"troubleshooting/energyplus/#multiple-versions-installed","title":"Multiple Versions Installed","text":"<p>When multiple versions exist, idfkit uses the newest by default.</p> <p>Use a specific version:</p> <pre><code># Find a specific version\nconfig = find_energyplus(\"/Applications/EnergyPlus-23-2-0\")\nresult = simulate(model, weather, energyplus=config)\n</code></pre>"},{"location":"troubleshooting/energyplus/#permission-denied","title":"Permission Denied","text":"<p>Linux/macOS:</p> <pre><code>chmod +x /usr/local/EnergyPlus-*/energyplus\n</code></pre> <p>Windows: Run as Administrator or check file permissions.</p>"},{"location":"troubleshooting/energyplus/#simulation-failures","title":"Simulation Failures","text":""},{"location":"troubleshooting/energyplus/#fatal-errors","title":"Fatal Errors","text":"<p>Check the error report for details:</p> <pre><code>result = simulate(model, weather)\n\nif not result.success:\n    for err in result.errors.fatal:\n        print(f\"Fatal: {err.message}\")\n</code></pre>"},{"location":"troubleshooting/energyplus/#common-fatal-errors","title":"Common Fatal Errors","text":"Error Cause Solution \"No surfaces in zone\" Zone has no surfaces Add surfaces referencing the zone \"Weather file has no data\" Date mismatch Check RunPeriod matches weather file \"Node not found\" HVAC node missing Fix HVAC connections \"Schedule not found\" Missing schedule Add the referenced schedule"},{"location":"troubleshooting/energyplus/#severe-errors","title":"Severe Errors","text":"<p>Severe errors don't stop the simulation but may cause incorrect results:</p> <pre><code>for err in result.errors.severe:\n    print(f\"Severe: {err.message}\")\n</code></pre>"},{"location":"troubleshooting/energyplus/#warnings","title":"Warnings","text":"<p>Warnings are informational but worth reviewing:</p> <pre><code>print(f\"Warning count: {result.errors.warning_count}\")\n</code></pre>"},{"location":"troubleshooting/energyplus/#model-issues","title":"Model Issues","text":""},{"location":"troubleshooting/energyplus/#missing-outputsqlite","title":"Missing Output:SQLite","text":"<p>idfkit automatically adds <code>Output:SQLite</code> if missing. However, if you see SQL-related errors:</p> <pre><code># Verify SQL output is present\nif \"Output:SQLite\" not in model:\n    model.add(\"Output:SQLite\", option_type=\"SimpleAndTabular\")\n</code></pre>"},{"location":"troubleshooting/energyplus/#version-mismatch","title":"Version Mismatch","text":"<p>If the model was created for a different EnergyPlus version:</p> <pre><code># Load with explicit version\nmodel = load_idf(\"old_model.idf\", version=(24, 1, 0))\n</code></pre>"},{"location":"troubleshooting/energyplus/#geometry-errors","title":"Geometry Errors","text":"<p>Common geometry issues:</p> Error Solution \"Surface has zero area\" Fix vertex coordinates \"Zone volume is zero\" Ensure zone is fully enclosed \"Surface vertices are not planar\" Adjust vertices to be coplanar <p>Use idfkit's geometry tools to diagnose:</p> <pre><code>from idfkit.geometry import calculate_surface_area, calculate_zone_volume\n\nfor surface in model[\"BuildingSurface:Detailed\"].values():\n    area = calculate_surface_area(surface)\n    if area &lt;= 0:\n        print(f\"Invalid surface: {surface.name}\")\n\nvolume = calculate_zone_volume(model, \"Zone1\")\nif volume &lt;= 0:\n    print(\"Zone has invalid volume\")\n</code></pre>"},{"location":"troubleshooting/energyplus/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/energyplus/#simulation-takes-too-long","title":"Simulation Takes Too Long","text":"<ol> <li> <p>Use design-day mode for testing: <pre><code>result = simulate(model, weather, design_day=True)\n</code></pre></p> </li> <li> <p>Reduce timesteps: <pre><code># Check current timesteps\nif \"Timestep\" in model:\n    print(model[\"Timestep\"].values()[0].number_of_timesteps_per_hour)\n</code></pre></p> </li> <li> <p>Simplify the model:</p> </li> <li>Reduce zone count</li> <li>Simplify HVAC systems</li> <li>Use ideal loads for initial testing</li> </ol>"},{"location":"troubleshooting/energyplus/#high-memory-usage","title":"High Memory Usage","text":"<p>Large models may consume significant memory:</p> <ol> <li> <p>Close SQL connections when done:    <pre><code>sql = result.sql\n# ... use sql ...\ndel sql  # Or let it go out of scope\n</code></pre></p> </li> <li> <p>Process results incrementally in batch runs</p> </li> </ol>"},{"location":"troubleshooting/energyplus/#output-issues","title":"Output Issues","text":""},{"location":"troubleshooting/energyplus/#no-sql-output","title":"No SQL Output","text":"<p>If <code>result.sql</code> is <code>None</code>:</p> <ol> <li> <p>Check that EnergyPlus completed:    <pre><code>print(f\"Exit code: {result.exit_code}\")\nprint(f\"Success: {result.success}\")\n</code></pre></p> </li> <li> <p>Check the error report:    <pre><code>print(result.errors.summary())\n</code></pre></p> </li> <li> <p>Verify output files exist:    <pre><code>print(f\"SQL path: {result.sql_path}\")\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/energyplus/#missing-time-series-data","title":"Missing Time-Series Data","text":"<p>If <code>get_timeseries()</code> returns <code>None</code>:</p> <ol> <li> <p>Verify the variable name matches exactly:    <pre><code># List available variables\nfor var in result.sql.get_available_variables():\n    if \"Temperature\" in var.name:\n        print(f\"{var.name} | {var.key_value}\")\n</code></pre></p> </li> <li> <p>Check the reporting frequency in your model</p> </li> <li> <p>Verify the output was requested:    <pre><code># Add output request (name is optional for Output:Variable)\nmodel.add(\n    \"Output:Variable\",\n    key_value=\"*\",\n    variable_name=\"Zone Mean Air Temperature\",\n    reporting_frequency=\"Hourly\",\n)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/energyplus/#platform-specific-issues","title":"Platform-Specific Issues","text":""},{"location":"troubleshooting/energyplus/#macos","title":"macOS","text":"<p>Gatekeeper blocks EnergyPlus:</p> <pre><code>xattr -d com.apple.quarantine /Applications/EnergyPlus-*/energyplus\n</code></pre>"},{"location":"troubleshooting/energyplus/#linux","title":"Linux","text":"<p>Shared library errors:</p> <pre><code># Install required libraries\nsudo apt-get install libgl1-mesa-glx libxkbcommon-x11-0\n</code></pre>"},{"location":"troubleshooting/energyplus/#windows","title":"Windows","text":"<p>Path with spaces:</p> <p>Avoid installing EnergyPlus in paths with spaces. If necessary, use short paths or quotes.</p> <p>Long path issues:</p> <p>Enable long paths in Windows or use shorter directory names.</p>"},{"location":"troubleshooting/energyplus/#getting-help","title":"Getting Help","text":"<p>If you can't resolve an issue:</p> <ol> <li>Check the idfkit GitHub issues</li> <li>Check the EnergyPlus documentation</li> <li>Search the EnergyPlus Helpdesk</li> </ol> <p>When reporting issues, include:</p> <ul> <li>idfkit version (<code>idfkit.__version__</code>)</li> <li>EnergyPlus version</li> <li>Python version</li> <li>Operating system</li> <li>Error messages and stack traces</li> <li>Minimal reproducible example</li> </ul>"},{"location":"troubleshooting/energyplus/#see-also","title":"See Also","text":"<ul> <li>Common Errors \u2014 General error reference</li> <li>Simulation Error Handling \u2014 Parsing error reports</li> <li>Running Simulations \u2014 Simulation guide</li> </ul>"},{"location":"troubleshooting/errors/","title":"Common Errors","text":"<p>This page covers common errors you may encounter when using idfkit and how to resolve them.</p>"},{"location":"troubleshooting/errors/#idfkit-errors","title":"idfkit Errors","text":""},{"location":"troubleshooting/errors/#energyplusnotfounderror","title":"EnergyPlusNotFoundError","text":"<pre><code>EnergyPlusNotFoundError: Could not find EnergyPlus installation\n</code></pre> <p>Cause: idfkit cannot locate an EnergyPlus installation.</p> <p>Solutions:</p> <ol> <li> <p>Install EnergyPlus from energyplus.net/downloads</p> </li> <li> <p>Set the <code>ENERGYPLUS_DIR</code> environment variable:    <pre><code>export ENERGYPLUS_DIR=/path/to/EnergyPlus-24-1-0\n</code></pre></p> </li> <li> <p>Pass the path explicitly:    <pre><code>from idfkit.simulation import find_energyplus\nconfig = find_energyplus(\"/path/to/EnergyPlus\")\nresult = simulate(model, weather, energyplus=config)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/errors/#simulationerror","title":"SimulationError","text":"<pre><code>SimulationError: Weather file not found: /path/to/weather.epw\n</code></pre> <p>Cause: The specified weather file doesn't exist.</p> <p>Solutions:</p> <ol> <li>Verify the path is correct</li> <li>Use absolute paths instead of relative</li> <li>Download weather files using <code>WeatherDownloader</code></li> </ol>"},{"location":"troubleshooting/errors/#simulationerror-timeout","title":"SimulationError (Timeout)","text":"<pre><code>SimulationError: Simulation timed out after 3600 seconds\n</code></pre> <p>Cause: The simulation exceeded the timeout limit.</p> <p>Solutions:</p> <ol> <li> <p>Increase the timeout:    <pre><code>result = simulate(model, weather, timeout=7200.0)\n</code></pre></p> </li> <li> <p>Use design-day-only mode for testing:    <pre><code>result = simulate(model, weather, design_day=True)\n</code></pre></p> </li> <li> <p>Check for infinite loops in the model</p> </li> </ol>"},{"location":"troubleshooting/errors/#nodesigndayserror","title":"NoDesignDaysError","text":"<pre><code>NoDesignDaysError: No heating design days found in DDY file\n</code></pre> <p>Cause: The DDY file doesn't contain the requested design day type.</p> <p>Solutions:</p> <ol> <li> <p>Check available design days:    <pre><code>ddm = DesignDayManager(\"file.ddy\")\nprint(ddm.summary())\n</code></pre></p> </li> <li> <p>Use a different percentile:    <pre><code>ddm.apply_to_model(model, heating=\"99%\", cooling=\"1%\")\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/errors/#geocodingerror","title":"GeocodingError","text":"<pre><code>GeocodingError: No results found for address: ...\n</code></pre> <p>Cause: The address couldn't be geocoded.</p> <p>Solutions:</p> <ol> <li>Try a more specific address</li> <li>Try a different format (city name, ZIP code, landmark)</li> <li>Use coordinates directly:    <pre><code>results = index.nearest(41.88, -87.63)\n</code></pre></li> </ol>"},{"location":"troubleshooting/errors/#import-errors","title":"Import Errors","text":""},{"location":"troubleshooting/errors/#missing-optional-dependencies","title":"Missing Optional Dependencies","text":"<pre><code>ImportError: pandas is required for DataFrame conversion\n</code></pre> <p>Solutions:</p> <pre><code># Install specific extra\npip install idfkit[dataframes]\n\n# Or install all extras\npip install idfkit[all]\n</code></pre> <p>Common extras:</p> Feature Install Command DataFrames <code>pip install idfkit[dataframes]</code> Plotting (matplotlib) <code>pip install idfkit[plot]</code> Plotting (plotly) <code>pip install idfkit[plotly]</code> Progress bars (tqdm) <code>pip install idfkit[progress]</code> S3 Storage <code>pip install idfkit[s3]</code> Weather refresh <code>pip install idfkit[weather]</code>"},{"location":"troubleshooting/errors/#boto3-not-found","title":"boto3 Not Found","text":"<pre><code>ImportError: boto3 is required for S3FileSystem\n</code></pre> <p>Solution:</p> <pre><code>pip install idfkit[s3]\n</code></pre>"},{"location":"troubleshooting/errors/#validation-errors","title":"Validation Errors","text":""},{"location":"troubleshooting/errors/#reference-to-non-existent-object","title":"Reference to Non-Existent Object","text":"<pre><code>[ERROR] People:'Zone1_People'.zone_name: Reference to non-existent object 'ZONE1'\n</code></pre> <p>Cause: A field references an object name that doesn't exist.</p> <p>Solutions:</p> <ol> <li>Check the object name spelling (case-insensitive)</li> <li>Add the missing object:    <pre><code>model.add(\"Zone\", \"Zone1\", ...)\n</code></pre></li> <li>Fix the reference:    <pre><code>people_obj.zone_name = \"Correct_Zone_Name\"\n</code></pre></li> </ol>"},{"location":"troubleshooting/errors/#required-field-missing","title":"Required Field Missing","text":"<pre><code>[ERROR] People:'Zone1_People'.activity_level_schedule_name: Required field is missing\n</code></pre> <p>Cause: A required field wasn't provided.</p> <p>Solution:</p> <pre><code># Add the required field\nmodel.add(\n    \"People\", \"Zone1_People\",\n    zone_or_zonelist_or_space_or_spacelist_name=\"Zone1\",\n    number_of_people_schedule_name=\"Always_On\",\n    activity_level_schedule_name=\"Activity_Schedule\",  # Required!\n    number_of_people_calculation_method=\"People\",\n    number_of_people=10,\n)\n</code></pre>"},{"location":"troubleshooting/errors/#file-format-errors","title":"File Format Errors","text":""},{"location":"troubleshooting/errors/#invalid-idf-syntax","title":"Invalid IDF Syntax","text":"<pre><code>ValueError: Failed to parse IDF at line 42\n</code></pre> <p>Cause: The IDF file has invalid syntax.</p> <p>Solutions:</p> <ol> <li>Check for unclosed objects (missing <code>;</code>)</li> <li>Check for invalid field separators</li> <li>Validate with EnergyPlus directly first</li> </ol>"},{"location":"troubleshooting/errors/#version-mismatch","title":"Version Mismatch","text":"<pre><code>ValueError: Schema for version (99, 0, 0) not found\n</code></pre> <p>Cause: The IDF version isn't supported.</p> <p>Solutions:</p> <ol> <li> <p>Check supported versions:    <pre><code>from idfkit import get_schema_manager\nprint(get_schema_manager().get_available_versions())\n</code></pre></p> </li> <li> <p>Specify a supported version:    <pre><code>doc = load_idf(\"file.idf\", version=(24, 1, 0))\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/errors/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/errors/#slow-station-index-loading","title":"Slow Station Index Loading","text":"<p>Cause: First load compiles the index from source files.</p> <p>Solution: Subsequent loads are instant (uses cached index).</p>"},{"location":"troubleshooting/errors/#large-memory-usage","title":"Large Memory Usage","text":"<p>Cause: Loading many large models or keeping many results in memory.</p> <p>Solutions:</p> <ol> <li>Use <code>model = None</code> to release memory after use</li> <li>Process batch results incrementally:    <pre><code>for result in batch:\n    process(result)\n    # Result memory released when loop continues\n</code></pre></li> </ol>"},{"location":"troubleshooting/errors/#see-also","title":"See Also","text":"<ul> <li>EnergyPlus Issues \u2014 EnergyPlus-specific errors</li> <li>Simulation Error Handling \u2014 Detailed error parsing</li> </ul>"},{"location":"weather/","title":"Weather Overview","text":"<p>The weather module provides tools for searching weather stations, downloading weather files, and applying ASHRAE design day conditions to your models.</p>"},{"location":"weather/#quick-start","title":"Quick Start","text":"<pre><code>from idfkit.weather import StationIndex, WeatherDownloader\n\n# Load station index (instant, no network needed)\nindex = StationIndex.load()\nprint(f\"{len(index)} stations from {len(index.countries)} countries\")\n\n# Search by name\nresults = index.search(\"chicago ohare\")\nstation = results[0].station\nprint(f\"Found: {station.display_name}\")\n\n# Download weather files\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\nprint(f\"EPW: {files.epw}\")\nprint(f\"DDY: {files.ddy}\")\n</code></pre>"},{"location":"weather/#key-features","title":"Key Features","text":""},{"location":"weather/#55000-weather-stations","title":"55,000+ Weather Stations","text":"<p>The bundled index contains data from climate.onebuilding.org, covering:</p> <ul> <li>~55,000 dataset entries from 10 world regions</li> <li>~17,300 unique physical stations</li> <li>248 countries and territories</li> </ul>"},{"location":"weather/#no-network-required","title":"No Network Required","text":"<p><code>StationIndex.load()</code> works instantly without network access \u2014 the index is pre-compiled and bundled with the package.</p>"},{"location":"weather/#address-based-search","title":"Address-Based Search","text":"<p>Find the nearest weather station to any address:</p> <pre><code>from idfkit.weather import StationIndex, geocode\n\nindex = StationIndex.load()\nresults = index.nearest(*geocode(\"350 Fifth Avenue, New York, NY\"))\n\nfor r in results[:3]:\n    print(f\"{r.station.display_name}: {r.distance_km:.0f} km\")\n</code></pre>"},{"location":"weather/#ashrae-design-days","title":"ASHRAE Design Days","text":"<p>Apply standard design day conditions to your model:</p> <pre><code>from idfkit.weather import apply_ashrae_sizing\n\n# Apply ASHRAE 90.1 design conditions\nadded = apply_ashrae_sizing(model, station, standard=\"90.1\")\nprint(f\"Added {len(added)} design days\")\n</code></pre>"},{"location":"weather/#module-components","title":"Module Components","text":"Component Description <code>StationIndex</code> Search and filter weather stations <code>WeatherDownloader</code> Download EPW and DDY files <code>DesignDayManager</code> Parse and apply design days <code>geocode()</code> Convert addresses to coordinates"},{"location":"weather/#installation","title":"Installation","text":"<p>The core weather module requires no extra dependencies:</p> <pre><code>from idfkit.weather import StationIndex\n\nindex = StationIndex.load()  # Works out of the box\n</code></pre> <p>To refresh the index from upstream:</p> <pre><code>pip install idfkit[weather]  # Adds openpyxl\n</code></pre> <pre><code>if index.check_for_updates():\n    index = StationIndex.refresh()  # Downloads latest data\n</code></pre>"},{"location":"weather/#workflow-example","title":"Workflow Example","text":"<p>Complete workflow from address to simulation-ready model:</p> <pre><code>from idfkit import load_idf\nfrom idfkit.weather import (\n    StationIndex,\n    WeatherDownloader,\n    DesignDayManager,\n    geocode,\n)\n\n# Load your model\nmodel = load_idf(\"building.idf\")\n\n# Find nearest station to project location\nindex = StationIndex.load()\nlat, lon = geocode(\"123 Main St, Chicago, IL\")\nstation = index.nearest(lat, lon)[0].station\n\n# Download weather files\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\n\n# Apply design days\nddm = DesignDayManager(files.ddy)\nddm.apply_to_model(\n    model,\n    heating=\"99.6%\",\n    cooling=\"1%\",\n    update_location=True,\n)\n\n# Now ready for simulation\nfrom idfkit.simulation import simulate\n\nresult = simulate(model, files.epw)\n</code></pre>"},{"location":"weather/#data-source","title":"Data Source","text":"<p>All weather data comes from climate.onebuilding.org, which provides:</p> <ul> <li>TMYx (Typical Meteorological Year) files</li> <li>Multiple year ranges per station (2007-2021, 2009-2023, etc.)</li> <li>EPW format for EnergyPlus simulation</li> <li>DDY files with ASHRAE design day conditions</li> </ul>"},{"location":"weather/#next-steps","title":"Next Steps","text":"<ul> <li>Station Search \u2014 Find weather stations</li> <li>Weather Downloads \u2014 Download EPW/DDY files</li> <li>Design Days \u2014 Apply ASHRAE conditions</li> <li>Weather Pipeline Concepts \u2014 Architecture details</li> </ul>"},{"location":"weather/design-days/","title":"Design Days","text":"<p>The <code>DesignDayManager</code> parses DDY files and applies ASHRAE design day conditions to your EnergyPlus models for HVAC sizing.</p>"},{"location":"weather/design-days/#basic-usage","title":"Basic Usage","text":"<pre><code>from idfkit.weather import DesignDayManager\n\n# Parse a DDY file\nddm = DesignDayManager(\"chicago.ddy\")\n\n# Print summary\nprint(ddm.summary())\n\n# Apply design days to model\nadded = ddm.apply_to_model(\n    model,\n    heating=\"99.6%\",\n    cooling=\"1%\",\n)\nprint(f\"Added {len(added)} design days\")\n</code></pre>"},{"location":"weather/design-days/#quick-apply","title":"Quick Apply","text":"<p>Use <code>apply_ashrae_sizing()</code> for a streamlined workflow:</p> <pre><code>from idfkit.weather import apply_ashrae_sizing\n\n# Apply standard design conditions (downloads DDY from station automatically)\nadded = apply_ashrae_sizing(\n    model,\n    station,\n    standard=\"90.1\",  # ASHRAE 90.1 criteria\n)\n</code></pre>"},{"location":"weather/design-days/#design-day-types","title":"Design Day Types","text":"<p>DDY files contain multiple design day types classified by ASHRAE criteria:</p>"},{"location":"weather/design-days/#heating-design-days","title":"Heating Design Days","text":"Type Description Use Case <code>HEATING_99_6</code> 99.6% heating DB ASHRAE 90.1 (coldest) <code>HEATING_99</code> 99% heating DB ASHRAE 62.1 <code>HTG_WIND_99_6</code> Heating wind 99.6% Wind-driven infiltration"},{"location":"weather/design-days/#cooling-design-days","title":"Cooling Design Days","text":"Type Description Use Case <code>COOLING_DB_0_4</code> 0.4% cooling DB=&gt;MWB Extreme cooling <code>COOLING_DB_1</code> 1% cooling DB=&gt;MWB ASHRAE 90.1 <code>COOLING_DB_2</code> 2% cooling DB=&gt;MWB Less extreme <code>COOLING_WB_0_4</code> 0.4% cooling WB=&gt;MDB Humidity-driven <code>COOLING_WB_1</code> 1% cooling WB=&gt;MDB ASHRAE 90.1 <code>COOLING_ENTH_0_4</code> 0.4% enthalpy Peak enthalpy <code>COOLING_ENTH_1</code> 1% enthalpy Standard enthalpy"},{"location":"weather/design-days/#other-types","title":"Other Types","text":"Type Description <code>DEHUMID_0_4</code> 0.4% dehumidification <code>DEHUMID_1</code> 1% dehumidification <code>HUMIDIFICATION_99_6</code> 99.6% humidification <code>HUMIDIFICATION_99</code> 99% humidification"},{"location":"weather/design-days/#accessing-design-days","title":"Accessing Design Days","text":""},{"location":"weather/design-days/#by-type","title":"By Type","text":"<pre><code>from idfkit.weather import DesignDayManager, DesignDayType\n\nddm = DesignDayManager(\"chicago.ddy\")\n\n# Get specific design day\nhtg = ddm.get(DesignDayType.HEATING_99_6)\nif htg:\n    print(f\"Heating 99.6% DB: {htg.maximum_dry_bulb_temperature}\u00b0C\")\n\nclg = ddm.get(DesignDayType.COOLING_DB_1)\nif clg:\n    print(f\"Cooling 1% DB: {clg.maximum_dry_bulb_temperature}\u00b0C\")\n</code></pre>"},{"location":"weather/design-days/#all-design-days","title":"All Design Days","text":"<pre><code># All classified annual design days (returns a list of IDFObject)\nfor dd_obj in ddm.annual:\n    print(dd_obj.name)\n\n# Monthly design days\nfor dd_obj in ddm.monthly:\n    print(dd_obj.name)\n</code></pre>"},{"location":"weather/design-days/#applying-to-models","title":"Applying to Models","text":""},{"location":"weather/design-days/#basic-application","title":"Basic Application","text":"<pre><code>added = ddm.apply_to_model(\n    model,\n    heating=\"99.6%\",  # Use 99.6% heating conditions\n    cooling=\"1%\",  # Use 1% cooling conditions\n)\n</code></pre>"},{"location":"weather/design-days/#with-wet-bulb-design-day","title":"With Wet-Bulb Design Day","text":"<pre><code>added = ddm.apply_to_model(\n    model,\n    heating=\"99.6%\",\n    cooling=\"1%\",\n    include_wet_bulb=True,  # Also add WB=&gt;MDB cooling design day\n)\n</code></pre>"},{"location":"weather/design-days/#skip-site-location-update","title":"Skip Site Location Update","text":"<p>By default, <code>apply_to_model</code> also updates the <code>Site:Location</code> object. To skip this, set <code>update_location=False</code>:</p> <pre><code>added = ddm.apply_to_model(\n    model,\n    heating=\"99.6%\",\n    cooling=\"1%\",\n    update_location=False,  # Keep existing Site:Location\n)\n</code></pre>"},{"location":"weather/design-days/#ashrae-standards","title":"ASHRAE Standards","text":"<p>Different standards recommend different percentiles:</p>"},{"location":"weather/design-days/#ashrae-901-energy-standard","title":"ASHRAE 90.1 (Energy Standard)","text":"<pre><code>added = ddm.apply_to_model(\n    model,\n    heating=\"99.6%\",\n    cooling=\"1%\",\n)\n\n# Or use the convenience function (takes a WeatherStation, not a path)\nadded = apply_ashrae_sizing(model, station, standard=\"90.1\")\n</code></pre>"},{"location":"weather/design-days/#ashrae-621-ventilation","title":"ASHRAE 62.1 (Ventilation)","text":"<pre><code>added = ddm.apply_to_model(\n    model,\n    heating=\"99%\",  # Less extreme\n    cooling=\"1%\",\n)\n</code></pre>"},{"location":"weather/design-days/#design-day-object-fields","title":"Design Day Object Fields","text":"<p>When you access a design day, it's an <code>IDFObject</code> with these fields:</p> Field Description <code>name</code> Design day name <code>month</code> Month (1-12) <code>day_of_month</code> Day of month <code>day_type</code> Day type string <code>maximum_dry_bulb_temperature</code> Peak dry-bulb temp (\u00b0C) <code>daily_dry_bulb_temperature_range</code> Diurnal range (\u00b0C) <code>humidity_condition_type</code> How humidity is specified <code>wetbulb_or_dewpoint_at_maximum_dry_bulb</code> Humidity value <code>wind_speed</code> Design wind speed (m/s) <code>wind_direction</code> Wind direction (degrees)"},{"location":"weather/design-days/#summary-output","title":"Summary Output","text":"<pre><code>ddm = DesignDayManager(\"chicago.ddy\")\nprint(ddm.summary())\n</code></pre> <p>Output: <pre><code>Design days from: chicago.ddy\n\n  Location: Chicago Ohare Intl AP\n  Design days found: 114\n  Annual (classified): 18\n  Monthly: 96\n\n  [heating_99.6] Chicago Ohare Intl AP Ann Htg 99.6% Condns DB\n  [heating_99] Chicago Ohare Intl AP Ann Htg 99% Condns DB\n  [cooling_db_0.4] Chicago Ohare Intl AP Ann Clg .4% Condns DB=&gt;MWB\n  ...\n</code></pre></p>"},{"location":"weather/design-days/#location-object","title":"Location Object","text":"<p>DDY files also contain <code>Site:Location</code> data:</p> <pre><code>location = ddm.location\nif location:\n    print(f\"Site: {location.name}\")\n    print(f\"Latitude: {location.latitude}\")\n    print(f\"Longitude: {location.longitude}\")\n    print(f\"Time Zone: {location.time_zone}\")\n    print(f\"Elevation: {location.elevation} m\")\n</code></pre>"},{"location":"weather/design-days/#complete-workflow","title":"Complete Workflow","text":"<pre><code>from idfkit import load_idf\nfrom idfkit.weather import (\n    StationIndex,\n    WeatherDownloader,\n    DesignDayManager,\n    geocode,\n)\n\n# Load model\nmodel = load_idf(\"building.idf\")\n\n# Find nearest station\nindex = StationIndex.load()\nlat, lon = geocode(\"123 Main St, Chicago, IL\")\nstation = index.nearest(lat, lon)[0].station\n\n# Download weather files\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\n\n# Apply design days\nddm = DesignDayManager(files.ddy)\nadded = ddm.apply_to_model(\n    model,\n    heating=\"99.6%\",\n    cooling=\"1%\",\n    include_wet_bulb=True,\n    update_location=True,\n)\n\nprint(f\"Added {len(added)} design days\")\nprint(f\"Location: {model['Site:Location'].values()[0].name}\")\n</code></pre>"},{"location":"weather/design-days/#error-handling","title":"Error Handling","text":"<pre><code>from idfkit.exceptions import NoDesignDaysError\n\ntry:\n    ddm = DesignDayManager(\"incomplete.ddy\")\n    ddm.apply_to_model(model, heating=\"99.6%\", cooling=\"1%\")\nexcept NoDesignDaysError as e:\n    print(f\"Missing design days: {e}\")\n</code></pre>"},{"location":"weather/design-days/#see-also","title":"See Also","text":"<ul> <li>Weather Downloads \u2014 Get DDY files</li> <li>Station Search \u2014 Find weather stations</li> <li>Weather Pipeline \u2014 Architecture details</li> </ul>"},{"location":"weather/downloads/","title":"Weather Downloads","text":"<p>The <code>WeatherDownloader</code> downloads EPW and DDY weather files from climate.onebuilding.org with automatic caching.</p>"},{"location":"weather/downloads/#basic-usage","title":"Basic Usage","text":"<pre><code>from idfkit.weather import StationIndex, WeatherDownloader\n\n# Find a station\nindex = StationIndex.load()\nstation = index.search(\"chicago ohare\")[0].station\n\n# Download weather files\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\n\nprint(f\"EPW: {files.epw}\")\nprint(f\"DDY: {files.ddy}\")\n</code></pre>"},{"location":"weather/downloads/#weatherfiles","title":"WeatherFiles","text":"<p>The <code>download()</code> method returns a <code>WeatherFiles</code> object:</p> Attribute Type Description <code>epw</code> <code>Path</code> Path to the EPW file <code>ddy</code> <code>Path</code> Path to the DDY file <code>stat</code> <code>Path | None</code> Path to the STAT file (may be None) <code>zip_path</code> <code>Path</code> Path to the original downloaded ZIP archive <code>station</code> <code>WeatherStation</code> The station this download corresponds to <pre><code>files = downloader.download(station)\n\n# Use for simulation\nfrom idfkit.simulation import simulate\n\nresult = simulate(model, files.epw)\n\n# Use for design days\nfrom idfkit.weather import DesignDayManager\n\nddm = DesignDayManager(files.ddy)\n</code></pre>"},{"location":"weather/downloads/#caching","title":"Caching","text":"<p>Downloaded files are cached locally to avoid redundant downloads:</p> <pre><code># First download: fetches from internet\nfiles1 = downloader.download(station)\n\n# Second download: instant from cache\nfiles2 = downloader.download(station)\n\nassert files1.epw == files2.epw  # Same cached file\n</code></pre>"},{"location":"weather/downloads/#cache-location","title":"Cache Location","text":"<p>Default locations by platform:</p> Platform Default Path Linux <code>~/.cache/idfkit/weather/files/</code> macOS <code>~/Library/Caches/idfkit/weather/files/</code> Windows <code>%LOCALAPPDATA%\\idfkit\\cache\\weather\\files\\</code>"},{"location":"weather/downloads/#custom-cache-directory","title":"Custom Cache Directory","text":"<pre><code>from pathlib import Path\n\ndownloader = WeatherDownloader(cache_dir=Path(\"/data/weather_cache\"))\n</code></pre>"},{"location":"weather/downloads/#clear-cache","title":"Clear Cache","text":"<pre><code>import shutil\n\nshutil.rmtree(downloader.cache_dir)\n</code></pre>"},{"location":"weather/downloads/#download-process","title":"Download Process","text":"<p>The downloader:</p> <ol> <li>Checks if files are already cached</li> <li>Downloads the ZIP file from the station's URL</li> <li>Extracts EPW and DDY files</li> <li>Stores in the cache directory</li> <li>Returns paths to the extracted files</li> </ol>"},{"location":"weather/downloads/#error-handling","title":"Error Handling","text":"<pre><code>from idfkit.weather import WeatherDownloader\n\ndownloader = WeatherDownloader()\n\ntry:\n    files = downloader.download(station)\nexcept Exception as e:\n    print(f\"Download failed: {e}\")\n</code></pre> <p>Common errors:</p> <ul> <li>Network connectivity issues</li> <li>Invalid station URL</li> <li>Server temporarily unavailable</li> </ul>"},{"location":"weather/downloads/#offline-usage","title":"Offline Usage","text":"<p>Once files are cached, no network is needed:</p> <pre><code># Pre-download files while online\ndownloader = WeatherDownloader()\nfor station in my_stations:\n    downloader.download(station)\n\n# Later, offline usage works\nfiles = downloader.download(station)  # From cache\n</code></pre>"},{"location":"weather/downloads/#batch-downloads","title":"Batch Downloads","text":"<p>Download files for multiple stations:</p> <pre><code>from idfkit.weather import StationIndex, WeatherDownloader\n\nindex = StationIndex.load()\ndownloader = WeatherDownloader()\n\n# Download for multiple cities\ncities = [\"chicago\", \"new york\", \"los angeles\", \"houston\"]\nweather_files = {}\n\nfor city in cities:\n    station = index.search(city)[0].station\n    files = downloader.download(station)\n    weather_files[city] = files\n    print(f\"Downloaded: {station.display_name}\")\n</code></pre>"},{"location":"weather/downloads/#file-format-details","title":"File Format Details","text":""},{"location":"weather/downloads/#epw-energyplus-weather","title":"EPW (EnergyPlus Weather)","text":"<ul> <li>Hourly weather data for a typical meteorological year</li> <li>Contains temperature, humidity, solar radiation, wind, etc.</li> <li>Used by <code>simulate()</code> for annual simulations</li> </ul>"},{"location":"weather/downloads/#ddy-design-day","title":"DDY (Design Day)","text":"<ul> <li>ASHRAE design day conditions</li> <li>Contains <code>SizingPeriod:DesignDay</code> objects</li> <li>Used for HVAC sizing calculations</li> </ul>"},{"location":"weather/downloads/#integration-example","title":"Integration Example","text":"<p>Complete workflow:</p> <pre><code>from idfkit import load_idf\nfrom idfkit.weather import (\n    StationIndex,\n    WeatherDownloader,\n    DesignDayManager,\n    geocode,\n)\nfrom idfkit.simulation import simulate\n\n# Load model\nmodel = load_idf(\"building.idf\")\n\n# Find station near project site\nindex = StationIndex.load()\nlat, lon = geocode(\"123 Main St, Chicago, IL\")\nstation = index.nearest(lat, lon)[0].station\n\n# Download weather files\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\n\n# Apply design days\nddm = DesignDayManager(files.ddy)\nddm.apply_to_model(model, heating=\"99.6%\", cooling=\"1%\")\n\n# Run simulation\nresult = simulate(model, files.epw, design_day=True)\nprint(f\"Success: {result.success}\")\n</code></pre>"},{"location":"weather/downloads/#see-also","title":"See Also","text":"<ul> <li>Station Search \u2014 Find weather stations</li> <li>Design Days \u2014 Apply design day conditions</li> <li>Weather Pipeline \u2014 Architecture details</li> </ul>"},{"location":"weather/geocoding/","title":"Geocoding","text":"<p>The <code>geocode()</code> function converts addresses to coordinates using the free Nominatim (OpenStreetMap) service.</p>"},{"location":"weather/geocoding/#basic-usage","title":"Basic Usage","text":"<pre><code>from idfkit.weather import geocode\n\n# Get coordinates for an address\nlat, lon = geocode(\"350 Fifth Avenue, New York, NY\")\nprint(f\"Empire State Building: {lat:.4f}, {lon:.4f}\")\n</code></pre>"},{"location":"weather/geocoding/#with-station-search","title":"With Station Search","text":"<p>Combine with <code>StationIndex.nearest()</code> for address-based weather station lookup:</p> <pre><code>from idfkit.weather import StationIndex, geocode\n\nindex = StationIndex.load()\n\n# One-liner using splat operator\nresults = index.nearest(*geocode(\"Willis Tower, Chicago, IL\"))\n\n# First result is the nearest station\nstation = results[0].station\nprint(f\"Nearest: {station.display_name} ({results[0].distance_km:.1f} km)\")\n</code></pre>"},{"location":"weather/geocoding/#address-formats","title":"Address Formats","text":"<p>The geocoder accepts various address formats:</p> <pre><code># Full address\nlat, lon = geocode(\"123 Main Street, Springfield, IL 62701\")\n\n# Landmark\nlat, lon = geocode(\"Eiffel Tower, Paris\")\n\n# City only\nlat, lon = geocode(\"Tokyo, Japan\")\n\n# Partial address\nlat, lon = geocode(\"Times Square, New York\")\n</code></pre>"},{"location":"weather/geocoding/#error-handling","title":"Error Handling","text":"<pre><code>from idfkit.weather import geocode, GeocodingError\n\ntry:\n    lat, lon = geocode(\"Nonexistent Place XYZ123\")\nexcept GeocodingError as e:\n    print(f\"Geocoding failed: {e}\")\n</code></pre>"},{"location":"weather/geocoding/#common-errors","title":"Common Errors","text":"Situation Behavior Address not found Raises <code>GeocodingError</code> Network error Raises <code>GeocodingError</code> Rate limited Automatically retries with delay"},{"location":"weather/geocoding/#rate-limiting","title":"Rate Limiting","text":"<p>Nominatim requires a maximum of 1 request per second. The <code>geocode()</code> function automatically handles rate limiting:</p> <pre><code># These are automatically spaced 1 second apart\nfor address in addresses:\n    lat, lon = geocode(address)  # Rate limited internally\n    print(f\"{address}: {lat:.2f}, {lon:.2f}\")\n</code></pre>"},{"location":"weather/geocoding/#no-api-key-required","title":"No API Key Required","text":"<p>Nominatim is a free service that doesn't require an API key. However:</p> <ul> <li>Be respectful of usage limits</li> <li>Avoid bulk geocoding (use batch geocoding services for large datasets)</li> <li>Cache results when possible</li> </ul>"},{"location":"weather/geocoding/#caching-results","title":"Caching Results","text":"<p>For repeated lookups, cache the coordinates:</p> <pre><code>from functools import lru_cache\n\n\n@lru_cache(maxsize=100)\ndef cached_geocode(address: str) -&gt; tuple[float, float]:\n    return geocode(address)\n\n\n# Subsequent calls are instant\nlat, lon = cached_geocode(\"123 Main St\")\nlat, lon = cached_geocode(\"123 Main St\")  # From cache\n</code></pre>"},{"location":"weather/geocoding/#complete-workflow","title":"Complete Workflow","text":"<pre><code>from idfkit import load_idf\nfrom idfkit.weather import (\n    StationIndex,\n    WeatherDownloader,\n    DesignDayManager,\n    geocode,\n)\nfrom idfkit.simulation import simulate\n\n# Project location\nproject_address = \"1600 Pennsylvania Avenue, Washington, DC\"\n\n# Find nearest weather station\nindex = StationIndex.load()\nlat, lon = geocode(project_address)\nresults = index.nearest(lat, lon)\n\nstation = results[0].station\nprint(f\"Project location: {lat:.4f}, {lon:.4f}\")\nprint(f\"Nearest station: {station.display_name} ({results[0].distance_km:.1f} km)\")\n\n# Download weather data\ndownloader = WeatherDownloader()\nfiles = downloader.download(station)\n\n# Load model and apply design days\nmodel = load_idf(\"building.idf\")\nddm = DesignDayManager(files.ddy)\nddm.apply_to_model(model, heating=\"99.6%\", cooling=\"1%\", update_location=True)\n\n# Run simulation\nresult = simulate(model, files.epw, design_day=True)\n</code></pre>"},{"location":"weather/geocoding/#accuracy-notes","title":"Accuracy Notes","text":"<ul> <li>Geocoding accuracy varies by location and address specificity</li> <li>Results may vary slightly over time as OpenStreetMap data is updated</li> <li>For critical applications, verify coordinates manually</li> </ul>"},{"location":"weather/geocoding/#alternative-direct-coordinates","title":"Alternative: Direct Coordinates","text":"<p>If you already know the coordinates, skip geocoding entirely:</p> <pre><code># Direct coordinate lookup\nresults = index.nearest(40.7484, -73.9857)  # Empire State Building\n</code></pre>"},{"location":"weather/geocoding/#see-also","title":"See Also","text":"<ul> <li>Station Search \u2014 Find weather stations</li> <li>Weather Downloads \u2014 Download weather files</li> <li>Weather Overview \u2014 Module overview</li> </ul>"},{"location":"weather/station-search/","title":"Station Search","text":"<p>The <code>StationIndex</code> provides fast searching and filtering of 55,000+ weather station entries from climate.onebuilding.org.</p>"},{"location":"weather/station-search/#loading-the-index","title":"Loading the Index","text":"<pre><code>from idfkit.weather import StationIndex\n\n# Instant load from bundled data (no network needed)\nindex = StationIndex.load()\n\nprint(f\"Stations: {len(index)}\")\nprint(f\"Countries: {len(index.countries)}\")\n</code></pre>"},{"location":"weather/station-search/#search-by-name","title":"Search by Name","text":"<p>Fuzzy text search across station names, cities, and WMO numbers:</p> <pre><code># Search by name\nresults = index.search(\"chicago ohare\")\n\nfor r in results[:5]:\n    print(f\"{r.station.display_name} (score={r.score:.2f})\")\n</code></pre>"},{"location":"weather/station-search/#searchresult-attributes","title":"SearchResult Attributes","text":"Attribute Type Description <code>station</code> <code>WeatherStation</code> The matching station <code>score</code> <code>float</code> Match score (0.0 to 1.0)"},{"location":"weather/station-search/#search-tips","title":"Search Tips","text":"<pre><code># City name\nresults = index.search(\"New York\")\n\n# Airport code pattern\nresults = index.search(\"JFK\")\n\n# WMO number\nresults = index.search(\"725300\")\n\n# Country + city\nresults = index.search(\"London UK\")\n</code></pre>"},{"location":"weather/station-search/#search-by-coordinates","title":"Search by Coordinates","text":"<p>Find stations nearest to a location using great-circle distance:</p> <pre><code># Nearest to downtown Chicago\nresults = index.nearest(41.88, -87.63)\n\nfor r in results[:5]:\n    print(f\"{r.station.display_name}: {r.distance_km:.1f} km\")\n</code></pre>"},{"location":"weather/station-search/#function-signature","title":"Function Signature","text":"<pre><code>def nearest(\n    self,\n    latitude: float,\n    longitude: float,\n    *,\n    limit: int = 5,\n    max_distance_km: float | None = None,\n    country: str | None = None,\n) -&gt; list[SpatialResult]:\n</code></pre>"},{"location":"weather/station-search/#spatialresult-attributes","title":"SpatialResult Attributes","text":"Attribute Type Description <code>station</code> <code>WeatherStation</code> The nearby station <code>distance_km</code> <code>float</code> Distance in kilometers"},{"location":"weather/station-search/#search-by-address","title":"Search by Address","text":"<p>Combine <code>geocode()</code> with <code>nearest()</code> for address-based search:</p> <pre><code>from idfkit.weather import StationIndex, geocode\n\nindex = StationIndex.load()\n\n# One-liner using splat operator\nresults = index.nearest(*geocode(\"Willis Tower, Chicago, IL\"))\n\n# Or step by step\nlat, lon = geocode(\"350 Fifth Avenue, New York, NY\")\nresults = index.nearest(lat, lon)\n</code></pre>"},{"location":"weather/station-search/#filter-by-country","title":"Filter by Country","text":"<pre><code># Get all stations in a country\nus_stations = index.filter(country=\"USA\")\nprint(f\"US stations: {len(us_stations)}\")\n\n# Get all stations in a state/region\ncalifornia = [s for s in us_stations if s.state == \"CA\"]\n</code></pre>"},{"location":"weather/station-search/#filter-by-coordinates","title":"Filter by Coordinates","text":"<p>Use <code>nearest()</code> with <code>max_distance_km</code> to find stations within a geographic area:</p> <pre><code># Find all stations within 100 km of a point\nstations = index.nearest(\n    41.0,\n    -88.5,\n    max_distance_km=100.0,\n    limit=50,\n)\n</code></pre>"},{"location":"weather/station-search/#get-by-wmo-number","title":"Get by WMO Number","text":"<pre><code># Get specific station by WMO\nresults = index.get_by_wmo(\"725300\")\n\nfor station in results:\n    print(f\"{station.display_name}: {station.source}\")\n</code></pre> <p>Note: WMO numbers are not unique \u2014 multiple entries can share a WMO (different year ranges, data sources).</p>"},{"location":"weather/station-search/#weatherstation-attributes","title":"WeatherStation Attributes","text":"Attribute Type Description <code>city</code> <code>str</code> City/station name <code>state</code> <code>str</code> State/province/region <code>country</code> <code>str</code> Country name <code>wmo</code> <code>str</code> WMO station number <code>source</code> <code>str</code> Data source identifier (e.g., \"SRC-TMYx\") <code>latitude</code> <code>float</code> Station latitude <code>longitude</code> <code>float</code> Station longitude <code>timezone</code> <code>float</code> UTC offset (hours from GMT) <code>elevation</code> <code>float</code> Elevation in meters <code>url</code> <code>str</code> Download URL for weather files <code>display_name</code> <code>str</code> Formatted name (city, state, country)"},{"location":"weather/station-search/#listing-countries","title":"Listing Countries","text":"<pre><code># Get all available countries\ncountries = index.countries\n\nfor country in sorted(countries)[:10]:\n    count = len(index.filter(country=country))\n    print(f\"{country}: {count} stations\")\n</code></pre>"},{"location":"weather/station-search/#refreshing-the-index","title":"Refreshing the Index","text":"<p>The bundled index works without network access. To get the latest data:</p> <pre><code># Check if upstream has updates\nif index.check_for_updates():\n    print(\"Updates available\")\n\n    # Refresh from climate.onebuilding.org (requires openpyxl)\n    index = StationIndex.refresh()\n</code></pre> <p>Refresh requires: <code>pip install idfkit[weather]</code></p>"},{"location":"weather/station-search/#performance","title":"Performance","text":"<p>The index uses efficient data structures for fast searching:</p> Operation Typical Time <code>load()</code> ~100ms <code>search(query)</code> ~10ms <code>nearest(lat, lon)</code> ~50ms <code>filter(country=...)</code> ~5ms"},{"location":"weather/station-search/#best-practices","title":"Best Practices","text":"<ol> <li>Load once \u2014 Keep the index in memory for multiple searches</li> <li>Use spatial search \u2014 More accurate than name matching for locations</li> <li>Check multiple results \u2014 First result isn't always the best match</li> <li>Verify WMO \u2014 Same physical station may have multiple entries</li> </ol>"},{"location":"weather/station-search/#see-also","title":"See Also","text":"<ul> <li>Weather Downloads \u2014 Download files for a station</li> <li>Geocoding \u2014 Convert addresses to coordinates</li> <li>Weather Pipeline \u2014 Architecture details</li> </ul>"}]}